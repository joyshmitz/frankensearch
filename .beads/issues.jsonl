{"id":"bd-11n","title":"Per-Hit Search Result Explanations","description":"Add opt-in per-hit explanations that decompose why each result was ranked where it is. This is the #1 feature request pattern in search libraries — users need to debug \"why did this rank higher than that?\"\n\n## Background\n\nFrankensearch is a hybrid search system that combines lexical (BM25), semantic (fast and quality embeddings), and reranking signals via Reciprocal Rank Fusion (RRF). Each result's final score is a composite of 4+ score sources, each with its own normalization and weighting. Debugging relevance issues requires understanding the contribution of each source, but currently there is no way to inspect score decomposition at the per-hit level.\n\n## Design\n\n### Data Structures\n\n```rust\npub struct HitExplanation {\n    pub final_score: f64,\n    pub components: Vec<ScoreComponent>,\n    pub phase: SearchPhase,\n    pub rank_movement: Option<RankMovement>,\n}\n\npub struct ScoreComponent {\n    pub source: ScoreSource,\n    pub raw_score: f64,\n    pub normalized_score: f64,\n    pub rrf_contribution: f64,\n    pub weight: f64,\n}\n\npub enum ScoreSource {\n    LexicalBm25 { matched_terms: Vec<String>, tf: f64, idf: f64 },\n    SemanticFast { embedder: String, cosine_sim: f64 },\n    SemanticQuality { embedder: String, cosine_sim: f64 },\n    Rerank { model: String, logit: f64, sigmoid: f64 },\n}\n\npub struct RankMovement {\n    pub initial_rank: usize,\n    pub refined_rank: usize,\n    pub delta: i32,\n    pub reason: String, // \"promoted by quality embedder\", \"demoted after rerank\"\n}\n```\n\n### Activation\n\n`TwoTierConfig { explain: true, .. }` — when false (default), no allocation overhead. The explain flag gates all explanation-related computation and allocation, ensuring zero cost for production workloads that don't need debugging.\n\n### Integration Points\n\n- **RRF fusion**: record per-source rank and RRF contribution for each hit\n- **Score normalization**: record raw -> normalized mapping so users can see how raw scores were transformed\n- **Two-tier blend**: record fast vs quality contribution for each hit\n- **Rerank**: record pre/post rerank scores and the logit/sigmoid values from the reranker model\n- **TwoTierSearcher**: attach HitExplanation to FusedHit when explain=true\n\n## Justification\n\nDebugging search relevance is the hardest part of operating a hybrid search system. Without explanations, users resort to printf debugging across 4+ score sources, manually correlating log lines to understand why a particular document ranked where it did. This makes frankensearch self-documenting at runtime. Every major search engine (Elasticsearch, Lucene, Vespa) provides this feature because it's essential for relevance tuning.\n\n## Considerations\n\n- Memory allocation: when explain=false, zero additional allocations per hit\n- Serialization: HitExplanation should implement Serialize for JSON output (debugging tools)\n- String formatting: provide a human-readable Display impl for terminal/log output\n- Partial explanations: if a score source is not active (e.g., no reranker configured), that component is simply absent from the components list\n\n## Testing\n\n- [ ] Unit: verify explanation components sum to final score (within floating-point epsilon)\n- [ ] Unit: verify RankMovement correctly tracks position changes across phases\n- [ ] Unit: verify zero overhead when explain=false (benchmark: search with explain=false should show no regression vs current baseline)\n- [ ] Unit: verify all ScoreSource variants are correctly populated\n- [ ] Integration: full pipeline explanation with all score sources active (BM25 + fast + quality + rerank)\n- [ ] Integration: partial pipeline (e.g., only BM25 + fast) produces correct partial explanation","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T21:58:35.056497314Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:05.208869191Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-11n","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T22:02:02.364970097Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-11n","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T22:02:02.480475489Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-11n","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T22:02:02.254207866Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-11n","depends_on_id":"bd-z3j","type":"blocks","created_at":"2026-02-13T22:05:05.208824848Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1b8","title":"MRL Adaptive Dimensionality at Search Time","description":"Implement Matryoshka Representation Learning (MRL) adaptive dimensionality for search-time dimension truncation. MRL-trained models (which includes potion-128M and many modern sentence transformers) produce embeddings where the first N dimensions carry the most information. We can search with truncated embeddings for speed, then use full dimensions for re-scoring.\n\n## Design\n\n```rust\npub struct MrlConfig {\n    pub search_dims: usize,     // Dimensions for initial scan (default: 64)\n    pub rescore_dims: usize,    // Dimensions for re-scoring top candidates (default: full)\n    pub rescore_top_k: usize,   // Re-score this many candidates (default: 3x limit)\n    pub enabled: bool,          // Default: false\n}\n\nimpl VectorIndex {\n    /// Search using only first `search_dims` dimensions, then re-score top candidates\n    /// with full `rescore_dims` dimensions for better accuracy.\n    pub fn mrl_search(\n        &self,\n        query: &[f32],\n        limit: usize,\n        config: &MrlConfig,\n    ) -> Vec<VectorHit>;\n}\n```\n\n## Algorithm\n\n1. Truncate query to first search_dims dimensions\n2. Scan all vectors using only first search_dims dimensions (fewer multiply-accumulate ops)\n3. Collect top rescore_top_k candidates\n4. Re-score candidates using full rescore_dims dimensions\n5. Return top limit results\n\n## Performance Model\n\n- Standard search (384 dims): 384 multiply-accumulate per vector\n- MRL search (64 + rescore 30): 64*N + 384*30 = 64N + 11520\n- Break-even at N ≈ 36 vectors. For N > 100, MRL is faster.\n- For 10K vectors: 640K vs 3.84M ops = 6x speedup on initial scan\n- Net speedup depends on rescore_top_k but typically 2-4x for large indices\n\n## SIMD Considerations\n\n- 64 dims = 8 f32x8 operations (perfect alignment)\n- 128 dims = 16 f32x8 operations\n- Truncation points should be multiples of 8 for SIMD efficiency\n\n## Index Format\n\nNo FSVI changes needed. We store full-dimension vectors and truncate at search time. The dimension information is in the header, and truncation is a runtime operation.\n\n## Why This Matters\n\nFor large indices (100K+ vectors), search latency is dominated by the dot product scan. MRL gives a 2-4x speedup with minimal quality loss because the first 64 dimensions of MRL-trained models capture 90%+ of the variance. This is particularly valuable for the fast tier where latency budget is tight (<15ms).\n\n## Testing\n\n- Unit: truncated search returns same top-1 as full search (on MRL-trained embeddings)\n- Unit: search_dims must be <= actual dimension\n- Unit: SIMD alignment (search_dims multiple of 8)\n- Unit: rescore_top_k < limit → fall back to full search\n- Integration: MRL search quality vs full search (recall@10 comparison)\n- Benchmark: speedup factor for various index sizes and search_dims","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T22:00:36.745371880Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:28.046429281Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1b8","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T22:02:27.829647768Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1b8","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T22:02:27.940407775Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1b8","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T22:02:28.046355974Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1co","title":"Index Warm-Up and Adaptive Prefault Strategy","description":"Implement index warm-up and adaptive page prefaulting for memory-mapped FSVI indices. Cold-start latency for mmap'd indices can be 10-100x higher than warm due to page faults. This bead adds controlled prefaulting to eliminate cold-start variance.\n\n## Background\n\nFrankensearch's FSVI indices are memory-mapped for zero-copy access. This is optimal for steady-state performance: the OS page cache keeps frequently-accessed pages in memory, and search reads directly from mapped memory without any copying. However, the first search after process start (or after memory pressure evicts pages) triggers a cascade of page faults that makes cold-start search dramatically slower.\n\nFor xf with 50K documents (~20MB index), cold search is approximately 50ms vs ~1ms warm — a 50x difference. For larger indices, this gap grows. This latency variance is problematic for interactive applications where consistent sub-10ms response times are expected.\n\n## Design\n\n### Configuration\n\n```rust\npub struct WarmUpConfig {\n    pub strategy: WarmUpStrategy,\n    pub max_bytes: usize,        // Budget: don't prefault more than this (default: 256MB)\n    pub parallel_readers: usize, // Concurrent prefault threads (default: 2)\n}\n\npub enum WarmUpStrategy {\n    None,                        // No prefaulting (current behavior)\n    Full,                        // Touch every page sequentially\n    Header,                      // Prefault header + record table only (smallest footprint)\n    Adaptive(AdaptiveConfig),    // Heat-map based intelligent prefaulting\n}\n\npub struct AdaptiveConfig {\n    pub heat_decay: f64,         // Exponential decay factor (default: 0.95)\n    pub min_heat: f64,           // Minimum heat to prefault (default: 0.1)\n}\n```\n\n### Adaptive Strategy\n\nThe adaptive strategy learns which pages are actually accessed during typical queries:\n\n1. Maintain a per-page heat map: 1 bit per 4KB page = 32KB overhead for a 1GB index\n2. Increment heat on every page access (detected via access pattern tracking at the VectorIndex level)\n3. Heat decays exponentially per search cycle: heat *= heat_decay\n4. On warm-up, prefault pages above min_heat threshold in heat-descending order (hottest pages first)\n5. Stop when max_bytes budget is exhausted\n\nThis means after a few search cycles, the system learns that (for example) the header, record table, and the first 30% of vector slabs are \"hot\" and should be prefaulted, while the remaining 70% of vectors (rarely-accessed tail) can be faulted on demand.\n\n### OS Integration\n\n- Linux: `madvise(MADV_WILLNEED)` to request kernel prefaulting\n- macOS: `madvise(MADV_WILLNEED)` (same API, different kernel behavior)\n- Fallback: sequential read through targeted pages for portability on other platforms\n\n## Justification\n\nIn production, frankensearch indices are memory-mapped for zero-copy access. But the first search after process start (or after memory pressure evicts pages) is dramatically slower due to page faults. Prefaulting eliminates this variance.\n\nThe adaptive strategy is key: rather than always prefaulting the entire index (wasteful for large indices where only a fraction of pages are typically accessed), we learn which pages are actually accessed during typical queries and only prefault those. This provides the benefits of full prefaulting (consistent latency) with a fraction of the memory and I/O cost.\n\n## Considerations\n\n- Memory pressure: prefaulting competes with other processes for page cache space. The max_bytes budget prevents frankensearch from evicting other processes' pages.\n- Background prefaulting: warm-up should run in background threads so it doesn't block the first search. If a search arrives before warm-up completes, it proceeds normally (with possible page faults for non-prefaulted pages).\n- Multiple indices: when multiple FSVI indices are open (e.g., fast + quality embeddings), warm-up should respect the total max_bytes budget across all indices.\n- Interaction with S3-FIFO cache (bd-l7v): the cache and prefaulting are complementary — cache handles repeated access patterns, prefaulting handles cold start.\n\n## Testing\n\n- [ ] Unit: WarmUpStrategy::None is a no-op (no system calls)\n- [ ] Unit: WarmUpStrategy::Header only touches header + record table bytes\n- [ ] Unit: heat map increment/decay math is correct\n- [ ] Unit: max_bytes budget is respected (never prefault more than budget)\n- [ ] Unit: heat map size calculation for various index sizes\n- [ ] Integration: verify cold-start latency improvement (before/after warm-up, measure p50/p99)\n- [ ] Integration: verify warm-up completes within reasonable time (< 2s for 256MB budget)\n- [ ] Benchmark: prefault overhead vs cold search latency savings (quantify the tradeoff)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T22:00:30.949232738Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:20.500063416Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1co","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T22:02:20.353734149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1co","depends_on_id":"bd-l7v","type":"blocks","created_at":"2026-02-13T22:02:20.500001501Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1cr","title":"Implement robust statistics primitives for search monitoring","description":"Implement robust statistics primitives for search monitoring that are stable under outliers and heavy-tailed latency distributions. Replace mean/stddev with Median+MAD, Huber M-estimator, and streaming t-digest for zero-allocation quantile estimation.\n\nGraveyard entry: §12.15 Robust Statistics Primitives + §9.4 Sketching & Streaming\nEV score: 48 (Impact=3, Confidence=4, Reuse=4, Effort=1, Friction=1)\nPriority tier: A\n\nArchitecture:\npub struct RobustMetrics {\n    tdigest: TDigest,           // Streaming quantiles (any percentile)\n    median_mad: MedianMAD,      // Robust center + spread\n    huber: HuberEstimator,      // Outlier-resistant mean\n    count: u64,\n    last_reset: Instant,\n}\n\nComponents:\n\n1. TDigest (streaming quantile estimation):\n   - Compression parameter: 100 (default)\n   - Memory: ~4KB per metric stream\n   - Update: O(log delta) per observation\n   - Query: any quantile in O(delta) — p50, p90, p95, p99, p999\n   - Merge: two t-digests can be merged (for per-thread → global aggregation)\n   - Use existing `tdigest` crate (or implement ~200 lines)\n\n2. Median + MAD (Median Absolute Deviation):\n   - Robust center: median (breakdown point 50%)\n   - Robust spread: MAD = median(|x_i - median|) * 1.4826\n   - Requires sorted window; use circular buffer of last N observations (N=1000)\n\n3. Huber M-estimator:\n   - Iteratively reweighted least squares with tuning constant k=1.345\n   - Breakdown point: min(k, 1-k) ≈ 20%\n   - For normally-distributed data, converges to mean (no regression)\n   - Streaming variant: exponentially weighted Huber mean\n\n4. HyperLogLog (cardinality estimation):\n   - Estimate unique queries, unique doc_ids in results\n   - Memory: 12KB for <2% error\n   - Use existing `hyperloglog` crate\n\nIntegration into TwoTierMetrics (bd-3un.24):\n- Replace raw latency fields with RobustMetrics\n- TwoTierMetrics.fast_latency → RobustMetrics (t-digest p50/p90/p99)\n- TwoTierMetrics.refine_latency → RobustMetrics\n- TwoTierMetrics.query_count → HyperLogLog (unique queries)\n\nConcurrency: Per-thread RobustMetrics with periodic merge (lock-free via atomic swap of TDigest).\n\nBudgeted mode: <500ns per metric update. Memory: ~4KB per metric × ~6 metrics = ~24KB total.\n\nFallback: Raw metric recording (f64 values) — zero impact on search correctness.\n\nFile: frankensearch-core/src/metrics.rs\n\nReference: Dunning & Ertl (2019) \"t-digest\", Huber (1981) \"Robust Statistics\", Flajolet et al. (2007) \"HyperLogLog\"\nBaseline comparator: mean/stddev (current), P2 quantile estimator (bd-3un.24 comment — t-digest is more flexible: any quantile, not just predetermined)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:45:39.859430152Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:40.259959692Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["graveyard","metrics","monitoring","phase7"],"dependencies":[{"issue_id":"bd-1cr","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.346608715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cr","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:46:07.593489045Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":83,"issue_id":"bd-1cr","author":"Dicklesworthstone","text":"REFINEMENT PASS 1 — Architecture clarifications and integration notes:\n\n1. CRATE DEPENDENCY: Consider using the `tdigest` crate (pure Rust, no unsafe) rather than implementing from scratch. Check: crates.io/crates/tdigest. If it doesn't meet our needs (missing merge, wrong API), implement inline (~200 lines). For HyperLogLog, check `hyperloglogplus` crate.\n\n2. INTEGRATION WITH TwoTierMetrics: This bead's RobustMetrics struct is designed to DROP INTO the existing TwoTierMetrics struct (bd-3un.24). Specifically:\n   - TwoTierMetrics.fast_latency: Duration -> RobustMetrics (streaming quantiles)\n   - TwoTierMetrics.quality_latency: Duration -> RobustMetrics\n   - TwoTierMetrics.fusion_latency: Duration -> RobustMetrics\n   - TwoTierMetrics.unique_queries: HyperLogLog\n   The public API remains the same (latency getters return Duration), but internally we now track robust aggregates.\n\n3. REPORTING: Add a report() method that emits a tracing::info! span with:\n   - p50, p90, p95, p99, p999 latencies (from t-digest)\n   - Median + MAD center/spread (from MedianMAD)\n   - Huber robust mean (from HuberEstimator)\n   - Unique query count estimate (from HyperLogLog)\n   This integrates naturally with bd-3un.39 (structured tracing).\n\n4. NO DEPENDENCY ON bd-3un.24 NEEDED: This bead defines the primitives in frankensearch-core/src/metrics.rs. The TwoTierSearcher (bd-3un.24) consumes these primitives but doesn't need to exist first. The metrics module is standalone — it only needs error types (bd-3un.2).\n\n5. TEST REQUIREMENTS (covered by bd-3un.31): Add tests for:\n   - TDigest: insert 10K normal samples, verify p50 within 1% of true median\n   - TDigest: merge two digests, verify quantiles match combined dataset\n   - MedianMAD: known dataset [1,2,3,4,100] → median=3, MAD=1.4826\n   - HuberEstimator: normal data converges to mean; contaminated data resists outliers\n   - HyperLogLog: 10K unique strings → estimate within 5% of 10K\n   - Concurrent updates from 4 threads don't panic or lose data","created_at":"2026-02-13T20:51:36Z"},{"id":156,"issue_id":"bd-1cr","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — LIGHT REVISION (per-thread metrics merge):\n\nThe \"per-thread RobustMetrics with periodic merge (lock-free via atomic swap)\" pattern maps to asupersync's structured concurrency:\n\nBEFORE:\n  - Per-thread TDigest with atomic swap for lock-free merge\n  - Manual timer for periodic merge\n\nAFTER:\n  - Per-task RobustMetrics (each asupersync task has its own)\n  - asupersync::sync::Mutex or asupersync::channel::watch for periodic aggregation\n  - Merge triggered by region close (structured guarantee: all task metrics are collected)\n\n  pub struct MetricsCollector {\n      global: asupersync::sync::Mutex<RobustMetrics>,\n      merge_interval: Duration,\n  }\n\n  impl MetricsCollector {\n      /// Periodic merge task (runs in a region alongside search tasks)\n      pub async fn merge_loop(&self, cx: &Cx) {\n          loop {\n              cx.sleep(self.merge_interval).await;\n              if cx.is_cancel_requested() { break; }\n              // Merge per-task metrics into global\n              let mut global = self.global.lock(cx).await.unwrap();\n              global.merge_tdigests();\n          }\n      }\n  }\n\nNo fundamental architecture change — just using asupersync sync primitives instead of std. The lock-free atomic swap pattern is fine to keep if benchmarks show it's faster.","created_at":"2026-02-13T21:06:24Z"},{"id":242,"issue_id":"bd-1cr","author":"Dicklesworthstone","text":"REVIEW FIX — Mathematical corrections and performance reconciliation:\n\n1. HUBER BREAKDOWN POINT CORRECTION: The body states \"Breakdown point: min(k, 1-k) ≈ 20%\". This is INCORRECT. The Huber M-estimator of location has a breakdown point of 1/(n+1) → 0% as n→∞. It has a bounded influence function (which is different from breakdown point). The 1.345 tuning constant gives 95% asymptotic efficiency under normality.\n\n   CORRECTED TEXT: \"The Huber M-estimator with k=1.345 has a bounded influence function (robust to individual outliers) and 95% asymptotic efficiency under normality. Breakdown point is 0% (like all M-estimators of location without scale). For true breakdown robustness, pair with MAD scale estimate.\"\n\n   For this use case (latency monitoring with occasional outliers), bounded influence is sufficient — we don't expect 50% of observations to be corrupted.\n\n2. MEDIANMAD PERFORMANCE RECONCILIATION: Computing exact median on a circular buffer of N=1000 requires O(N log N) sorting or O(N) quickselect — both exceed the \"<500ns per update\" budget.\n\n   FIX: Use the t-digest (already in RobustMetrics) for median estimation instead of maintaining a separate sorted window. T-digest p50 is accurate to within ~0.5% and update is O(log delta) ≈ O(1) amortized.\n\n   REVISED MedianMAD:\n   pub struct MedianMAD {\n       tdigest: TDigest,  // REUSE the same t-digest for median\n       // MAD = median(|x_i - median|) — also computed via a second t-digest\n       deviation_tdigest: TDigest,\n   }\n\n   This eliminates the separate circular buffer and meets the <500ns budget.\n\n3. HYPERLOGLOG PRECISION: Add \"precision parameter p=14 (16,384 registers, ~12KB, standard error 0.81%)\" to the body.\n\n4. TEST REQUIREMENT ADDITIONS:\n   - Huber: verify bounded influence (inserting one extreme outlier shifts estimate by < 1 MAD)\n   - MedianMAD via t-digest: p50 within 1% of true median on 10K normal samples\n   - MedianMAD: MAD within 2% of true MAD on 10K normal samples\n   - Performance: 10K metric updates complete in < 5ms total (< 500ns amortized)","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-1do","title":"Quality-Tier Circuit Breaker","description":"Implement a circuit breaker pattern for the quality tier (Phase 2) of the TwoTierSearcher. When the quality tier is consistently slow, failing, or not improving results, the circuit breaker trips and subsequent queries skip Phase 2 entirely, returning only Phase 1 (fast) results until the circuit resets.\n\nThis is a simpler, more pragmatic alternative to the full sequential testing gate (bd-2ps). While bd-2ps uses e-processes for statistically rigorous decisions, this circuit breaker uses simple health metrics for operational reliability.\n\n## Design\n\n```rust\npub struct CircuitBreakerConfig {\n    pub enabled: bool,                    // Default: true\n    pub failure_threshold: u32,           // Consecutive failures to trip (default: 5)\n    pub latency_threshold_ms: u64,        // Quality tier latency to count as \"slow\" (default: 500ms)\n    pub improvement_threshold: f64,       // Min Kendall tau improvement to count as \"useful\" (default: 0.05)\n    pub half_open_interval_ms: u64,       // Try quality tier again after this (default: 30000ms)\n    pub reset_threshold: u32,             // Consecutive successes in half-open to close (default: 3)\n}\n\npub enum CircuitState {\n    Closed,     // Normal: quality tier active\n    Open,       // Tripped: skip quality tier\n    HalfOpen,   // Testing: try quality tier on next query\n}\n\npub struct CircuitBreaker {\n    state: AtomicU8,\n    consecutive_failures: AtomicU32,\n    consecutive_successes: AtomicU32,\n    last_trip_time: AtomicU64,\n    metrics: CircuitMetrics,\n}\n\npub struct CircuitMetrics {\n    pub trips: u64,\n    pub resets: u64,\n    pub queries_skipped: u64,\n    pub avg_skip_savings_ms: f64,\n}\n```\n\n## Failure Conditions (any one triggers a failure count)\n\n1. Quality tier exceeds latency_threshold_ms\n2. Quality tier returns an error\n3. Quality tier results have Kendall tau < improvement_threshold vs fast results (not improving ranking)\n\n## State Machine\n\n- **Closed → Open**: after failure_threshold consecutive failures\n- **Open → HalfOpen**: after half_open_interval_ms\n- **HalfOpen → Closed**: after reset_threshold consecutive successes\n- **HalfOpen → Open**: on any failure\n\n## Why This Matters\n\nThe quality tier (MiniLM-L6-v2 at ~128ms) is the largest latency contributor. In some scenarios — system under load, model warm-up, or queries where fast-tier results are already excellent — the quality tier adds latency without improving results. The circuit breaker automatically adapts, giving consumers consistently fast responses when quality refinement isn't helping.\n\nThis pairs with bd-1cr (robust statistics) for computing rolling latency percentiles and with the existing TwoTierMetrics for Kendall tau calculation.\n\n## Testing\n\n- Unit: circuit starts Closed\n- Unit: consecutive failures trip to Open\n- Unit: timeout transitions to HalfOpen\n- Unit: successes in HalfOpen reset to Closed\n- Unit: failure in HalfOpen returns to Open\n- Unit: metrics tracking (trips, resets, queries_skipped)\n- Integration: circuit breaker with simulated slow quality tier\n- Integration: verify no quality tier calls when Open\n- Benchmark: circuit breaker overhead (should be <1μs per query)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T22:00:59.100227145Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:32.013413816Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1do","depends_on_id":"bd-1cr","type":"blocks","created_at":"2026-02-13T22:02:32.013357480Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1do","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T22:02:31.906972454Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1hw","title":"Incremental Append-Only FSVI Index Updates","description":"Implement WAL-based append-only mutation for the FSVI vector index format. Currently, any document addition requires a full index rebuild. This is the #1 usability friction point for frankensearch consumers.\n\n## Background\n\nThe FSVI (FrankenSearch Vector Index) format is frankensearch's custom binary vector index. It stores document embeddings in a compact, memory-mappable layout optimized for top-k cosine similarity search. However, the current design is fully immutable: adding a single document requires reading the entire index, appending the new record in memory, and writing a completely new index file. For large indices (e.g., xf with 50K+ tweets), this means re-processing the entire corpus on every addition.\n\n## Design\n\n### Append Buffer (WAL)\nNew vectors are appended to a separate `.fsvi.wal` file. This file uses the same binary layout as the main index (raw records + vector slabs) but without the header. This keeps the WAL format trivially compatible with the main index reader.\n\n### Search Merges\n`top_k_search` reads both the main index and the WAL, merges results via a unified BinaryHeap. The WAL is small relative to the main index, so the merge overhead is minimal.\n\n### Compaction\nWhen the WAL exceeds a configurable threshold (default: 10% of main index size or 1000 records), compact by rewriting main index + WAL into a new main index. This is a background operation that does not block reads.\n\n### Atomic Swap\nCompaction writes to `.fsvi.new`, then `rename()` over the old file (atomic on POSIX). This ensures readers never see a partially-written index.\n\n### fsync Discipline\nWAL append is fsync'd per batch (not per record) for durability without excessive I/O. Single appends are batches of size 1.\n\n## API Surface\n\n```rust\nimpl VectorIndex {\n    pub fn append(&mut self, doc_id: &str, vector: &[f32]) -> Result<(), SearchError>;\n    pub fn append_batch(&mut self, entries: &[(String, Vec<f32>)]) -> Result<(), SearchError>;\n    pub fn compact(&mut self) -> Result<CompactionStats, SearchError>;\n    pub fn needs_compaction(&self) -> bool;\n    pub fn wal_record_count(&self) -> usize;\n}\n```\n\n## Performance Targets\n\n- Single append: <100us (excluding fsync)\n- Batch append (100 docs): <5ms\n- Search overhead from WAL merge: <10% vs main-only search (for WAL size < 10% of main)\n\n## Justification\n\nWithout incremental append, every consumer must rebuild the entire index when adding a single document. For xf (50K+ tweets), this means re-embedding everything. Append-only makes frankensearch practical for live, growing datasets. This is the single highest-impact usability improvement for all three consumers (cass, xf, mcp_agent_mail_rust).\n\n## Considerations\n\n- WAL file must be crash-safe: partial writes should be detectable (length check + CRC per batch)\n- Compaction must not block concurrent reads\n- Memory-mapped WAL: for small WALs, mmap is fine; for large WALs approaching compaction threshold, sequential read may be preferred\n- Interaction with soft-delete tombstones: WAL entries can also be tombstoned, and compaction naturally removes them\n\n## Testing\n\n- [ ] Unit: append single vector, verify searchable immediately\n- [ ] Unit: append batch, verify all vectors searchable\n- [ ] Unit: compaction reduces file count, search results unchanged\n- [ ] Unit: WAL threshold detection (needs_compaction)\n- [ ] Unit: wal_record_count accuracy\n- [ ] Integration: concurrent append + search (verify no corruption)\n- [ ] Integration: crash recovery (partial WAL write detection)\n- [ ] Benchmark: append latency (single and batch)\n- [ ] Benchmark: search overhead vs WAL size (1%, 5%, 10%)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:58:09.662893964Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:27.765231044Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1hw","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T22:01:54.353544279Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hw","depends_on_id":"bd-3un.28","type":"blocks","created_at":"2026-02-13T22:01:57.555118045Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hw","depends_on_id":"bd-sot","type":"blocks","created_at":"2026-02-13T22:05:27.765177463Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-21g","title":"Implement adaptive fusion parameters via Bayesian online learning","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:31:19.664609739Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:54.177361270Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","bayesian","fusion","phase6"],"dependencies":[{"issue_id":"bd-21g","depends_on_id":"bd-3un","type":"blocks","created_at":"2026-02-13T20:31:46.264499419Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:50:53.974355938Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T20:31:37.086376331Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.21","type":"blocks","created_at":"2026-02-13T20:31:37.166256878Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.22","type":"blocks","created_at":"2026-02-13T20:31:37.246866950Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T21:50:54.077311143Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.43","type":"blocks","created_at":"2026-02-13T21:50:54.177292541Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T21:50:53.872152980Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":35,"issue_id":"bd-21g","author":"Dicklesworthstone","text":"Implement Bayesian online learning for adaptive fusion parameters (RRF K constant and blend factor). Instead of fixed constants, maintain conjugate priors that update from implicit relevance feedback.\n\nMATHEMATICAL FOUNDATION:\n\n1. Adaptive Blend Factor via Beta-Bernoulli Model:\n   - Prior: Beta(7, 3) encoding the initial 0.7 blend factor\n   - Update: observe whether quality-tier reranking improved results (via click/dwell signals)\n   - Posterior: Beta(7 + successes, 3 + failures)\n   - Blend factor = E[posterior] = alpha / (alpha + beta)\n   - Thompson sampling variant: sample blend factor from posterior for exploration\n\n2. Adaptive RRF K via Gamma-Normal Model:\n   - Prior: Gamma(60, 1) encoding the K=60 default\n   - Update: observe fusion quality (Kemeny distance between fused ranking and click-derived ranking)\n   - Posterior update via conjugate machinery\n   - K = E[posterior]\n\n3. Per-Query-Class Adaptation:\n   - Maintain separate Beta posteriors per query classification (from bd-3un.43):\n     Short keyword queries may prefer higher K (uniform weighting)\n     Natural language queries may prefer lower K (top-heavy weighting)\n     Identifier queries may prefer blend_factor closer to 0.0 (fast tier is fine)\n\n4. Evidence Ledger:\n   - Every search emits a structured record:\n     query_hash, query_class, blend_factor_used, k_used, fast_ndcg, quality_ndcg, blended_ndcg, rank_correlation\n   - Enables offline analysis and posterior initialization for new deployments\n\n5. Regret Bound:\n   - Under Beta-Bernoulli Thompson sampling: E[Regret(T)] is O(sqrt(T log T))\n   - Provably converges to optimal parameters\n\nImplementation:\n\npub struct AdaptiveFusionParams {\n    blend_alpha: AtomicF64,   // Beta posterior for blend factor\n    blend_beta: AtomicF64,\n    k_alpha: AtomicF64,       // Gamma posterior for RRF K\n    k_beta: AtomicF64,\n    min_samples: usize,       // Don't adapt until N queries (default: 50)\n}\n\nFile: frankensearch-fusion/src/adaptive.rs\n\nThis is a pure addition. The fixed defaults remain as the zero-observation case. No API changes needed; TwoTierConfig gains an optional adaptive_params field.\n\nAlien-artifact characteristics:\n- Mathematical rigor: conjugate Bayesian posteriors with formal update rules\n- Explainability: evidence ledger tracks every decision\n- Formal guarantees: Thompson sampling regret bound\n- Graceful degradation: falls back to fixed defaults with insufficient data\n- Operational excellence: O(1) per query, two floats of state per parameter\n","created_at":"2026-02-13T20:31:30Z"},{"id":243,"issue_id":"bd-21g","author":"Dicklesworthstone","text":"REVIEW FIX — Mathematical corrections and missing infrastructure:\n\n1. GAMMA-NORMAL CONJUGACY ERROR: The body claims \"Adaptive RRF K via Gamma-Normal Model\" with \"Prior: Gamma(60, 1).\" The Gamma distribution is conjugate to Poisson/Exponential likelihoods, NOT to a Normal likelihood for the location parameter. RRF K is a location parameter (optimal value around 60), not a precision parameter.\n\n   FIX: Use a Normal-Normal model instead:\n   - Prior: N(60, 10²) — mean 60, std dev 10 (encoding uncertainty around K=60)\n   - Likelihood: Each observed \"optimal K\" from NDCG evaluation is N(K_true, sigma²)\n   - Posterior: N(mu_n, sigma²_n) with standard conjugate update\n\n   pub struct AdaptiveK {\n       mu: f64,        // Posterior mean (starts at 60.0)\n       sigma_sq: f64,  // Posterior variance (starts at 100.0)\n       sigma_obs: f64, // Observation noise (estimated or fixed, e.g., 15.0)\n       n: u64,         // Number of observations\n   }\n\n   impl AdaptiveK {\n       pub fn update(&mut self, observed_optimal_k: f64) {\n           // Normal-Normal conjugate update\n           let precision_prior = 1.0 / self.sigma_sq;\n           let precision_obs = 1.0 / (self.sigma_obs * self.sigma_obs);\n           let precision_post = precision_prior + precision_obs;\n           self.mu = (precision_prior * self.mu + precision_obs * observed_optimal_k) / precision_post;\n           self.sigma_sq = 1.0 / precision_post;\n           self.n += 1;\n       }\n       pub fn sample(&self) -> f64 {\n           // Thompson sampling from posterior\n           // sample ~ N(mu, sigma_sq)\n           let z: f64 = standard_normal_sample();\n           (self.mu + self.sigma_sq.sqrt() * z).max(1.0)  // K must be >= 1\n       }\n   }\n\n2. AtomicF64 FIX: Use bit-cast pattern since std has no AtomicF64:\n   use std::sync::atomic::{AtomicU64, Ordering};\n\n   fn atomic_load_f64(a: &AtomicU64, order: Ordering) -> f64 {\n       f64::from_bits(a.load(order))\n   }\n   fn atomic_store_f64(a: &AtomicU64, val: f64, order: Ordering) {\n       a.store(val.to_bits(), order);\n   }\n\n   Or add `atomic_float = \"1\"` to workspace deps for a cleaner API.\n\n3. MISSING DEPENDENCIES — Add:\n   - bd-3un.5 (ScoredResult, FusedHit types used in evidence evaluation)\n   - bd-3un.2 (SearchError for error handling)\n   - bd-3un.38 (test fixture corpus for evidence/calibration)\n   - bd-3un.43 (query classification — referenced for per-class posteriors)\n\n4. DEPENDENCY TYPE FIX: bd-3un should be parent-child, not blocks.\n\n5. TEST REQUIREMENTS:\n   - Posterior convergence: 100 observations of K=80 → posterior mean converges to ~80\n   - Prior recovery: zero observations → mu = 60, sigma_sq = 100\n   - Per-query-class independence: update Identifier class, NL class unchanged\n   - Thompson sampling: 1000 samples from N(60, 100) → mean ≈ 60, all > 0\n   - Blend factor: 100 successes → blend converges toward 1.0\n   - Evidence ledger: each update emits a structured tracing record\n   - AtomicF64 round-trip: store then load preserves exact value","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-22k","title":"Implement score calibration service (Platt/isotonic/temperature)","description":"Implement a ScoreCalibrator trait and calibration layer that converts heterogeneous raw model scores (BM25 [0,inf), cosine [-1,1], reranker logits (-inf,inf)) into calibrated probabilities [0,1] before fusion. This makes blend_factor and RRF score combination mathematically meaningful.\n\nGraveyard entry: §12.16 Calibration Service Abstraction\nEV score: 50 (Impact=5, Confidence=4, Reuse=5, Effort=2, Friction=1)\nPriority tier: A\n\nArchitecture:\npub trait ScoreCalibrator: Send + Sync {\n    fn calibrate(&self, raw_score: f64) -> f64;  // raw -> [0,1] probability\n    fn calibrate_batch(&self, scores: &mut [f64]);\n    fn ece(&self) -> f64;  // Expected Calibration Error\n    fn name(&self) -> &str;\n}\n\nImplementations:\n1. Identity — passthrough (default, backward-compatible)\n2. TemperatureScaling — single parameter T: calibrated = sigmoid(score / T)\n   - T learned offline via NLL minimization on validation set\n   - O(1) per score, <10ns overhead\n3. PlattScaling — logistic regression: calibrated = sigmoid(a * score + b)\n   - Parameters (a, b) fit offline via L-BFGS on held-out data\n   - O(1) per score\n4. IsotonicRegression — non-parametric monotonic mapping\n   - Learned offline: piecewise-constant monotone function\n   - O(log n) per score (binary search on breakpoints)\n   - Guaranteed monotonic (preserves ranking order within each source)\n\nIntegration points:\n- Before RRF fusion (bd-3un.20): calibrate lexical + semantic scores\n- Before two-tier blending (bd-3un.21): calibrate fast + quality scores\n- After reranking (bd-3un.26): calibrate reranker output (replaces raw sigmoid)\n\nCalibration training (offline):\n- Use test fixture corpus (bd-3un.38) as calibration set\n- For each score source: fit calibrator on (raw_score, relevance_label) pairs\n- Store calibration parameters as JSON artifact (sha256 signed)\n- Load at search time; recalibrate periodically\n\nMonitoring:\n- ECE (Expected Calibration Error): partition [0,1] into 10 bins, measure |avg_confidence - accuracy| per bin\n- Brier score: mean squared error of calibrated probabilities vs relevance\n- ECE > 0.10 for 5 windows → automatic fallback to Identity + trigger retrain\n\nBudgeted mode: <1us per score calibration. Memory: ~1KB for isotonic breakpoints.\n\nIsomorphism proof: Isotonic regression is monotonic by construction → calibrated scores preserve original ranking order within each source. Prove: for all i<j, raw[i] <= raw[j] implies calibrated[i] <= calibrated[j].\n\nFile: frankensearch-fusion/src/calibration.rs\n\nReference: Platt (1999) \"Probabilistic outputs for SVMs\", Zadrozny & Elkan (2002) \"Transforming classifier scores\", Guo et al. (2017) \"On Calibration of Modern Neural Networks\"\nBaseline comparator: Raw score passthrough (current), naive min-max (bd-3un.19)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:45:35.751761205Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:41.103656603Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["calibration","fusion","graveyard","phase6"],"dependencies":[{"issue_id":"bd-22k","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.262027302Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.19","type":"blocks","created_at":"2026-02-13T20:46:04.380803839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:05:50.586803172Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T21:05:51.792699682Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.21","type":"blocks","created_at":"2026-02-13T21:22:20.769790571Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.26","type":"blocks","created_at":"2026-02-13T21:22:21.043156501Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:50:00.537623549Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:46:04.463501565Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":82,"issue_id":"bd-22k","author":"Dicklesworthstone","text":"REFINEMENT PASS 1 — Architecture clarifications and integration notes:\n\n1. DEPENDENCY JUSTIFICATION: This bead depends on bd-3un.19 (normalization) because calibration REPLACES naive min-max normalization as the score preparation step. Once calibration is available, the fusion pipeline should prefer calibrated scores over normalized scores. The normalization module still exists as a simpler fallback and for contexts where calibration data isn't available.\n\n2. RELATIONSHIP WITH bd-21g (Bayesian adaptive fusion): Calibrated scores make the Bayesian posterior updates in bd-21g more meaningful. When scores are properly calibrated to [0,1] probabilities, the Beta-Bernoulli blend factor updates have clearer semantics (success = calibrated quality score > calibrated fast score). This is a soft dependency — bd-21g works without calibration but works BETTER with it.\n\n3. CALIBRATION TRAINING PIPELINE: The offline calibration training should be a standalone binary/script (lives in examples/ or tools/) that:\n   a. Loads test fixture corpus (bd-3un.38)\n   b. Runs each scorer (BM25, fast cosine, quality cosine, reranker) on all query-doc pairs\n   c. Fits calibrators using known relevance labels\n   d. Serializes calibration parameters to JSON\n   e. Writes to data_dir/calibration/<scorer_id>.json\n   The CalibrationLayer loads these at search time.\n\n4. ONLINE TEMPERATURE SCALING: For production use without offline calibration data, TemperatureScaling can be fit online using a simple gradient descent on the last N queries (where N=100). This is the recommended default for new deployments.\n\n5. TEST REQUIREMENTS (covered by bd-3un.31): Add tests for:\n   - Identity calibrator preserves scores exactly\n   - Temperature scaling with T=1.0 equals sigmoid\n   - Platt scaling monotonicity (higher raw score = higher calibrated score)\n   - Isotonic regression monotonicity guarantee\n   - ECE computation correctness on known distribution\n   - Batch calibration matches sequential calibration\n   - Round-trip: serialize calibrator params to JSON and reload","created_at":"2026-02-13T20:51:35Z"},{"id":219,"issue_id":"bd-22k","author":"Dicklesworthstone","text":"REVISION (review pass 5 - dependency corrections):\n\n1. ADDED bd-3un.21 (blend) and bd-3un.26 (rerank) DEPENDENCIES: The bead body explicitly lists three integration points:\n   - \"Before RRF fusion (bd-3un.20)\": already a dependency\n   - \"Before two-tier blending (bd-3un.21)\": was MISSING, now added\n   - \"After reranking (bd-3un.26)\": was MISSING, now added\n\n   Calibration must understand the output format and score ranges of both blend and rerank steps to correctly transform scores.\n\n2. ASUPERSYNC NOTE: Per-score calibration (calibrate/calibrate_batch) is pure computation: O(1) per score, <10ns. No async needed. The offline training pipeline (fitting Platt/isotonic on calibration corpus) could optionally accept &Cx for cancellation during long batch jobs, but is not required for V1.\n","created_at":"2026-02-13T21:23:35Z"},{"id":249,"issue_id":"bd-22k","author":"Dicklesworthstone","text":"REVIEW FIX — ECE monitoring window and temperature scaling scope:\n\n1. ECE MONITORING WINDOW: The body says \"ECE > 0.10 for 5 windows → fallback\" but never defines a window. Define:\n   - Window = 500 queries (configurable via FRANKENSEARCH_CALIBRATION_WINDOW env var)\n   - ECE is computed per-window on the last 500 scored results\n   - 5 consecutive windows exceeding threshold = 2500 queries of poor calibration\n\n2. TEMPERATURE SCALING SCOPE: Temperature scaling (calibrated = sigmoid(score / T)) is most appropriate for:\n   - Reranker output (cross-encoder logits) — ideal use case\n   - Cosine similarity scores (bounded [-1, 1]) — acceptable\n\n   It is LESS appropriate for:\n   - Raw BM25 scores (unbounded [0, ∞)) — normalize to [0, 1] first\n   - RRF scores (small values ~0.01-0.03) — Platt scaling is better here\n\n   Add a recommendation: use TemperatureScaling for reranker, PlattScaling for BM25/RRF, IsotonicRegression for any source when sufficient calibration data exists.\n\n3. PER-QUERY-CLASS CALIBRATION NOTE: Different query classes (Identifier vs NaturalLanguage) produce different score distributions. For best calibration:\n   - Fit separate calibrators per (source, query_class) pair\n   - This is a soft dependency on bd-3un.43 (query classification)\n   - For V1: single calibrator per source is sufficient\n   - For V2: per-query-class calibration with Mondrian-style partitioning","created_at":"2026-02-13T21:50:41Z"}]}
{"id":"bd-26e","title":"Typed Filter Predicates","description":"Add typed filter predicates that consumers can apply at search time to narrow results without modifying the index. Currently, filtering must happen post-search (wasteful) or by building separate indices (complex).\n\n## Background\n\nAll three consumer codebases (cass, xf, mcp_agent_mail_rust) need to filter search results by document type, date range, or custom predicates. Without built-in filter support, each consumer reimplements filtering post-hoc, which loses the performance benefits of early filtering. Post-filtering is particularly wasteful for selective filters: if only 5% of documents match, the search engine computes scores for 20x more documents than necessary.\n\n## Design\n\n### Filter Trait\n\n```rust\npub trait SearchFilter: Send + Sync {\n    fn matches(&self, doc_id: &str, metadata: Option<&serde_json::Value>) -> bool;\n    fn name(&self) -> &str;\n}\n```\n\n### Built-in Filters\n\n```rust\npub struct DocTypeFilter(HashSet<String>);              // Match doc_type field\npub struct DateRangeFilter(Option<i64>, Option<i64>);   // created_at range (unix timestamps)\npub struct BitsetFilter(HashSet<u64>);                  // Pre-computed doc_id_hash set\npub struct PredicateFilter(Box<dyn Fn(&str) -> bool + Send + Sync>);  // Arbitrary closure\n\npub struct FilterChain {\n    filters: Vec<Box<dyn SearchFilter>>,\n    mode: FilterMode,  // All (AND) or Any (OR)\n}\n\npub enum FilterMode {\n    All,  // AND: all filters must match\n    Any,  // OR: any filter matching is sufficient\n}\n```\n\n### Integration\n\n**Vector search (FSVI)**: Filters are applied DURING the top-k scan, not after. This is the key performance insight. When scanning vectors for top-k cosine similarity, we check the filter predicate for each candidate and skip non-matching records. This means if you want the top-10 results matching doc_type=\"tweet\", we scan vectors and skip non-tweets, rather than fetching top-100 and filtering to tweets. This is critical for performance when the filter is highly selective (e.g., <10% of documents match).\n\n**Lexical search (Tantivy)**: Filters translate directly to BooleanQuery clauses with MUST clauses. Tantivy handles these natively and efficiently via its query engine. DocTypeFilter becomes a TermQuery, DateRangeFilter becomes a RangeQuery.\n\n**RRF fusion**: For cross-source filters that operate on fused results, apply after fusion. This handles cases where the filter depends on combined metadata from multiple sources.\n\n## Justification\n\n- **cass**: needs to filter by session_id, message_type (user vs assistant), date range\n- **xf**: needs to filter by tweet_type (original, retweet, reply), author, date range\n- **mcp_agent_mail_rust**: needs to filter by sender, recipient, read/unread status, TTL expiration\n\nWithout built-in filter support, each consumer reimplements filtering post-hoc, losing early-exit performance benefits and duplicating logic across codebases.\n\n## Considerations\n\n- Filter cost: SearchFilter::matches should be cheap (O(1) ideally). Expensive filters should use BitsetFilter with pre-computed sets.\n- Metadata availability: vector search may not have metadata readily available. The doc_id-based path (matching on ID alone) is always available; metadata-based filtering requires the metadata to be stored alongside vectors or looked up from Tantivy.\n- Filter selectivity estimation: for very selective filters (<1% match rate), consider early termination heuristics to avoid scanning the entire index.\n- Interaction with WAL (Idea 1): filters must also apply to WAL records during merged search.\n\n## Testing\n\n- [ ] Unit: DocTypeFilter matches/rejects correctly\n- [ ] Unit: DateRangeFilter with None boundaries (open-ended ranges)\n- [ ] Unit: DateRangeFilter with both boundaries set\n- [ ] Unit: BitsetFilter with known hash set\n- [ ] Unit: FilterChain AND semantics (all must match)\n- [ ] Unit: FilterChain OR semantics (any match suffices)\n- [ ] Unit: PredicateFilter with arbitrary closure\n- [ ] Unit: empty FilterChain matches everything\n- [ ] Integration: filtered vector search returns only matching results\n- [ ] Integration: filtered lexical search translates to correct Tantivy query\n- [ ] Integration: cross-source filter applied after RRF fusion\n- [ ] Benchmark: early-exit filter vs post-filter performance (measure with 5%, 50%, 95% selectivity)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T21:59:28.961595886Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:23.461801334Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-26e","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T22:02:10.634284527Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26e","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T22:02:10.749906606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26e","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T22:02:10.526416954Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz","title":"Epic: Build frankensearch-fast-search (fsfs) as a standalone machine-wide search product","description":"Context:\nCreate a separate binary product, frankensearch-fast-search (fsfs), that turns frankensearch into a first-class standalone local search tool for entire-machine text corpora.\n\nProduct vision:\n- Default scope: user home directories with configurable roots/exclusions.\n- Index high-value textual artifacts (code/docs/config) while intelligently downranking or skipping low-value expensive sources (gigantic logs, vendored/generated artifacts, binaries).\n- Two primary UX surfaces:\n  1) Agent-first CLI mode with JSON + TOON output, stream-friendly and automation-native.\n  2) Deluxe FrankenTUI mode with powerful interactive search, explanations, and operational introspection inspired by ftui-demo showcase patterns.\n\nNon-negotiable engineering bar:\n- Insane performance under normal conditions and graceful degradation under host pressure.\n- Deterministic, auditable decision-making for expensive operations (embedding generation, scheduling, throttling).\n- Evidence-ledger-backed explainability for policy decisions and adaptive mode switches.\n- Comprehensive unit/property/integration/e2e/perf/soak validation with rich logging artifacts.\n\nAlien strategy anchors:\n- First-principles expected-loss decision contracts (action costs explicit).\n- Calibration/guard layers (conformal/e-process style) for adaptive controller safety.\n- Strict profile-first optimization loop with one-lever evidence discipline.","acceptance_criteria":"1) fsfs ships as a standalone binary with machine-wide corpus discovery and high-value indexing policies.\n2) Agent CLI mode supports JSON and TOON output with stable, automation-friendly contracts.\n3) Deluxe FrankenTUI mode delivers advanced interactive search, explanations, and operational controls.\n4) Adaptive resource governance provides measurable graceful degradation under CPU/memory/IO pressure.\n5) Evidence-ledger, testing, and performance gates are comprehensive and reproducible.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-13T22:00:40.703384748Z","created_by":"ubuntu","updated_at":"2026-02-13T22:00:44.672840298Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien","deluxe-tui","frankensearch","fsfs","machine-search","standalone"]}
{"id":"bd-2hz.1","title":"Workstream: First-principles product semantics, decision contracts, and safety model","description":"Goal:\nDefine fsfs decision semantics from first principles so all downstream behavior (what to index, when to embed, how to degrade) is mathematically explicit, auditable, and user-centered.\n\nScope:\n- mode contracts for agent CLI + deluxe TUI\n- expected-loss action framework\n- safety/privacy boundaries and deterministic fallback policy","acceptance_criteria":"1) Product semantics and mode contracts (CLI/TUI) are explicit and unambiguous.\n2) Expected-loss decision framework is defined for key expensive actions.\n3) Safety/privacy and deterministic fallback policies are documented and testable.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:10.433214291Z","created_by":"ubuntu","updated_at":"2026-02-13T22:01:23.271646477Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien","decision-contracts","fsfs","phase-foundation"],"dependencies":[{"issue_id":"bd-2hz.1","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:10.433214291Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.1.1","title":"Define fsfs dual-mode product contract (agent CLI + deluxe TUI)","description":"Task:\nDefine exact mode semantics, invariants, and shared capability boundaries for fsfs CLI and TUI.\n\nMust include:\n- command/query/result semantic parity between modes\n- explicit divergence policy where UX differs intentionally\n- output stability/versioning commitments for machine consumers\n- minimum discoverability and recovery behavior requirements for humans","acceptance_criteria":"1) CLI and TUI semantic parity boundaries are explicitly specified.\n2) Intentional mode divergences are documented with rationale.\n3) Contract is sufficient for downstream implementation without reinterpretation.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.243699482Z","created_by":"ubuntu","updated_at":"2026-02-13T22:03:40.769085032Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["contracts","fsfs","mode-contracts"],"dependencies":[{"issue_id":"bd-2hz.1.1","depends_on_id":"bd-2hz.1","type":"parent-child","created_at":"2026-02-13T22:02:01.243699482Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.1.2","title":"Build expected-loss action matrices for ingest/embed/degrade decisions","description":"Task:\nFormalize high-impact runtime decisions using explicit states/actions/losses and tie them to operational objectives.\n\nMust include:\n- action families: index now/later/skip, embed now/defer/disable, degrade/recover\n- cost asymmetry definitions (false include vs false exclude, latency vs quality, compute vs recall)\n- machine-readable decision contract fields for auditing and tests","acceptance_criteria":"1) Loss matrices exist for ingest/embed/degrade decision families.\n2) Action/state definitions are machine-readable and testable.\n3) Fallback triggers are encoded for high-risk decisions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.351131389Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:13.918888473Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["decision-theory","fsfs","loss-model"],"dependencies":[{"issue_id":"bd-2hz.1.2","depends_on_id":"bd-2hz.1","type":"parent-child","created_at":"2026-02-13T22:02:01.351131389Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.1.2","depends_on_id":"bd-2hz.1.1","type":"blocks","created_at":"2026-02-13T22:05:13.918844420Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.1.3","title":"Define scope/privacy/redaction boundaries for machine-wide search","description":"Task:\nSpecify hard boundaries for what fsfs may scan, persist, emit, and display across modes.\n\nMust include:\n- root scope defaults + explicit opt-in/opt-out semantics\n- sensitive path/data class handling\n- redaction behavior for logs, explain payloads, and replay artifacts\n- threat model notes for local multi-user environments","acceptance_criteria":"1) Scope defaults and opt-in/opt-out behavior are unambiguous.\n2) Sensitive-data handling and redaction obligations are explicit.\n3) Local threat model assumptions are documented and actionable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.464861897Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:14.033600531Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","privacy","safety"],"dependencies":[{"issue_id":"bd-2hz.1.3","depends_on_id":"bd-2hz.1","type":"parent-child","created_at":"2026-02-13T22:02:01.464861897Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.1.3","depends_on_id":"bd-2hz.1.1","type":"blocks","created_at":"2026-02-13T22:05:14.033553243Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.1.4","title":"Author alien recommendation contracts for fsfs adaptive controllers","description":"Task:\nCreate complete recommendation-contract cards for top fsfs adaptive subsystems (ingestion policy, degradation scheduler, ranking policy).\n\nEach card must include:\n- EV score, priority tier, adoption wedge\n- budgeted mode and fallback trigger\n- isomorphism proof plan and baseline comparator\n- repro artifact requirements and rollback plan","acceptance_criteria":"1) Recommendation contracts are complete for top adaptive subsystems.\n2) EV/risk/fallback/repro fields are filled with concrete values.\n3) Contracts are reusable by implementation and test planning tasks.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.575794678Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:14.253832158Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien","contracts","fsfs"],"dependencies":[{"issue_id":"bd-2hz.1.4","depends_on_id":"bd-2hz.1","type":"parent-child","created_at":"2026-02-13T22:02:01.575794678Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.1.4","depends_on_id":"bd-2hz.1.2","type":"blocks","created_at":"2026-02-13T22:05:14.143505360Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.1.4","depends_on_id":"bd-2hz.1.3","type":"blocks","created_at":"2026-02-13T22:05:14.253769511Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10","title":"Workstream: Comprehensive testing, deterministic simulation, and e2e logging","description":"Goal:\nProvide confidence via deep unit/property/integration/e2e/perf/soak coverage and first-class diagnostics.\n\nScope:\n- deterministic simulation harness\n- detailed e2e scripts and artifacts\n- failure replay and debugging workflows","acceptance_criteria":"1) Unit/property/integration/e2e/perf/soak coverage exists for fsfs critical paths.\n2) Deterministic simulation covers compute-pressure and degradation transitions.\n3) E2E failure artifacts enable fast replay and diagnosis.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:11.429515748Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:49.019759604Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","fsfs","phase-quality","testing"],"dependencies":[{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:11.429515748Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.2","type":"blocks","created_at":"2026-02-13T22:04:48.309146832Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.3","type":"blocks","created_at":"2026-02-13T22:04:48.380983455Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.4","type":"blocks","created_at":"2026-02-13T22:04:48.468364007Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.5","type":"blocks","created_at":"2026-02-13T22:04:48.574220947Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.6","type":"blocks","created_at":"2026-02-13T22:04:48.684938628Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.7","type":"blocks","created_at":"2026-02-13T22:04:48.796731922Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.8","type":"blocks","created_at":"2026-02-13T22:04:48.907222117Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.10","depends_on_id":"bd-2hz.9","type":"blocks","created_at":"2026-02-13T22:04:49.019715762Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10.1","title":"Create unit-test matrix across fsfs modules","description":"Task:\nDefine and implement unit-test coverage expectations per fsfs subsystem.\n\nMust include:\n- happy/edge/error path matrix\n- explicit invariant checks for decision logic\n- traceable mapping from tests to contracts","acceptance_criteria":"1) Unit-test matrix covers happy/edge/error cases across modules.\n2) Decision-contract invariants are directly asserted in tests.\n3) Coverage mapping enables gap analysis by subsystem.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.594184444Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.768563541Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","testing","unit"],"dependencies":[{"issue_id":"bd-2hz.10.1","depends_on_id":"bd-2hz.10","type":"parent-child","created_at":"2026-02-13T22:03:20.594184444Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10.2","title":"Add property and fuzz tests for parser/classifier/index invariants","description":"Task:\nDesign stochastic and property-based checks for robustness in critical data paths.\n\nMust include:\n- parser/config/schema fuzzing\n- classifier/index invariants\n- shrinkable minimal counterexample reporting","acceptance_criteria":"1) Property/fuzz targets cover high-risk parser/classifier/index paths.\n2) Counterexample shrinking/reporting is documented.\n3) Fuzz strategy integrates with CI/nightly execution model.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.712670467Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.876308726Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","fuzzing","property-tests"],"dependencies":[{"issue_id":"bd-2hz.10.2","depends_on_id":"bd-2hz.10","type":"parent-child","created_at":"2026-02-13T22:03:20.712670467Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10.3","title":"Build deterministic simulation harness for pressure/degradation controllers","description":"Task:\nCreate deterministic simulation environment for adaptive controller validation.\n\nMust include:\n- synthetic workload and pressure scenario generator\n- deterministic timing/state replay\n- oracle checks for transition correctness and fallback triggers","acceptance_criteria":"1) Deterministic simulation scenarios cover pressure and degradation transitions.\n2) Replay determinism is sufficient for debugging controller failures.\n3) Oracle checks validate transition and fallback correctness.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.828357751Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.985637437Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deterministic-sim","fsfs","testing"],"dependencies":[{"issue_id":"bd-2hz.10.3","depends_on_id":"bd-2hz.10","type":"parent-child","created_at":"2026-02-13T22:03:20.828357751Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10.4","title":"Build CLI-mode e2e scripts with rich diagnostics and artifacts","description":"Task:\nImplement end-to-end scripts for agent CLI flows with detailed, structured logging.\n\nMust include:\n- index/search/explain/degrade command scenarios\n- artifact capture (logs, traces, manifests, outputs)\n- replay guidance for failed scenarios","acceptance_criteria":"1) CLI e2e suite covers index/search/explain/degrade flows end-to-end.\n2) Diagnostic artifacts are comprehensive and reproducible.\n3) Failure output includes direct replay guidance.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.943614248Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.093860026Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","fsfs","logging"],"dependencies":[{"issue_id":"bd-2hz.10.4","depends_on_id":"bd-2hz.10","type":"parent-child","created_at":"2026-02-13T22:03:20.943614248Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10.5","title":"Build deluxe TUI e2e interaction suite with deterministic replay","description":"Task:\nImplement e2e coverage for advanced TUI workflows and interaction correctness.\n\nMust include:\n- search/navigation/explain/degraded-mode flows\n- frame/state snapshots across sizes and modes\n- reproducible replay artifacts for failures","acceptance_criteria":"1) TUI e2e suite covers core interactive and degraded-mode workflows.\n2) Snapshot/replay artifacts capture enough state for diagnosis.\n3) Multi-size/mode behavior is validated explicitly.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.060383598Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.201220371Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","e2e","fsfs"],"dependencies":[{"issue_id":"bd-2hz.10.5","depends_on_id":"bd-2hz.10","type":"parent-child","created_at":"2026-02-13T22:03:21.060383598Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.10.6","title":"Add soak/fault-injection suites for long-run reliability","description":"Task:\nDefine long-run and failure-stress validation for fsfs indexing/query loops.\n\nMust include:\n- sustained workload soak scenarios\n- resource starvation and partial-failure injections\n- leak/drift detection with threshold alerts","acceptance_criteria":"1) Soak/fault suites exercise long-run reliability under stressors.\n2) Leak/drift thresholds are explicit and enforced.\n3) Fault scenarios include starvation and partial-failure classes.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.176239487Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.309836658Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fault-injection","fsfs","soak"],"dependencies":[{"issue_id":"bd-2hz.10.6","depends_on_id":"bd-2hz.10","type":"parent-child","created_at":"2026-02-13T22:03:21.176239487Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.11","title":"Workstream: Packaging, migration, rollout, and operator documentation","description":"Goal:\nShip fsfs as an adoptable product with strong migration paths for existing host projects and clear operational guidance.\n\nScope:\n- packaging/release/install paths\n- migration playbooks (cass/xf/mcp_agent_mail_rust/frankenterm/etc.)\n- rollout, canary, fallback, and docs","acceptance_criteria":"1) Packaging/install/release workflow supports reliable standalone adoption.\n2) Migration playbooks cover existing host projects and future adopters.\n3) Rollout/canary/fallback/documentation enable safe production-style usage.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:11.545254286Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:49.460434615Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","fsfs","phase-rollout","release"],"dependencies":[{"issue_id":"bd-2hz.11","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:11.545254286Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.11","depends_on_id":"bd-2hz.10","type":"blocks","created_at":"2026-02-13T22:04:49.348222276Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.11","depends_on_id":"bd-2hz.6","type":"blocks","created_at":"2026-02-13T22:04:49.122826872Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.11","depends_on_id":"bd-2hz.7","type":"blocks","created_at":"2026-02-13T22:04:49.236964605Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.11","depends_on_id":"bd-2hz.8","type":"blocks","created_at":"2026-02-13T22:04:49.460379522Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.11.1","title":"Define fsfs packaging/release/install workflow","description":"Task:\nDesign standalone binary distribution and installation experience for fsfs.\n\nMust include:\n- cross-platform build/release matrix\n- checksum/signature strategy\n- install and upgrade UX expectations","acceptance_criteria":"1) Packaging/release/install workflow is defined for target platforms.\n2) Integrity checks (checksums/signatures) are specified.\n3) Upgrade path expectations are documented.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.299995386Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.416007847Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","packaging","release"],"dependencies":[{"issue_id":"bd-2hz.11.1","depends_on_id":"bd-2hz.11","type":"parent-child","created_at":"2026-02-13T22:03:21.299995386Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.11.2","title":"Write config/policy/operations documentation with scenario playbooks","description":"Task:\nCreate documentation that makes fsfs behavior and controls transparent and actionable.\n\nMust include:\n- config and policy reference\n- decision/degradation interpretation guide\n- troubleshooting and recovery runbooks","acceptance_criteria":"1) Docs cover configuration, policy semantics, and operations clearly.\n2) Scenario playbooks include troubleshooting and recovery flows.\n3) Documentation enables adoption without tribal knowledge.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.418723673Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.522884587Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","fsfs","operators"],"dependencies":[{"issue_id":"bd-2hz.11.2","depends_on_id":"bd-2hz.11","type":"parent-child","created_at":"2026-02-13T22:03:21.418723673Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.11.3","title":"Create migration playbooks for host projects adopting fsfs","description":"Task:\nDefine adoption paths for replacing existing search layers in key host projects.\n\nMust include:\n- project-specific cutover and validation checklists\n- compatibility and rollback considerations\n- post-cutover verification and monitoring steps","acceptance_criteria":"1) Migration playbooks exist for priority host projects.\n2) Cutover validation and rollback steps are explicit.\n3) Post-cutover monitoring checks are actionable and measurable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.535554277Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.629019398Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","integration","migrations"],"dependencies":[{"issue_id":"bd-2hz.11.3","depends_on_id":"bd-2hz.11","type":"parent-child","created_at":"2026-02-13T22:03:21.535554277Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.11.4","title":"Define staged rollout (shadow/canary/default) and fallback protocol","description":"Task:\nSpecify safe rollout path for fsfs across environments and host projects.\n\nMust include:\n- shadow-run comparison strategy\n- canary success/failure thresholds\n- deterministic rollback triggers and procedure","acceptance_criteria":"1) Shadow/canary/default rollout phases are explicitly defined.\n2) Success/failure thresholds and rollback triggers are deterministic.\n3) Protocol is usable for project-by-project adoption planning.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.650042326Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.729878242Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["canary","fsfs","rollout"],"dependencies":[{"issue_id":"bd-2hz.11.4","depends_on_id":"bd-2hz.11","type":"parent-child","created_at":"2026-02-13T22:03:21.650042326Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.11.5","title":"Build reproducible demo/benchmark showcase suite for fsfs","description":"Task:\nCreate high-signal demo scripts that prove fsfs capability and performance claims.\n\nMust include:\n- repeatable demo scenarios\n- benchmark-backed claims with artifact hashes\n- failure-mode demonstration for graceful degradation","acceptance_criteria":"1) Demo suite demonstrates core features and degradation behavior reproducibly.\n2) Benchmark claims are backed by artifact hashes and scripts.\n3) Showcase materials are suitable for technical validation and onboarding.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:21.764595146Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:27.837991116Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["demo","fsfs","showcase"],"dependencies":[{"issue_id":"bd-2hz.11.5","depends_on_id":"bd-2hz.11","type":"parent-child","created_at":"2026-02-13T22:03:21.764595146Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.2","title":"Workstream: Machine-wide corpus discovery and utility-aware ingestion policy","description":"Goal:\nBuild robust machine-wide discovery and file eligibility policy that maximizes search value per compute/storage unit.\n\nScope:\n- root walking + exclusion policy\n- binary/log/vendor/generated detection\n- utility scoring + embed eligibility classification","acceptance_criteria":"1) Machine-wide discovery and exclusion policy is comprehensive and configurable.\n2) File eligibility policy distinguishes high-value text vs low-value expensive artifacts.\n3) Utility-aware ingestion decisions are explainable and reproducible.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:10.543167217Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:46.155148492Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["corpus","fsfs","ingestion","phase-corpus"],"dependencies":[{"issue_id":"bd-2hz.2","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:10.543167217Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:46.155082318Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.2.1","title":"Design root discovery defaults and exclusion precedence model","description":"Task:\nDefine traversal roots and exclusion precedence for large heterogeneous machines.\n\nMust include:\n- default roots (home-centric) and override model\n- precedence across .gitignore/.ignore/fsfs config/system excludes\n- loop/symlink/mount-boundary behavior and safety guards","acceptance_criteria":"1) Root/exclusion precedence is deterministic and comprehensive.\n2) Symlink/mount/loop safety behavior is explicitly defined.\n3) Override behavior supports both strict and permissive workflows.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.689876163Z","created_by":"ubuntu","updated_at":"2026-02-13T22:03:41.206721595Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["corpus","discovery","fsfs"],"dependencies":[{"issue_id":"bd-2hz.2.1","depends_on_id":"bd-2hz.2","type":"parent-child","created_at":"2026-02-13T22:02:01.689876163Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.2.2","title":"Implement text-vs-binary and encoding classification policy","description":"Task:\nDesign robust file-type eligibility classification for ingestion paths.\n\nMust include:\n- binary/content sniff heuristics\n- encoding detection and normalization fallback policy\n- corrupt/partial file handling behavior\n- confidence signals for downstream utility scoring","acceptance_criteria":"1) Binary/text/encoding classification policy covers expected file variants.\n2) Corrupt/partial file handling avoids pipeline instability.\n3) Confidence signals are available for downstream policy decisions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.801776836Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:14.361861206Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["classification","fsfs","ingestion"],"dependencies":[{"issue_id":"bd-2hz.2.2","depends_on_id":"bd-2hz.2","type":"parent-child","created_at":"2026-02-13T22:02:01.801776836Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.2","depends_on_id":"bd-2hz.2.1","type":"blocks","created_at":"2026-02-13T22:05:14.361809469Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.2.3","title":"Define high-cost artifact detectors (logs/vendor/generated/library code)","description":"Task:\nBuild smart heuristics for skipping or downgrading low-value expensive content.\n\nMust include:\n- giant log detection (size/churn/redundancy patterns)\n- vendored/generated/library-tree detection\n- compressed/archive and transient build artifact policies\n- override hooks for user-forced inclusion","acceptance_criteria":"1) High-cost artifact classes are detected with clear rules.\n2) Skip/downgrade behavior is explainable and overrideable.\n3) Policy minimizes wasteful embedding/index work on low-value artifacts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:01.912461131Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:14.471033313Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cost-control","fsfs","heuristics"],"dependencies":[{"issue_id":"bd-2hz.2.3","depends_on_id":"bd-2hz.2","type":"parent-child","created_at":"2026-02-13T22:02:01.912461131Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.3","depends_on_id":"bd-2hz.2.1","type":"blocks","created_at":"2026-02-13T22:05:14.470989420Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.2.4","title":"Define utility scoring and ingestion class assignment","description":"Task:\nClassify files into ingest classes: full semantic+lexical, lexical-only, metadata-only, skip.\n\nMust include:\n- feature set for utility score computation\n- deterministic tie-break rules\n- explanation fields for class decisions\n- calibration/fallback strategy for uncertain classifications","acceptance_criteria":"1) Ingestion class assignment rules are deterministic and auditable.\n2) Utility scoring features and tie-breaks are documented.\n3) Uncertain classifications have explicit fallback behavior.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:02.024211693Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:14.801805739Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["decision-contracts","fsfs","utility-scoring"],"dependencies":[{"issue_id":"bd-2hz.2.4","depends_on_id":"bd-2hz.1.2","type":"blocks","created_at":"2026-02-13T22:05:14.801749985Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.4","depends_on_id":"bd-2hz.2","type":"parent-child","created_at":"2026-02-13T22:02:02.024211693Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.4","depends_on_id":"bd-2hz.2.2","type":"blocks","created_at":"2026-02-13T22:05:14.580533534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.4","depends_on_id":"bd-2hz.2.3","type":"blocks","created_at":"2026-02-13T22:05:14.691467960Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.2.5","title":"Specify incremental change-detection contract for file updates","description":"Task:\nDefine how fsfs detects and schedules changed content without excessive rescans.\n\nMust include:\n- mtime/size/hash tradeoff policy\n- rename/move detection behavior\n- crash/restart recovery semantics for pending changes\n- stale-state reconciliation guarantees","acceptance_criteria":"1) Change detection policy balances accuracy and overhead.\n2) Rename/move/restart scenarios have deterministic handling.\n3) Recovery semantics prevent orphaned or stale indexing state.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:02.135653186Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:15.047303366Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","incremental","ingestion"],"dependencies":[{"issue_id":"bd-2hz.2.5","depends_on_id":"bd-2hz.2","type":"parent-child","created_at":"2026-02-13T22:02:02.135653186Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.5","depends_on_id":"bd-2hz.2.1","type":"blocks","created_at":"2026-02-13T22:05:14.938286169Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.2.5","depends_on_id":"bd-2hz.2.2","type":"blocks","created_at":"2026-02-13T22:05:15.047252441Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.3","title":"Workstream: Incremental indexing and storage architecture for fsfs","description":"Goal:\nDesign and implement fsfs indexing architecture for fast incremental updates, durable state, and low-latency query serving.\n\nScope:\n- catalog/changelog model\n- lexical/vector index pipelines\n- watcher/backfill/update orchestration","acceptance_criteria":"1) Incremental indexing architecture supports durable updates and fast refresh.\n2) Lexical/vector pipelines and state model are coherent and performance-aware.\n3) Watcher/backfill orchestration handles initial and ongoing indexing safely.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:10.653500786Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:49.910538017Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","indexing","phase-indexing","storage"],"dependencies":[{"issue_id":"bd-2hz.3","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:10.653500786Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:46.266704191Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3","depends_on_id":"bd-2hz.2","type":"blocks","created_at":"2026-02-13T22:04:46.378390696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T22:04:49.795987311Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T22:04:49.684424909Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T22:04:49.570311672Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T22:04:49.910480339Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.3.1","title":"Scaffold fsfs binary crate and configuration model","description":"Task:\nAdd standalone fsfs binary crate(s) and configuration surface aligned with workspace/runtime constraints (asupersync-only async).\n\nMust include:\n- command entrypoints and runtime wiring\n- config loading/validation/override precedence\n- clean separation between library core and binary UX adapters","acceptance_criteria":"1) fsfs binary scaffold and config model are defined and bounded.\n2) Runtime design respects asupersync-only async constraints.\n3) Separation between core logic and UX adapters is explicit.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:20.334089241Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:15.264604337Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","fsfs","indexing"],"dependencies":[{"issue_id":"bd-2hz.3.1","depends_on_id":"bd-2hz.1.1","type":"blocks","created_at":"2026-02-13T22:05:15.157246627Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.1","depends_on_id":"bd-2hz.3","type":"parent-child","created_at":"2026-02-13T22:02:20.334089241Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.1","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T22:05:15.264555686Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.3.2","title":"Design fsfs catalog/changelog schema in frankensqlite","description":"Task:\nDefine durable metadata/state model for tracked files, versions, ingest class, and index status.\n\nMust include:\n- schema for file identity, revision, eligibility class, and pipeline status\n- crash-safe changelog and replay semantics\n- indexes for fast incremental lookups and cleanup operations","acceptance_criteria":"1) Catalog/changelog schema captures required indexing state entities.\n2) Replay/recovery semantics are deterministic under interruption.\n3) Query indexes support expected incremental workloads.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:20.467628935Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:15.484617035Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","fsfs","indexing"],"dependencies":[{"issue_id":"bd-2hz.3.2","depends_on_id":"bd-2hz.3","type":"parent-child","created_at":"2026-02-13T22:02:20.467628935Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.2","depends_on_id":"bd-2hz.3.1","type":"blocks","created_at":"2026-02-13T22:05:15.373818072Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.2","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T22:05:15.484571730Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.3.3","title":"Implement lexical indexing pipeline for selected corpus","description":"Task:\nDesign lexical indexing path for utility-selected files with incremental updates.\n\nMust include:\n- chunking/tokenization strategy for diverse text formats\n- update/delete semantics on file change or policy reclassification\n- latency and throughput targets for initial and incremental indexing","acceptance_criteria":"1) Lexical indexing path supports initial + incremental updates.\n2) Reclassification and deletion semantics are well-defined.\n3) Throughput/latency targets are specified with measurable criteria.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:20.563062018Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:15.718114867Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","indexing","lexical"],"dependencies":[{"issue_id":"bd-2hz.3.3","depends_on_id":"bd-2hz.2.4","type":"blocks","created_at":"2026-02-13T22:05:15.718058883Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.3","depends_on_id":"bd-2hz.3","type":"parent-child","created_at":"2026-02-13T22:02:20.563062018Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.3","depends_on_id":"bd-2hz.3.2","type":"blocks","created_at":"2026-02-13T22:05:15.596753081Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.3.4","title":"Implement vector embedding/index pipeline with revision tracking","description":"Task:\nDesign semantic indexing pipeline for eligible files with strict revision coherence.\n\nMust include:\n- chunk generation and embedding job scheduling model\n- revision-aware vector index writes and stale invalidation\n- fast/quality embedder policy hooks and fallback behavior","acceptance_criteria":"1) Vector pipeline preserves revision coherence and stale invalidation.\n2) Chunking and embed scheduling strategy are documented.\n3) Fallback behavior is explicit when embedding is constrained or unavailable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:20.677094642Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:16.057342152Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embeddings","fsfs","indexing"],"dependencies":[{"issue_id":"bd-2hz.3.4","depends_on_id":"bd-2hz.2.4","type":"blocks","created_at":"2026-02-13T22:05:15.945986447Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.4","depends_on_id":"bd-2hz.3","type":"parent-child","created_at":"2026-02-13T22:02:20.677094642Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.4","depends_on_id":"bd-2hz.3.2","type":"blocks","created_at":"2026-02-13T22:05:15.829716142Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.4","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T22:05:16.057298140Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.3.5","title":"Design watcher/backfill orchestration and crash-safe resume","description":"Task:\nDefine orchestration between initial backfill, ongoing watchers, and resumable work queues.\n\nMust include:\n- startup bootstrap strategy for large machines\n- bounded queue semantics with backpressure\n- deterministic replay/resume after interruption or crash","acceptance_criteria":"1) Backfill/watcher orchestration is deterministic and resumable.\n2) Queue/backpressure design avoids unbounded growth.\n3) Crash-safe resume semantics are sufficient for long-running hosts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:20.789113135Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:16.393612952Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","indexing","orchestration"],"dependencies":[{"issue_id":"bd-2hz.3.5","depends_on_id":"bd-2hz.2.5","type":"blocks","created_at":"2026-02-13T22:05:16.167805607Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.5","depends_on_id":"bd-2hz.3","type":"parent-child","created_at":"2026-02-13T22:02:20.789113135Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.5","depends_on_id":"bd-2hz.3.3","type":"blocks","created_at":"2026-02-13T22:05:16.280654457Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.3.5","depends_on_id":"bd-2hz.3.4","type":"blocks","created_at":"2026-02-13T22:05:16.393553070Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.4","title":"Workstream: Adaptive compute-pressure control and graceful degradation","description":"Goal:\nMake fsfs resilient under host pressure with explicit, safe degradation modes and recoverable control loops.\n\nScope:\n- pressure sensing + control states\n- budget scheduler and backpressure\n- deterministic safe-mode transitions","acceptance_criteria":"1) Compute-pressure states and transitions are explicitly modeled.\n2) Budget/backpressure logic has deterministic safe-mode behavior.\n3) Degradation policy preserves correctness while reducing resource footprint.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:10.765197137Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:46.710349433Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["degradation","fsfs","phase-control","resource-governance"],"dependencies":[{"issue_id":"bd-2hz.4","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:10.765197137Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:46.488158979Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4","depends_on_id":"bd-2hz.2","type":"blocks","created_at":"2026-02-13T22:04:46.600796143Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4","depends_on_id":"bd-2hz.3","type":"blocks","created_at":"2026-02-13T22:04:46.710295242Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.4.1","title":"Implement host pressure sensing and control-state model","description":"Task:\nSpecify host pressure telemetry and conversion into stable control states.\n\nMust include:\n- CPU/memory/IO/load signal collection and smoothing\n- state definitions (normal, constrained, degraded, emergency)\n- hysteresis and anti-flap rules for state transitions","acceptance_criteria":"1) Pressure telemetry and state definitions are complete and measurable.\n2) Transition hysteresis prevents rapid flapping.\n3) State model is consumable by scheduler and UX layers.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:20.899717421Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:16.503557866Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["control-loop","fsfs","resource-governance"],"dependencies":[{"issue_id":"bd-2hz.4.1","depends_on_id":"bd-2hz.3.1","type":"blocks","created_at":"2026-02-13T22:05:16.503503945Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.1","depends_on_id":"bd-2hz.4","type":"parent-child","created_at":"2026-02-13T22:02:20.899717421Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.4.2","title":"Design budget scheduler for ingest/embed/query workloads","description":"Task:\nBuild policy for allocating compute budgets across concurrent fsfs work types.\n\nMust include:\n- queue priorities and starvation guards\n- fair-share vs latency-sensitive policy toggles\n- bounded admission and cancellation-correct semantics","acceptance_criteria":"1) Budget scheduler policy covers ingest/embed/query contention scenarios.\n2) Starvation and fairness guarantees are explicit.\n3) Bounded admission/cancellation semantics are testable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:21.009493777Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:16.723929766Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","scheduling","throttling"],"dependencies":[{"issue_id":"bd-2hz.4.2","depends_on_id":"bd-2hz.3.5","type":"blocks","created_at":"2026-02-13T22:05:16.723871697Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.2","depends_on_id":"bd-2hz.4","type":"parent-child","created_at":"2026-02-13T22:02:21.009493777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.2","depends_on_id":"bd-2hz.4.1","type":"blocks","created_at":"2026-02-13T22:05:16.613700710Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.4.3","title":"Define graceful degradation state machine and safe-mode behavior","description":"Task:\nSpecify exactly how fsfs sheds load while preserving correctness and usability.\n\nMust include:\n- feature shedding ladder (embed deferral, lexical-only, metadata-only, pause)\n- trigger/exit conditions and audit events\n- user-visible status and override controls","acceptance_criteria":"1) Degradation ladder is explicit with trigger and recovery conditions.\n2) Correctness-preserving behavior is defined for each degraded state.\n3) User-visible status and override behavior are clearly specified.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:21.123495964Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:17.066067269Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["degradation","fsfs","safety-mode"],"dependencies":[{"issue_id":"bd-2hz.4.3","depends_on_id":"bd-2hz.1.2","type":"blocks","created_at":"2026-02-13T22:05:17.065972932Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.3","depends_on_id":"bd-2hz.4","type":"parent-child","created_at":"2026-02-13T22:02:21.123495964Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.3","depends_on_id":"bd-2hz.4.1","type":"blocks","created_at":"2026-02-13T22:05:16.837222767Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.3","depends_on_id":"bd-2hz.4.2","type":"blocks","created_at":"2026-02-13T22:05:16.951599838Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.4.4","title":"Add calibration guards (conformal/e-process) for adaptive controllers","description":"Task:\nDefine anytime-valid guard rails for adaptive decisions in scheduler and ingestion policy.\n\nMust include:\n- calibration metrics and coverage targets\n- breach detection and fallback triggers\n- confidence/evidence fields for operator and test visibility","acceptance_criteria":"1) Calibration/anytime guard metrics and thresholds are defined.\n2) Breach handling includes deterministic fallback transitions.\n3) Evidence fields support audit and replay of controller decisions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:21.242704027Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:17.406005444Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["adaptive-control","calibration","fsfs"],"dependencies":[{"issue_id":"bd-2hz.4.4","depends_on_id":"bd-2hz.1.4","type":"blocks","created_at":"2026-02-13T22:05:17.405933700Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.4","depends_on_id":"bd-2hz.4","type":"parent-child","created_at":"2026-02-13T22:02:21.242704027Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.4","depends_on_id":"bd-2hz.4.2","type":"blocks","created_at":"2026-02-13T22:05:17.178320484Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.4","depends_on_id":"bd-2hz.4.3","type":"blocks","created_at":"2026-02-13T22:05:17.290793260Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.4.5","title":"Define strict/performance/degraded policy profiles and override semantics","description":"Task:\nCreate explicit operating profiles for different host conditions and user preferences.\n\nMust include:\n- profile defaults and capability boundaries\n- deterministic precedence rules for profile vs user overrides\n- migration-safe configuration evolution strategy","acceptance_criteria":"1) Profile semantics and boundaries are documented.\n2) Override precedence is deterministic and conflict-safe.\n3) Profile evolution strategy avoids silent behavioral drift.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:21.355645368Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:17.519377985Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","policy","profiles"],"dependencies":[{"issue_id":"bd-2hz.4.5","depends_on_id":"bd-2hz.4","type":"parent-child","created_at":"2026-02-13T22:02:21.355645368Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.4.5","depends_on_id":"bd-2hz.4.3","type":"blocks","created_at":"2026-02-13T22:05:17.519319215Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.5","title":"Workstream: Query execution, ranking, and explainability core","description":"Goal:\nDeliver blazing-fast and high-quality retrieval with transparent ranking rationale and phase-aware refinement behavior.\n\nScope:\n- query classification + retrieval budgets\n- fusion/ranking/snippets/provenance\n- explanation payloads for CLI/TUI","acceptance_criteria":"1) Query path covers fast/refined retrieval with transparent ranking rationale.\n2) Explainability payloads are available for both CLI and TUI consumers.\n3) Retrieval and ranking behavior remains stable under configuration changes.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:10.875909875Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:47.042408949Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["explainability","fsfs","phase-query","ranking"],"dependencies":[{"issue_id":"bd-2hz.5","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:10.875909875Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:46.819440909Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5","depends_on_id":"bd-2hz.3","type":"blocks","created_at":"2026-02-13T22:04:46.930768331Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5","depends_on_id":"bd-2hz.4","type":"blocks","created_at":"2026-02-13T22:04:47.042354768Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.5.1","title":"Define query intent classifier and retrieval budget mapping","description":"Task:\nClassify incoming queries and map them to fast/refined retrieval strategies and budgets.\n\nMust include:\n- intent categories and confidence model\n- budget policy by class (latency, fanout, rerank depth)\n- fallback path for uncertain/malformed queries","acceptance_criteria":"1) Query classes and confidence model are explicitly defined.\n2) Budget mapping is measurable and profile-aware.\n3) Uncertain-query fallback keeps behavior robust and explainable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:46.496545464Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:31.453520943Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["classification","fsfs","query"],"dependencies":[{"issue_id":"bd-2hz.5.1","depends_on_id":"bd-2hz.1.2","type":"blocks","created_at":"2026-02-13T22:05:31.343924652Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.1","depends_on_id":"bd-2hz.2.4","type":"blocks","created_at":"2026-02-13T22:05:31.453470128Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.1","depends_on_id":"bd-2hz.5","type":"parent-child","created_at":"2026-02-13T22:02:46.496545464Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.5.2","title":"Implement multi-stage retrieval orchestration and rank fusion policy","description":"Task:\nDesign end-to-end query execution across lexical/vector/rerank stages for fsfs runtime.\n\nMust include:\n- phase execution strategy and cancellation semantics\n- fusion and tie-break policy\n- degraded-mode retrieval behavior compatibility","acceptance_criteria":"1) Multi-stage orchestration is specified for normal and degraded paths.\n2) Fusion/tie-break policy is deterministic.\n3) Cancellation semantics preserve partial-result correctness.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:46.606689769Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:31.777136452Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","query","ranking"],"dependencies":[{"issue_id":"bd-2hz.5.2","depends_on_id":"bd-2hz.3.3","type":"blocks","created_at":"2026-02-13T22:05:31.667800949Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.2","depends_on_id":"bd-2hz.3.4","type":"blocks","created_at":"2026-02-13T22:05:31.777083633Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.2","depends_on_id":"bd-2hz.5","type":"parent-child","created_at":"2026-02-13T22:02:46.606689769Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.2","depends_on_id":"bd-2hz.5.1","type":"blocks","created_at":"2026-02-13T22:05:31.559875295Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.5.3","title":"Design snippet/highlight/provenance rendering contract","description":"Task:\nDefine snippet and highlight generation plus provenance metadata for trustworthy result interpretation.\n\nMust include:\n- snippet extraction strategy for varied text types\n- highlight stability and unicode correctness expectations\n- provenance payload fields (path, segment, revision, score contributors)","acceptance_criteria":"1) Snippet/highlight behavior is stable across supported text formats.\n2) Unicode correctness and offset provenance are specified.\n3) Payload fields support downstream CLI/TUI rendering needs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:46.713586774Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:31.884300299Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","provenance","snippets"],"dependencies":[{"issue_id":"bd-2hz.5.3","depends_on_id":"bd-2hz.5","type":"parent-child","created_at":"2026-02-13T22:02:46.713586774Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.3","depends_on_id":"bd-2hz.5.2","type":"blocks","created_at":"2026-02-13T22:05:31.884243834Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.5.4","title":"Define explanation payload schema for ranking and policy decisions","description":"Task:\nCreate machine/human-readable explanation schema for result ranking and ingestion/degradation decisions.\n\nMust include:\n- score component breakdowns\n- decision reason codes and confidence fields\n- compatibility with CLI JSON/TOON and TUI panels","acceptance_criteria":"1) Explanation schema covers ranking and policy decisions consistently.\n2) Reason codes and confidence semantics are standardized.\n3) Schema is compatible across JSON, TOON, and TUI views.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:46.821620758Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:32.211196970Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["explainability","fsfs","ranking"],"dependencies":[{"issue_id":"bd-2hz.5.4","depends_on_id":"bd-2hz.1.4","type":"blocks","created_at":"2026-02-13T22:05:32.104894365Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.4","depends_on_id":"bd-2hz.5","type":"parent-child","created_at":"2026-02-13T22:02:46.821620758Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.4","depends_on_id":"bd-2hz.5.2","type":"blocks","created_at":"2026-02-13T22:05:31.992362128Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.4","depends_on_id":"bd-2hz.8.1","type":"blocks","created_at":"2026-02-13T22:05:32.211153509Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.5.5","title":"Define recency/path/project priors and deterministic tuning controls","description":"Task:\nSpecify optional ranking priors and tuning controls without sacrificing determinism.\n\nMust include:\n- prior families and default weights\n- deterministic tie-break and reproducibility constraints\n- profile compatibility with strict/performance/degraded modes","acceptance_criteria":"1) Ranking priors and defaults are explicit with deterministic tie-breaks.\n2) Tuning controls preserve reproducibility constraints.\n3) Policy integrates cleanly with strict/performance/degraded profiles.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:46.930695670Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:32.528349530Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","ranking","tuning"],"dependencies":[{"issue_id":"bd-2hz.5.5","depends_on_id":"bd-2hz.4.5","type":"blocks","created_at":"2026-02-13T22:05:32.528276102Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.5","depends_on_id":"bd-2hz.5","type":"parent-child","created_at":"2026-02-13T22:02:46.930695670Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.5","depends_on_id":"bd-2hz.5.1","type":"blocks","created_at":"2026-02-13T22:05:32.316482742Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.5.5","depends_on_id":"bd-2hz.5.2","type":"blocks","created_at":"2026-02-13T22:05:32.422552221Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.6","title":"Workstream: Agent-first CLI mode (JSON/TOON) and protocol ergonomics","description":"Goal:\nCreate ultra-agent-centric CLI interface with deterministic machine-readable output and low-friction automation semantics.\n\nScope:\n- command surface + schemas\n- TOON integration via toon_rust\n- streaming/query/debug command ergonomics","acceptance_criteria":"1) Agent CLI command surface is ergonomic and automation-first.\n2) JSON and TOON output contracts are stable, versioned, and documented.\n3) Streaming/search/debug workflows support robust programmatic use.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:10.985012610Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:47.376364465Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-cli","fsfs","phase-cli","toon"],"dependencies":[{"issue_id":"bd-2hz.6","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:10.985012610Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:47.155266245Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6","depends_on_id":"bd-2hz.5","type":"blocks","created_at":"2026-02-13T22:04:47.263331180Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6","depends_on_id":"bd-2hz.8","type":"blocks","created_at":"2026-02-13T22:04:47.376305685Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.6.1","title":"Design fsfs command surface (search/index/status/explain/config)","description":"Task:\nDefine command taxonomy and argument semantics for agent-first workflows.\n\nMust include:\n- stable command contracts and aliases policy\n- composable flags for automation scripts\n- discoverability and help design for high-density workflows","acceptance_criteria":"1) Command taxonomy and arguments are complete for agent workflows.\n2) Script-friendly composability and alias policy are specified.\n3) Help/discovery behavior supports high-frequency operator use.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.045120138Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:32.634704633Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-cli","commands","fsfs"],"dependencies":[{"issue_id":"bd-2hz.6.1","depends_on_id":"bd-2hz.1.1","type":"blocks","created_at":"2026-02-13T22:05:32.634661993Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.1","depends_on_id":"bd-2hz.6","type":"parent-child","created_at":"2026-02-13T22:02:47.045120138Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.6.2","title":"Define versioned JSON output schema and compatibility guarantees","description":"Task:\nDefine machine contract for JSON mode including forward/backward compatibility policy.\n\nMust include:\n- field taxonomy and optionality rules\n- schema versioning/deprecation contract\n- explicit error object format and stable identifiers","acceptance_criteria":"1) JSON schema covers success/progress/error payloads.\n2) Compatibility/versioning policy is explicit and enforceable.\n3) Stable identifiers enable downstream automation reliably.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.156057938Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:32.850374250Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-cli","fsfs","json-schema"],"dependencies":[{"issue_id":"bd-2hz.6.2","depends_on_id":"bd-2hz.6","type":"parent-child","created_at":"2026-02-13T22:02:47.156057938Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.2","depends_on_id":"bd-2hz.6.1","type":"blocks","created_at":"2026-02-13T22:05:32.740957906Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.2","depends_on_id":"bd-2hz.8.1","type":"blocks","created_at":"2026-02-13T22:05:32.850331380Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.6.3","title":"Integrate TOON output mode via toon_rust with parity guarantees","description":"Task:\nAdd TOON output support with strict semantic parity to JSON contracts.\n\nMust include:\n- mapping rules JSON -> TOON\n- parity verification approach\n- compatibility strategy for streaming and batch outputs","acceptance_criteria":"1) TOON mapping preserves JSON semantics without loss.\n2) Parity validation approach is defined for batch and streaming modes.\n3) Compatibility strategy covers schema evolution over time.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.263666546Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:32.957335659Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-cli","fsfs","toon"],"dependencies":[{"issue_id":"bd-2hz.6.3","depends_on_id":"bd-2hz.6","type":"parent-child","created_at":"2026-02-13T22:02:47.263666546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.3","depends_on_id":"bd-2hz.6.2","type":"blocks","created_at":"2026-02-13T22:05:32.957286417Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.6.4","title":"Define streaming query protocol (NDJSON/TOON) and error/exit semantics","description":"Task:\nSpecify streaming mode protocol for low-latency agent integrations.\n\nMust include:\n- event taxonomy (progress, result, explain, warning, terminal)\n- stream termination and retry semantics\n- deterministic exit-code and failure categorization policy","acceptance_criteria":"1) Streaming protocol events and ordering semantics are explicit.\n2) Error/termination/exit-code behavior is deterministic.\n3) Retry/resume expectations are documented for agent clients.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.376335046Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:33.278886262Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-cli","fsfs","streaming"],"dependencies":[{"issue_id":"bd-2hz.6.4","depends_on_id":"bd-2hz.5.4","type":"blocks","created_at":"2026-02-13T22:05:33.278843081Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.4","depends_on_id":"bd-2hz.6","type":"parent-child","created_at":"2026-02-13T22:02:47.376335046Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.4","depends_on_id":"bd-2hz.6.2","type":"blocks","created_at":"2026-02-13T22:05:33.063595374Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.4","depends_on_id":"bd-2hz.6.3","type":"blocks","created_at":"2026-02-13T22:05:33.173942390Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.6.5","title":"Design ultra-agent ergonomics (compact mode, stable IDs, query templates)","description":"Task:\nDefine ergonomic quality layer for agent workflows at scale.\n\nMust include:\n- compact payload profile for token efficiency\n- stable result IDs for follow-up commands\n- templated query/explain flows for common agent tasks","acceptance_criteria":"1) Compact mode and stable ID design support token-efficient automation.\n2) Query template flows reduce repetitive agent prompting.\n3) Ergonomic layer remains schema-consistent with base CLI contracts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.491309172Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:33.564161094Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-cli","ergonomics","fsfs"],"dependencies":[{"issue_id":"bd-2hz.6.5","depends_on_id":"bd-2hz.6","type":"parent-child","created_at":"2026-02-13T22:02:47.491309172Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.5","depends_on_id":"bd-2hz.6.1","type":"blocks","created_at":"2026-02-13T22:05:33.384423213Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.6.5","depends_on_id":"bd-2hz.6.2","type":"blocks","created_at":"2026-02-13T22:05:33.564115829Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7","title":"Workstream: Deluxe FrankenTUI search interface","description":"Goal:\nBuild a feature-packed interactive fsfs interface inspired by and extending ftui demo showcase search experiences.\n\nScope:\n- advanced search screens and interactions\n- indexing/resource panels\n- galaxy-brain explainability views","acceptance_criteria":"1) Deluxe TUI provides advanced interactive search beyond basic list UI.\n2) Indexing/resource/insight panels are integrated into a coherent flow.\n3) Explainability views surface the key decision context for power users.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:11.094193731Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:47.723672828Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","frankentui","fsfs","phase-tui"],"dependencies":[{"issue_id":"bd-2hz.7","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:11.094193731Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:47.480810565Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7","depends_on_id":"bd-2hz.4","type":"blocks","created_at":"2026-02-13T22:04:47.567513037Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7","depends_on_id":"bd-2hz.5","type":"blocks","created_at":"2026-02-13T22:04:47.651647422Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7","depends_on_id":"bd-2hz.8","type":"blocks","created_at":"2026-02-13T22:04:47.723591285Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7.1","title":"Build deluxe TUI shell, navigation, and command palette foundation","description":"Task:\nDefine fsfs TUI shell architecture and global interaction model.\n\nMust include:\n- screen registry and context-preserving navigation\n- global keymap/mouse model\n- command palette action taxonomy and routing","acceptance_criteria":"1) TUI shell/navigation/keymap model is explicit and reusable.\n2) Command palette taxonomy supports search and ops actions coherently.\n3) Context preservation rules are documented for cross-screen movement.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.597533608Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:33.777515497Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","fsfs","shell"],"dependencies":[{"issue_id":"bd-2hz.7.1","depends_on_id":"bd-2hz.1.1","type":"blocks","created_at":"2026-02-13T22:05:33.669446805Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.1","depends_on_id":"bd-2hz.6.1","type":"blocks","created_at":"2026-02-13T22:05:33.777459582Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.1","depends_on_id":"bd-2hz.7","type":"parent-child","created_at":"2026-02-13T22:02:47.597533608Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7.2","title":"Implement ultra-fast interactive search screen with virtualization","description":"Task:\nDesign flagship search screen for sub-perceptual interactive feel at scale.\n\nMust include:\n- incremental query update behavior\n- high-cardinality result virtualization\n- inline explain toggles and jump actions","acceptance_criteria":"1) Interactive search screen behavior targets low-latency response.\n2) Result virtualization strategy handles high-cardinality workloads.\n3) Explain/jump affordances are integrated into primary flow.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.706808445Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:33.992439378Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","fsfs","search-ui"],"dependencies":[{"issue_id":"bd-2hz.7.2","depends_on_id":"bd-2hz.5.2","type":"blocks","created_at":"2026-02-13T22:05:33.992384695Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.2","depends_on_id":"bd-2hz.7","type":"parent-child","created_at":"2026-02-13T22:02:47.706808445Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.2","depends_on_id":"bd-2hz.7.1","type":"blocks","created_at":"2026-02-13T22:05:33.885144826Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7.3","title":"Implement indexing/jobs/resource pressure cockpit screens","description":"Task:\nCreate real-time screens for indexing progress, queue health, and resource states.\n\nMust include:\n- backlog and throughput visualizations\n- pressure/degradation indicators\n- actionable controls for pausing/throttling/recovery","acceptance_criteria":"1) Indexing/job/resource views expose actionable system state.\n2) Pressure/degradation indicators are unambiguous.\n3) Control actions map to safe operational semantics.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.815425941Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:34.314643113Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","fsfs","indexing-ui"],"dependencies":[{"issue_id":"bd-2hz.7.3","depends_on_id":"bd-2hz.3.5","type":"blocks","created_at":"2026-02-13T22:05:34.207702945Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.3","depends_on_id":"bd-2hz.4.1","type":"blocks","created_at":"2026-02-13T22:05:34.314575126Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.3","depends_on_id":"bd-2hz.7","type":"parent-child","created_at":"2026-02-13T22:02:47.815425941Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.3","depends_on_id":"bd-2hz.7.1","type":"blocks","created_at":"2026-02-13T22:05:34.098842732Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7.4","title":"Implement galaxy-brain explainability screens for fsfs decisions","description":"Task:\nDesign advanced explainability UI showing equations, substituted values, and plain-language intuition.\n\nMust include:\n- ranking and policy decision cards\n- evidence-trace drilldowns\n- explainability levels for novice to expert users","acceptance_criteria":"1) Galaxy-brain cards provide equation/value/intuition triad.\n2) Evidence drilldowns connect UI decisions to trace records.\n3) Multi-level explainability supports novice and expert users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:47.924630947Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:34.632764186Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","explainability","fsfs"],"dependencies":[{"issue_id":"bd-2hz.7.4","depends_on_id":"bd-2hz.5.4","type":"blocks","created_at":"2026-02-13T22:05:34.526370250Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.4","depends_on_id":"bd-2hz.7","type":"parent-child","created_at":"2026-02-13T22:02:47.924630947Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.4","depends_on_id":"bd-2hz.7.1","type":"blocks","created_at":"2026-02-13T22:05:34.420103622Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.4","depends_on_id":"bd-2hz.8.1","type":"blocks","created_at":"2026-02-13T22:05:34.632720755Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7.5","title":"Design degraded-mode UX and operator override controls","description":"Task:\nDefine how fsfs TUI communicates and controls degraded operation states.\n\nMust include:\n- state banners and transition context\n- safe override controls with guardrails\n- audit visibility for manual interventions","acceptance_criteria":"1) Degraded-state UX communicates status and impact clearly.\n2) Override controls are guarded and auditable.\n3) Transition visibility prevents operator confusion during pressure events.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:48.034348914Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:34.845575542Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["degradation","deluxe-tui","fsfs"],"dependencies":[{"issue_id":"bd-2hz.7.5","depends_on_id":"bd-2hz.4.3","type":"blocks","created_at":"2026-02-13T22:05:34.845513907Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.5","depends_on_id":"bd-2hz.7","type":"parent-child","created_at":"2026-02-13T22:02:48.034348914Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.5","depends_on_id":"bd-2hz.7.1","type":"blocks","created_at":"2026-02-13T22:05:34.739889090Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.7.6","title":"Define accessibility, theming, and frame-time quality constraints","description":"Task:\nSpecify accessibility and visual/performance constraints for long-running TUI usage.\n\nMust include:\n- keyboard-only parity\n- high-contrast/reduced-motion profiles\n- frame-time and flicker quality budgets","acceptance_criteria":"1) Accessibility profiles are defined with keyboard-first parity.\n2) Theming and motion options include high-contrast/reduced-motion modes.\n3) Frame-time/flicker quality budgets are explicit and testable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:02:48.145330967Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:34.954701032Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["a11y","deluxe-tui","fsfs"],"dependencies":[{"issue_id":"bd-2hz.7.6","depends_on_id":"bd-2hz.7","type":"parent-child","created_at":"2026-02-13T22:02:48.145330967Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.7.6","depends_on_id":"bd-2hz.7.1","type":"blocks","created_at":"2026-02-13T22:05:34.954657380Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.8","title":"Workstream: Evidence ledger, provenance, and observability fabric","description":"Goal:\nEnsure every adaptive decision and major runtime action is explainable, reproducible, and debuggable.\n\nScope:\n- evidence schemas and trace IDs\n- reproducibility manifests\n- redaction and retention controls","acceptance_criteria":"1) Evidence ledger schema captures major decisions/events with traceability IDs.\n2) Provenance manifests and reproducibility artifacts are generated reliably.\n3) Redaction/retention policies protect sensitive data without losing debuggability.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:11.208233148Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:47.893395299Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["evidence","fsfs","observability","phase-evidence"],"dependencies":[{"issue_id":"bd-2hz.8","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:11.208233148Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.8","depends_on_id":"bd-2hz.1","type":"blocks","created_at":"2026-02-13T22:04:47.803994625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.8","depends_on_id":"bd-2hz.3","type":"blocks","created_at":"2026-02-13T22:04:47.893353781Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.8.1","title":"Define fsfs evidence-ledger taxonomy and trace-link model","description":"Task:\nDefine canonical evidence events and linkage IDs for fsfs runtime decisions and operations.\n\nMust include:\n- trace_id/claim_id/policy_id style linking\n- event families for ingest, query, degrade, override, failures\n- machine-readable schema validation expectations","acceptance_criteria":"1) Evidence event taxonomy and link IDs are standardized.\n2) Schema supports major fsfs runtime decision classes.\n3) Validation rules are defined for producer/consumer conformance.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:19.349863798Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:25.569890259Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["evidence","fsfs","schema"],"dependencies":[{"issue_id":"bd-2hz.8.1","depends_on_id":"bd-2hz.8","type":"parent-child","created_at":"2026-02-13T22:03:19.349863798Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.8.2","title":"Define reproducibility artifact pack and capture lifecycle","description":"Task:\nSpecify reproducibility artifacts for benchmark, test, and incident replays.\n\nMust include:\n- env/manifest/repro-lock style artifact set\n- capture timing and retention policy\n- correlation between artifacts and evidence traces","acceptance_criteria":"1) Repro artifact pack contents and capture points are explicit.\n2) Artifact retention policy balances debuggability and storage cost.\n3) Artifacts are trace-linked for deterministic replay workflows.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:19.465950389Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:25.679792112Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","provenance","repro"],"dependencies":[{"issue_id":"bd-2hz.8.2","depends_on_id":"bd-2hz.8","type":"parent-child","created_at":"2026-02-13T22:03:19.465950389Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.8.3","title":"Design provenance attestation and startup verification flow","description":"Task:\nDefine how fsfs records and validates build/runtime provenance for trust and debugging.\n\nMust include:\n- signed/hashed provenance fields\n- startup validation behavior on mismatch\n- fallback and alert semantics when verification fails","acceptance_criteria":"1) Provenance fields and attestation checks are defined.\n2) Startup mismatch handling has explicit fallback and alert behavior.\n3) Verification model supports debugging and supply-chain confidence.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:19.577273864Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:25.788746081Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["attestation","fsfs","provenance"],"dependencies":[{"issue_id":"bd-2hz.8.3","depends_on_id":"bd-2hz.8","type":"parent-child","created_at":"2026-02-13T22:03:19.577273864Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.8.4","title":"Implement redaction/retention policy engine for logs and evidence","description":"Task:\nCreate policy model for sensitive content handling across logs, artifacts, and explain outputs.\n\nMust include:\n- data class taxonomy and transformation rules\n- default retention windows by artifact type\n- deterministic masking for replay-safe diagnostics","acceptance_criteria":"1) Redaction classes and transformations are deterministic.\n2) Retention rules are explicit per artifact/log category.\n3) Policies preserve enough signal for safe replay/debug use.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:19.682776191Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:25.898935993Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","privacy","redaction"],"dependencies":[{"issue_id":"bd-2hz.8.4","depends_on_id":"bd-2hz.8","type":"parent-child","created_at":"2026-02-13T22:03:19.682776191Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.8.5","title":"Design trace query and replay tooling contract","description":"Task:\nDefine tooling interfaces for querying evidence trails and replaying decisions by trace ID.\n\nMust include:\n- query/filter model for traces\n- replay entrypoint semantics\n- compatibility with CLI and TUI debug flows","acceptance_criteria":"1) Trace query and replay interfaces are clearly specified.\n2) CLI and TUI debug flows can consume the same trace model.\n3) Replay entrypoint requirements are sufficient for incident diagnosis.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:19.795314539Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.010236284Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["debuggability","evidence","fsfs"],"dependencies":[{"issue_id":"bd-2hz.8.5","depends_on_id":"bd-2hz.8","type":"parent-child","created_at":"2026-02-13T22:03:19.795314539Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9","title":"Workstream: Extreme performance engineering and optimization loop","description":"Goal:\nInstitutionalize profile-first optimization with measurable p50/p95/p99 wins and behavior-preserving proofs.\n\nScope:\n- benchmark harness + hotspot matrix\n- targeted optimization tracks\n- regression gates and rollback comparators","acceptance_criteria":"1) Profile-first optimization loop is operationalized with baseline/proof artifacts.\n2) High-impact hotspots have measurable p50/p95/p99 improvement targets.\n3) Regression gates and rollback comparators are enforceable in CI.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T22:01:11.317228892Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:48.223521135Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","optimization","performance","phase-performance"],"dependencies":[{"issue_id":"bd-2hz.9","depends_on_id":"bd-2hz","type":"parent-child","created_at":"2026-02-13T22:01:11.317228892Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.9","depends_on_id":"bd-2hz.3","type":"blocks","created_at":"2026-02-13T22:04:47.979157642Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.9","depends_on_id":"bd-2hz.4","type":"blocks","created_at":"2026-02-13T22:04:48.053908660Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.9","depends_on_id":"bd-2hz.5","type":"blocks","created_at":"2026-02-13T22:04:48.139449348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hz.9","depends_on_id":"bd-2hz.7","type":"blocks","created_at":"2026-02-13T22:04:48.223450353Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9.1","title":"Build fsfs baseline benchmark suite and golden dataset","description":"Task:\nDefine baseline benchmarks for crawl/index/query/TUI with representative corpora and reproducible setup.\n\nMust include:\n- benchmark matrix and dataset profiles\n- baseline comparator definitions\n- artifact capture for statistical analysis","acceptance_criteria":"1) Baseline benchmark matrix covers crawl/index/query/TUI paths.\n2) Golden datasets and comparators are reproducible and versioned.\n3) Artifact capture supports later statistical comparison.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:19.904682984Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.113181014Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["benchmarking","fsfs","performance"],"dependencies":[{"issue_id":"bd-2hz.9.1","depends_on_id":"bd-2hz.9","type":"parent-child","created_at":"2026-02-13T22:03:19.904682984Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9.2","title":"Implement profiling harness and opportunity-matrix workflow","description":"Task:\nOperationalize hotspot identification and prioritization for fsfs optimization cycles.\n\nMust include:\n- flamegraph/heap/syscall profile workflow\n- impact-confidence-effort scoring table\n- one-lever optimization iteration protocol","acceptance_criteria":"1) Profiling workflow captures CPU/alloc/syscall hotspots consistently.\n2) Opportunity matrix scoring guides optimization prioritization.\n3) One-lever iteration protocol is defined and enforceable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.018014097Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.216756243Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","hotspots","profiling"],"dependencies":[{"issue_id":"bd-2hz.9.2","depends_on_id":"bd-2hz.9","type":"parent-child","created_at":"2026-02-13T22:03:20.018014097Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9.3","title":"Optimize crawl/ingest hot path with behavior-preserving proofs","description":"Task:\nPlan targeted optimization track for discovery and ingestion bottlenecks.\n\nMust include:\n- top hotspot candidates and expected gains\n- isomorphism proof checklist for each lever\n- rollback strategy and guardrails","acceptance_criteria":"1) Ingest optimization track names prioritized hotspots and target gains.\n2) Isomorphism proof requirements are explicit per optimization lever.\n3) Rollback guardrails exist for each proposed change class.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.131748224Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.325099418Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","ingestion","optimization"],"dependencies":[{"issue_id":"bd-2hz.9.3","depends_on_id":"bd-2hz.9","type":"parent-child","created_at":"2026-02-13T22:03:20.131748224Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9.4","title":"Optimize query latency path (retrieval/fusion/explanation)","description":"Task:\nPlan targeted optimization track for query-time latency and throughput.\n\nMust include:\n- phase-wise latency decomposition\n- prioritized algorithm/data-structure levers\n- correctness-preserving verification protocol","acceptance_criteria":"1) Query optimization track decomposes latency by stage.\n2) Candidate levers are prioritized by measured impact and effort.\n3) Verification plan preserves ranking correctness and stability.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.247594055Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.433647116Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fsfs","optimization","query"],"dependencies":[{"issue_id":"bd-2hz.9.4","depends_on_id":"bd-2hz.9","type":"parent-child","created_at":"2026-02-13T22:03:20.247594055Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9.5","title":"Optimize TUI frame pipeline and interaction responsiveness","description":"Task:\nPlan optimization track for TUI frame stability and interaction latency under load.\n\nMust include:\n- frame-time hotspot decomposition\n- render/invalidation strategy improvements\n- flicker regression prevention checks","acceptance_criteria":"1) TUI frame optimization track identifies dominant render bottlenecks.\n2) Invalidation/flicker mitigation strategy is defined.\n3) Responsiveness improvements are tied to measurable frame metrics.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.359870313Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.545074375Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["deluxe-tui","fsfs","optimization"],"dependencies":[{"issue_id":"bd-2hz.9.5","depends_on_id":"bd-2hz.9","type":"parent-child","created_at":"2026-02-13T22:03:20.359870313Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hz.9.6","title":"Define statistical performance regression gates and CI integration","description":"Task:\nDefine pass/fail criteria and CI wiring for performance-sensitive workloads.\n\nMust include:\n- p50/p95/p99 and memory budget thresholds\n- statistical confidence policy for benchmark comparisons\n- flaky-run mitigation and triage outputs","acceptance_criteria":"1) Statistical gate criteria are explicit for latency/memory regressions.\n2) CI integration strategy includes confidence and flake handling.\n3) Failure reports provide actionable triage context.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T22:03:20.477108290Z","created_by":"ubuntu","updated_at":"2026-02-13T22:04:26.656341735Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["ci","fsfs","performance-gates"],"dependencies":[{"issue_id":"bd-2hz.9.6","depends_on_id":"bd-2hz.9","type":"parent-child","created_at":"2026-02-13T22:03:20.477108290Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2n6","title":"Negative/Exclusion Query Syntax","description":"Add support for negative/exclusion query syntax (-term, NOT phrase) in both lexical and semantic search. Users need to exclude specific terms or concepts from results.\n\n## Background\n\nExclusion queries are one of the most-requested search features across all search libraries. Users frequently need to find documents about a topic while excluding specific subtopics, authors, or terms. For example: \"rust async -tokio\" (find async Rust content that isn't about tokio) or \"machine learning NOT deep learning\" (find ML content that isn't specifically about deep learning).\n\nCurrently, frankensearch has no built-in support for negative queries. Users must post-filter results, which is both wasteful (computes scores for excluded documents) and imprecise (especially for semantic search where exclusion should reduce relevance scores, not just filter).\n\n## Syntax\n\n- `-term` : exclude documents containing \"term\"\n- `NOT \"exact phrase\"` : exclude documents containing exact phrase\n- `query -excluded` : search for \"query\" but exclude \"excluded\"\n- Multiple exclusions: `query -foo -bar` : exclude both \"foo\" and \"bar\"\n- Escaped dash: `\\-term` : literal dash, not exclusion\n\n## Implementation\n\n### Query Parsing\n\n```rust\npub struct ParsedQuery {\n    pub positive: String,              // The main query (what to find)\n    pub negative_terms: Vec<String>,   // Terms to exclude\n    pub negative_phrases: Vec<String>, // Phrases to exclude\n}\n\nimpl ParsedQuery {\n    pub fn parse(raw: &str) -> Self;\n    pub fn has_negations(&self) -> bool;\n}\n```\n\nThe parser handles:\n- Leading `-` as negation prefix (only when preceded by whitespace or at start)\n- `NOT` keyword (case-insensitive) before a quoted phrase\n- Escaped dashes (`\\-`) as literal characters\n- Edge cases: query with only negations, empty positive portion, consecutive negations\n\n### Lexical Implementation (Tantivy)\n\nNegative terms translate directly to BooleanQuery with MUST_NOT clauses. Tantivy handles this natively and efficiently — MUST_NOT clauses are evaluated during posting list intersection, so excluded documents never reach scoring.\n\n```rust\n// Pseudo-code for Tantivy query construction\nlet mut clauses = vec![];\nclauses.push((Occur::Must, positive_query));\nfor term in negative_terms {\n    clauses.push((Occur::MustNot, TermQuery::new(term)));\n}\nfor phrase in negative_phrases {\n    clauses.push((Occur::MustNot, PhraseQuery::new(phrase)));\n}\nBooleanQuery::new(clauses)\n```\n\n### Semantic Implementation\n\nSemantic exclusion is inherently approximate because it operates in embedding space. The approach:\n\n1. Embed each negative term/phrase using the same embedder as the positive query\n2. During vector search, compute cosine similarity of each candidate with negative embeddings\n3. Penalize candidates similar to negative terms:\n\n```\nadjusted_score = score - beta * max(sim(doc, neg_i) for neg_i in negative_embeddings)\n```\n\nWhere beta = 0.3 (configurable) controls the strength of the negative penalty. The `max` function ensures that a document similar to ANY negative term is penalized, but the penalty doesn't stack (a document about both \"foo\" and \"bar\" isn't double-penalized).\n\nThis is a novel approach that leverages frankensearch's embedding infrastructure. It provides approximate but effective semantic exclusion without requiring negative term annotation in the index.\n\n### Integration\n\n- **QueryClass::classify** should be called on the positive portion only (negations don't affect query classification)\n- **RRF fusion** applies negative penalties after individual source scoring (both lexical and semantic exclusions are resolved before fusion)\n- **TwoTierSearcher** passes negative embeddings to both fast and quality tier\n\n## Justification\n\nExclusion queries are one of the most-requested search features. Without them, users must post-filter results, which is both wasteful and imprecise for semantic search. The semantic negative embedding approach is novel and leverages frankensearch's embedding infrastructure to provide approximate but effective semantic exclusion — something most vector search libraries don't support at all.\n\n## Considerations\n\n- Performance: negative embedding computation adds one embedding call per negative term. For typical queries with 1-2 negations, this is <10ms overhead.\n- Beta tuning: 0.3 is conservative. Higher values (0.5+) more aggressively exclude, but may suppress legitimate results that happen to share vocabulary with the negative term. Make beta configurable per-query.\n- Empty positive query: if the query is only negations (e.g., \"-spam -ads\"), there's no positive signal to search for. Return an error or empty results with a diagnostic message.\n- Interaction with PRF (Idea 5): PRF should operate on the positive embedding only. Negative embeddings are a separate concern applied during scoring.\n\n## Testing\n\n- [ ] Unit: ParsedQuery::parse handles single negative term (`query -foo`)\n- [ ] Unit: ParsedQuery::parse handles multiple negative terms (`query -foo -bar`)\n- [ ] Unit: ParsedQuery::parse handles NOT phrase (`query NOT \"exact phrase\"`)\n- [ ] Unit: ParsedQuery::parse handles mixed positive and negative\n- [ ] Unit: ParsedQuery::parse handles escaped dashes (`\\-literal`)\n- [ ] Unit: ParsedQuery::parse handles only negatives (empty positive)\n- [ ] Unit: ParsedQuery::parse handles empty query\n- [ ] Unit: ParsedQuery::parse handles consecutive spaces and edge formatting\n- [ ] Integration: lexical exclusion produces correct results (excluded terms not in results)\n- [ ] Integration: semantic penalty reduces score of similar documents (verify score delta)\n- [ ] Integration: combined lexical + semantic exclusion in hybrid search\n- [ ] Benchmark: overhead of negative embedding computation (1, 2, 5 negative terms)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T22:01:41.696965116Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:27.008262237Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2n6","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T22:02:26.900110352Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2n6","depends_on_id":"bd-3un.43","type":"blocks","created_at":"2026-02-13T22:02:27.008213325Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ps","title":"Implement sequential testing gates (e-processes) for phase transitions","description":"Implement anytime-valid sequential testing (e-processes) for phase transition decisions in the TwoTierSearcher. An e-process accumulates evidence over time and can be checked at ANY query — no predetermined sample sizes needed. When the e-value exceeds 1/alpha, the decision (skip/don't skip quality tier) is statistically guaranteed.\n\nGraveyard entry: §0.18 Sequential Testing (e-processes, anytime-valid)\nEV score: 9.0 (Impact=3, Confidence=4, Reuse=3, Effort=2, Friction=2)\nPriority tier: B\n\nArchitecture:\npub struct PhaseGate {\n    e_value: f64,                    // Running e-value (product of e-factors)\n    alpha: f64,                      // Significance level (default 0.05)\n    decision: Option<PhaseDecision>, // None until evidence sufficient\n    observations: u64,               // Query count since last reset\n    timeout_queries: u64,            // Max queries before forced decision (default 500)\n}\n\npub enum PhaseDecision {\n    SkipQuality,   // Evidence: fast tier is sufficient\n    AlwaysRefine,  // Evidence: quality tier adds value\n}\n\nE-process update (per query):\n1. Observe: (fast_score, quality_score, user_click/relevance) for top-k results\n2. Compute e-factor: likelihood ratio test statistic for \"quality adds value\" vs \"fast sufficient\"\n   e_factor = P(observation | quality_adds_value) / P(observation | fast_sufficient)\n3. Update: e_value *= e_factor\n4. Check: if e_value > 1/alpha → PhaseDecision::AlwaysRefine\n         if 1/e_value > 1/alpha → PhaseDecision::SkipQuality\n         if observations > timeout → default to AlwaysRefine + reset\n\nProperties (Ville's inequality):\n- Under null hypothesis, P(e_value ever exceeds 1/alpha) <= alpha\n- Can check at any time without multiple-testing correction\n- Accumulated evidence is never wasted (unlike fixed-horizon tests)\n\nIntegration with bd-3un.24 (TwoTierSearcher):\n- PhaseGate runs alongside progressive iterator\n- Before quality embedding: check gate.decision\n- If SkipQuality: yield SearchPhase::RefinementFailed(\"skipped by e-gate\")\n- If None: proceed with normal refinement\n- After each query: gate.update(observation)\n\nComposability with bd-21g (adaptive Bayesian fusion):\n- E-process gates the PHASE decision (skip/refine)\n- Bayesian posterior tunes the PARAMETERS (K, blend_factor)\n- Timescale separation: e-process operates per-query; Bayesian updates per-window\n- Interference test: gate decision is binary (skip/refine) and does not affect parameter tuning\n\nBudgeted mode: O(1) per query update (single multiplication). Memory: 3 f64 values. <10ns per decision.\n\nFallback: gate.decision = None → always refine (safe default). Timeout after 500 queries → reset.\n\nFile: frankensearch-fusion/src/phase_gate.rs\n\nReference: Ramdas et al. (2020) \"Admissible Anytime-Valid Sequential Testing\", Grunwald et al. (2019) \"Safe Testing\"\nBaseline comparator: Fixed threshold skip (current bd-3un.24 comment), always-refine (safe default)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:45:46.915253464Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:54.603176672Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["adaptive","graveyard","phase7","sequential-testing"],"dependencies":[{"issue_id":"bd-2ps","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.522218639Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ps","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:50:54.603128913Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ps","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T20:46:17.183307375Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ps","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:50:02.884884749Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ps","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T21:22:22.344363177Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":85,"issue_id":"bd-2ps","author":"Dicklesworthstone","text":"REFINEMENT PASS 1 — Architecture clarifications and integration notes:\n\n1. DEPENDENCY ADDED: bd-2ps now depends on bd-3un.38 (test fixture corpus) in addition to bd-3un.24 (TwoTierSearcher). The test corpus provides the (query, relevance) pairs needed to compute e-factors during the calibration/warm-up phase of the e-process.\n\n2. INTERFERENCE WITH bd-21g (Bayesian adaptive fusion): The bead body already documents timescale separation (e-process per-query, Bayesian per-window) and orthogonality (gate is binary skip/refine, doesn't affect parameter tuning). This is correct and sufficient. An interference microbench should verify: enabling PhaseGate does not change the posterior convergence of AdaptiveFusionParams.\n\n3. E-FACTOR COMPUTATION DETAIL: The likelihood ratio for \"quality adds value\" vs \"fast sufficient\" should use:\n   - Numerator: P(rank_correlation(fast, quality) < tau | quality helps) — modeled as Beta(2, 5) prior\n   - Denominator: P(rank_correlation(fast, quality) < tau | fast sufficient) — modeled as Beta(5, 2) prior\n   Where rank_correlation is Kendall's tau between fast-only and quality-refined top-k.\n   This gives a concrete, computable e-factor per query.\n\n4. RESET SEMANTICS: When the e-process times out (observations > timeout_queries), it resets to e_value = 1.0 (neutral). This is correct because the data distribution may have shifted (new documents indexed). The reset allows re-accumulation of evidence from scratch.\n\n5. TEST REQUIREMENTS (covered by bd-3un.31): Add tests for:\n   - E-value stays bounded under null (quality always helps): simulate 1000 queries where quality improves ranking, verify e_value never triggers SkipQuality\n   - E-value triggers SkipQuality when fast is sufficient: simulate queries where fast and quality rankings are identical\n   - Timeout resets e_value to 1.0\n   - PhaseDecision::SkipQuality produces SearchPhase::RefinementFailed\n   - Composability: PhaseGate + AdaptiveFusionParams produce correct results together","created_at":"2026-02-13T20:51:38Z"},{"id":213,"issue_id":"bd-2ps","author":"Dicklesworthstone","text":"REVISION (review pass 5 - dependency corrections):\n\n1. REMOVED bd-21g DEPENDENCY: The PhaseGate decides a binary question (should we run quality refinement?) while bd-21g (Bayesian adaptive fusion) decides continuous parameters (blend_factor, K). These operate at different abstraction levels. The PhaseGate works perfectly without bd-21g. The TwoTierSearcher (bd-3un.24) is the integration point where both compose. Interaction testing belongs in bd-3un.31, not as a build dependency.\n\n2. ADDED bd-3un.5 DEPENDENCY: PhaseGate produces SearchPhase::RefinementFailed and uses SkipReason. These types are defined in bd-3un.5. Required for compilation.\n\n3. ASUPERSYNC NOTE: PhaseGate.decision() is pure computation (single multiplication). No async needed. Confirm: this method remains sync.\n","created_at":"2026-02-13T21:23:14Z"},{"id":247,"issue_id":"bd-2ps","author":"Dicklesworthstone","text":"REVIEW FIX — E-factor specification and hypothesis clarification:\n\n1. EXPLICIT HYPOTHESES:\n   H0 (null): \"Fast tier is sufficient — quality refinement does not improve ranking\"\n   H1 (alternative): \"Quality tier adds value — refinement significantly changes ranking\"\n\n   Under H0, fast-only and quality-refined rankings are essentially the same (high Kendall tau).\n   Under H1, quality refinement produces meaningfully different (and better) rankings (low Kendall tau).\n\n2. E-FACTOR COMPUTATION (promoted from comment to body-level specification):\n   Per query, compute Kendall's tau between fast-only top-k and quality-refined top-k.\n\n   e_factor = Beta_pdf(tau; alpha=2, beta=5) / Beta_pdf(tau; alpha=5, beta=2)\n\n   Where:\n   - Numerator: likelihood of observed tau under H1 (quality helps → expect low tau)\n     Beta(2, 5) has mode at 0.2 — concentrates mass on low correlation\n   - Denominator: likelihood of observed tau under H0 (fast sufficient → expect high tau)\n     Beta(5, 2) has mode at 0.8 — concentrates mass on high correlation\n   - tau is Kendall's rank correlation, rescaled to [0, 1] as (tau + 1) / 2\n\n   When quality significantly re-ranks results (low tau):\n     e_factor > 1 → evidence accumulates toward H1 (AlwaysRefine)\n   When quality barely changes ranking (high tau):\n     e_factor < 1 → evidence accumulates toward H0 (SkipQuality)\n\n3. TIMEOUT CONSIDERATION: The 500-query timeout should also have a time-based alternative:\n   pub struct PhaseGate {\n       // ...\n       timeout_queries: u64,           // Max queries before forced decision (default 500)\n       timeout_duration: Duration,     // Max time before forced decision (default 1 hour)\n   }\n   Timeout triggers whichever comes first. This handles both high-QPS and low-QPS scenarios.\n\n4. MISSING DEPENDENCY: Add bd-3un.2 (SearchError) for error paths in the phase gate (e.g., invalid alpha, NaN e_value).","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-2rq","title":"Multi-Index Federated Search with Scatter-Gather Fusion","description":"Implement federated search across multiple frankensearch indices with scatter-gather fusion. A single query fans out to N independent indices (each with their own embeddings and lexical data), results are gathered and fused into a unified ranking.\n\n## Use Case\n\nA consumer has multiple data sources (e.g., xf has tweets, likes, bookmarks, DMs — each with separate indices). Currently, the consumer must search each index separately and merge results manually. Federated search automates this with proper score normalization and weighted fusion.\n\n## Design\n\n```rust\npub struct FederatedSearcher {\n    indices: Vec<(String, TwoTierSearcher, f64)>,  // (name, searcher, weight)\n}\n\npub struct FederatedConfig {\n    pub fusion_method: FederatedFusion,\n    pub timeout_ms: u64,            // Per-index timeout (default: 500ms)\n    pub min_indices: usize,         // Minimum indices that must respond (default: 1)\n}\n\npub enum FederatedFusion {\n    Rrf { k: f64 },                 // Standard RRF across indices\n    WeightedScore,                  // Weighted sum of normalized scores\n    CombMNZ,                        // CombMNZ: score * count_of_indices_containing_doc\n}\n\nimpl FederatedSearcher {\n    pub async fn search(&self, cx: &Cx, query: &str, limit: usize) -> Vec<FederatedHit>;\n}\n\npub struct FederatedHit {\n    pub hit: FusedHit,\n    pub source_index: String,       // Which index this came from\n    pub source_rank: usize,         // Rank within that index\n    pub appeared_in: Vec<String>,   // All indices containing this doc\n}\n```\n\n## Scatter-Gather Pattern\n\n1. **Scatter**: Send query to all indices concurrently (using asupersync structured concurrency)\n2. **Wait**: Collect results with per-index timeout (graceful degradation if some indices are slow)\n3. **Normalize**: Apply per-index score normalization (important because different indices have different score distributions)\n4. **Gather**: Fuse results using chosen fusion method\n5. **Dedup**: Merge hits that appear in multiple indices (boost by appearance count for CombMNZ)\n\n## Why This Matters\n\nAs frankensearch is adopted by more consumers with diverse data sources, federated search becomes essential. xf alone has 4+ distinct content types that benefit from separate indices with tailored embeddings. Without federation, consumers reimplement the scatter-gather pattern each time.\n\nThe CombMNZ fusion method is particularly interesting: documents that appear in multiple indices are likely more relevant (they match across different embedding spaces and content types).\n\n## Testing\n\n- Unit: single index → behaves like normal search\n- Unit: two indices → results merged correctly\n- Unit: weighted fusion → weights affect ranking\n- Unit: CombMNZ → multi-index appearance boosts score\n- Unit: timeout → graceful degradation when one index is slow\n- Unit: min_indices enforcement\n- Integration: federated search across 3 test indices with different content\n- Benchmark: federation overhead vs individual searches","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T21:59:54.718422127Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:28.764323838Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2rq","depends_on_id":"bd-26e","type":"blocks","created_at":"2026-02-13T22:05:28.764273814Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2rq","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T22:02:20.021475738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2rq","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T22:02:16.652219619Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2tv","title":"Implicit Relevance Feedback Loop with Boost Map","description":"Implement an implicit relevance feedback loop that learns from consumer usage patterns to boost or demote documents in future searches. The consumer signals relevance implicitly (clicks, dwell time, selection) and the system maintains a per-document boost map.\n\n## Design\n\n```rust\npub struct FeedbackCollector {\n    boost_map: RwLock<HashMap<String, DocumentBoost>>,\n    config: FeedbackConfig,\n    decay_clock: Instant,\n}\n\npub struct FeedbackConfig {\n    pub enabled: bool,               // Default: false (opt-in)\n    pub decay_halflife_hours: f64,   // Boost decay half-life (default: 168 = 1 week)\n    pub max_boost: f64,              // Maximum boost multiplier (default: 2.0)\n    pub min_boost: f64,              // Minimum boost multiplier (default: 0.5)\n    pub signal_weights: SignalWeights,\n}\n\npub struct SignalWeights {\n    pub click: f64,       // Default: 1.0\n    pub dwell_long: f64,  // Default: 2.0 (>30s dwell time)\n    pub select: f64,      // Default: 3.0 (explicit selection/use)\n    pub skip: f64,        // Default: -0.5 (presented but not clicked)\n}\n\npub struct DocumentBoost {\n    pub boost: f64,               // Current multiplicative boost\n    pub positive_signals: u32,    // Total positive interactions\n    pub negative_signals: u32,    // Total negative interactions\n    pub last_signal: Instant,     // For decay computation\n}\n\npub enum FeedbackSignal {\n    Click { doc_id: String, query: String, rank: usize },\n    Dwell { doc_id: String, duration_secs: f64 },\n    Select { doc_id: String, query: String },\n    Skip { doc_id: String, query: String, rank: usize },\n}\n```\n\n## Integration with RRF\n\n- After RRF fusion, multiply each hit's score by its boost: `final_score = rrf_score * boost_map.get(doc_id).unwrap_or(1.0)`\n- Boost is applied AFTER fusion but BEFORE limit/offset, so it affects ranking\n\n## Decay Mechanism\n\n- Boosts decay exponentially: `effective_boost = 1.0 + (stored_boost - 1.0) * 2^(-elapsed_hours / halflife)`\n- Lazy decay: compute effective boost at query time, not on a timer\n- Periodic cleanup: remove entries with effective_boost ≈ 1.0 (±0.01)\n\n## Connection to Bayesian Adaptive Fusion (bd-21g)\n\n- The boost map provides signal data that the Bayesian online learner can use to update RRF K and blend factor\n- Specifically: clicked documents provide positive relevance labels, skipped documents provide negative labels\n- This creates a virtuous cycle: feedback → better fusion → better results → better feedback\n\n## Why This Matters\n\nStatic ranking treats every document the same regardless of how users interact with it. Implicit feedback allows frankensearch to learn from usage patterns and continuously improve result quality. This is especially valuable for cass (coding agent session search) where users repeatedly search for and use the same high-value sessions.\n\nThe boost map is intentionally simple (multiplicative boost with decay) rather than a full learning-to-rank system. This keeps it lightweight, interpretable, and easy to debug — important properties for a library component.\n\n## Testing\n\n- Unit: feedback signal updates boost correctly\n- Unit: decay computation at various time intervals\n- Unit: max_boost and min_boost clamping\n- Unit: cleanup removes near-1.0 boosts\n- Unit: skip signal reduces boost\n- Integration: feedback loop improves ranking over repeated queries\n- Integration: boost persistence (serialize/deserialize boost map)\n- Integration: interaction with RRF scoring\n- Benchmark: boost map lookup overhead per search result","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T22:02:02.581644590Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:45.052309405Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2tv","depends_on_id":"bd-21g","type":"blocks","created_at":"2026-02-13T22:02:45.052267958Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2tv","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T22:02:44.831563140Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2tv","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T22:02:44.942701265Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2u4","title":"Prefix-Optimized Incremental Search Mode","description":"Implement an incremental search mode optimized for as-you-type search experiences. When a user types character by character, each keystroke triggers a new search. Incremental mode reuses work from the previous keystroke to avoid redundant computation.\n\n## Design\n\n```rust\npub struct IncrementalSearcher {\n    last_query: Option<String>,\n    last_results: Option<Vec<FusedHit>>,\n    fast_embedder: Arc<dyn Embedder>,\n    config: IncrementalConfig,\n}\n\npub struct IncrementalConfig {\n    pub max_latency_ms: u64,         // Target latency per keystroke (default: 50ms)\n    pub min_prefix_len: usize,       // Don't search until this many chars (default: 2)\n    pub debounce_ms: u64,            // Debounce rapid keystrokes (default: 100ms)\n    pub use_hash_embedder: bool,     // Use FNV-1a for speed (default: true for first 3 chars)\n    pub refine_after_pause_ms: u64,  // Upgrade to full search after pause (default: 300ms)\n}\n```\n\n## Strategy Ladder\n\n1. **Prefix 1-2 chars**: Tantivy prefix query only (fastest, <5ms)\n2. **Prefix 3-4 chars**: Tantivy prefix + FNV-1a hash embedding (fast, <10ms)\n3. **Prefix 5+ chars**: Full hybrid search with fast embedder (potion, <15ms)\n4. **After 300ms pause**: Full two-tier search with quality refinement (~150ms)\n\n## Incremental Optimization\n\n- If new query is a prefix extension of last query (e.g., \"sea\" → \"sear\" → \"search\"):\n  - Reuse last result set as candidate pool (don't re-scan full index)\n  - Only re-rank within candidate pool + new Tantivy prefix matches\n  - This gives O(k) instead of O(n) for each keystroke\n\n## Lexical Integration\n\n- Tantivy natively supports prefix queries on `content_prefix` field\n- Each keystroke refines the prefix query (additive — can only narrow results)\n\n## Semantic Integration\n\n- Re-embedding per keystroke is too expensive for quality tier\n- Fast tier (potion, 0.57ms) is acceptable\n- FNV-1a hash embedding (0.07ms) is even better for very short prefixes\n\n## Why This Matters\n\nAs-you-type search is the expected UX in modern applications. Without incremental mode, consumers must throttle search calls (losing responsiveness) or accept high latency per keystroke (poor UX). The strategy ladder ensures consistently fast response regardless of query length.\n\n## Testing\n\n- Unit: min_prefix_len enforced (no search for 1 char)\n- Unit: prefix extension detection\n- Unit: candidate pool reuse (results subset of previous)\n- Unit: strategy selection based on prefix length\n- Unit: debounce timing\n- Integration: simulate typing sequence, verify incremental speedup\n- Integration: verify result quality matches non-incremental for completed query\n- Benchmark: per-keystroke latency across strategy ladder","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T22:01:18.624497386Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:36.515828075Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2u4","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T22:02:36.304274258Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2u4","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T22:02:36.411429998Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2u4","depends_on_id":"bd-3un.6","type":"blocks","created_at":"2026-02-13T22:02:36.515778201Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yj","title":"Implement conformal prediction wrappers for search quality guarantees","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:31:20.613664049Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:53.765876878Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","conformal","phase10","quality"],"dependencies":[{"issue_id":"bd-2yj","depends_on_id":"bd-3un","type":"blocks","created_at":"2026-02-13T20:31:46.351555875Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yj","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:50:53.658742528Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yj","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T20:31:38.031766694Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yj","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:31:38.113929262Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yj","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T21:50:53.765831523Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":36,"issue_id":"bd-2yj","author":"Dicklesworthstone","text":"Implement conformal prediction wrappers for distribution-free search quality guarantees. This provides formal coverage bounds on search results without distributional assumptions.\n\nMATHEMATICAL FOUNDATION:\n\nConformal prediction guarantees: P(relevant_doc in top_k) >= 1 - alpha, for any alpha, with NO distributional assumptions. This is the strongest formal guarantee possible for search quality.\n\nCore algorithm:\n1. CALIBRATION PHASE: Given a calibration set of (query, known_relevant_doc) pairs:\n   - For each pair, compute the nonconformity score = rank of relevant doc in search results\n   - Sort these scores to form the empirical distribution\n\n2. PREDICTION PHASE: For a new query:\n   - required_k = ceil((1-alpha) quantile of calibration scores) + 1\n   - Guarantee: with probability >= 1-alpha, the relevant doc is in the top required_k\n\nImplementation:\n\npub struct ConformalSearchCalibration {\n    nonconformity_scores: Vec<f32>,  // Sorted calibration scores\n    n_calibration: usize,\n}\n\nimpl ConformalSearchCalibration {\n    pub fn calibrate(searcher: &TwoTierSearcher, cal_set: &[(String, String)]) -> Self;\n\n    // Required k to guarantee coverage at level (1-alpha)\n    pub fn required_k(&self, alpha: f32) -> usize;\n\n    // Prediction interval for the rank of a relevant document\n    pub fn rank_prediction_interval(&self, alpha: f32) -> (usize, usize);\n\n    // p-value for a specific result: how unusual is this rank?\n    pub fn p_value(&self, observed_rank: usize) -> f32;\n}\n\nAdditional capabilities:\n\n1. Adaptive Conformal Prediction (ACI):\n   For non-stationary data (index changes over time), use Gibbs & Candes 2021 adaptive conformal:\n   - alpha_t = alpha + gamma * (err_{t-1} - alpha)\n   - Maintains coverage guarantee even as distribution shifts\n   - gamma controls adaptation speed (default: 0.01)\n\n2. Per-Query-Type Calibration:\n   Separate calibration sets per query classification (bd-3un.43):\n   - Short queries need different k than long queries\n   - Identifier queries need different k than natural language\n\n3. Conditional Coverage via Mondrian Conformal:\n   Guarantee coverage within each query type, not just marginally.\n\nFile: frankensearch-fusion/src/conformal.rs\nDependencies: bd-3un.24 (TwoTierSearcher), bd-3un.38 (test fixtures for calibration data)\n\nAlien-artifact characteristics:\n- Mathematical rigor: Vovk et al. conformal prediction framework\n- Formal guarantees: distribution-free finite-sample coverage P >= 1-alpha\n- Complete explainability: p-values for each result, required_k derivation\n- Graceful degradation: works with any embedder, any index size\n- Operational excellence: O(log n) per query (binary search on sorted calibration scores)\n","created_at":"2026-02-13T20:31:31Z"},{"id":245,"issue_id":"bd-2yj","author":"Dicklesworthstone","text":"REVIEW FIX — Missing tests, deps, and ASUPERSYNC note for conformal prediction:\n\n1. MISSING DEPENDENCIES — Add:\n   - bd-3un.2 (SearchError for calibration/prediction failures)\n   - bd-3un.5 (ScoredResult types for evaluating search quality)\n   - bd-3un.43 (QueryClass for Mondrian conformal, soft dependency)\n\n2. DEPENDENCY TYPE FIX: The epic relationship should be parent-child, not blocks.\n\n3. ASUPERSYNC NOTE: The calibrate() method runs searches via TwoTierSearcher, which is async. Therefore:\n   pub async fn calibrate(cx: &Cx, searcher: &TwoTierSearcher, cal_set: &[(String, String)]) -> Result<Self, SearchError>\n   Prediction methods (required_k, p_value) are sync — pure computation on precomputed nonconformity scores.\n\n4. NONCONFORMITY SCORE IMPROVEMENT: The body uses \"rank of relevant doc in search results\" as the nonconformity score. This produces integer-valued scores which can lead to unnecessarily large required_k values (ties are broken conservatively). Consider offering both:\n   - Rank-based (current, simpler, coarser)\n   - Score-based: 1.0 - cosine_similarity(query_emb, relevant_doc_emb), which provides continuous scores and tighter prediction sets\n\n5. MONDRIAN CONFORMAL: The body mentions per-query-type coverage but doesn't sketch the implementation. Add:\n   Mondrian conformal = separate ConformalSearchCalibration per QueryClass. The calibrate() method partitions cal_set by QueryClass::classify(query) and fits separate nonconformity distributions. This requires sufficient calibration data per class (minimum 20 per class recommended).\n\n6. TEST REQUIREMENTS (this bead had NONE):\n   - Coverage guarantee: on held-out test set, P(relevant_doc in top_required_k) >= 1-alpha\n   - Required_k monotonicity: lower alpha → higher required_k\n   - Required_k bounds: required_k >= 1 always, required_k <= calibration set size\n   - p_value uniformity: under null (random ranking), p-values are approximately Uniform[0,1]\n   - p_value bounds: 0 <= p_value <= 1\n   - ACI adaptation: after distribution shift (new index), alpha_t adjusts and coverage recovers\n   - Calibration round-trip: calibrate, serialize to JSON, deserialize, same required_k\n   - Empty calibration set: returns error (not panic)\n   - Single-element calibration set: works correctly (required_k = 1 for alpha < 1)\n   - Mondrian: per-class required_k values are independent\n   - Mondrian: class with 0 calibration data → falls back to global calibration","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-2yu","title":"Epic: Build FrankenTUI observability control plane for frankensearch fleet","description":"Context:\nBuild a first-class FrankenTUI-powered operations console for frankensearch that automatically discovers running frankensearch-enabled applications on a machine and exposes a unified real-time + historical control plane.\n\nWhy this exists:\n- frankensearch will replace bespoke search stacks in multiple projects (/dp/coding_agent_session_search, /dp/xf, /dp/mcp_agent_mail_rust, /dp/frankenterm, and future hosts).\n- Operators need one place to answer: what is indexed, what is stale, what is embedding now, what is resource usage, what are search latencies, what is actively being searched, and whether SLO/error budgets are healthy.\n- We want to leverage proven high-end FrankenTUI patterns from ftui-demo-showcase: screen registry, command palette, action timeline, performance HUD, explainability cockpit, deterministic replay, and accessibility overlays.\n\nMandatory product outcomes:\n1. Auto-detect running frankensearch instances and identify host project/integration with confidence metadata.\n2. Per-project dashboards with index size (words/tokens/lines/bytes/docs), embedding progress, CPU/memory/IO footprint + trends, and SLO health indicators.\n3. Live streaming search feed and aggregate counters over windows: 1m, 15m, 1h, 6h, 24h, 3d, 1w.\n4. Historical stats persisted in frankensqlite with retention/downsampling plus anomaly materialization.\n5. Strong quality bar: comprehensive unit tests + deterministic snapshot/e2e/perf/fault/soak scripts with detailed logging artifacts.\n6. Integration model scales beyond initial hosts via adapter SDK and conformance harness.","acceptance_criteria":"1) Fleet control-plane TUI discovers active frankensearch instances with reliable project attribution.\n2) Per-project dashboards show index size, embedding progress, search latency/memory, resource trends, and SLO/error-budget state.\n3) Live stream + historical windows (1m/15m/1h/6h/24h/3d/1w) are available from frankensqlite-backed data.\n4) Comprehensive unit/snapshot/e2e/perf/fault/soak tests run in CI with detailed artifacts and replay handles.\n5) Adapter SDK + conformance harness support current and future host integrations consistently.\n6) Operator docs/runbook and usability pilot validation confirm production-ready workflows.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-13T20:55:42.482623006Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:45.611612876Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["control-plane","epic","frankensearch","observability","tui"],"comments":[{"id":89,"issue_id":"bd-2yu","author":"Dicklesworthstone","text":"Background & design intent: This epic is intentionally modeled after the strongest ftui-demo-showcase patterns (dashboard composition, action timeline, performance HUD, explainability cockpit, virtualized search, deterministic replay, and accessibility overlays). We are explicitly avoiding a minimal 'stats table' TUI. The target is a compelling control plane that helps operators quickly diagnose search quality, indexing throughput, resource pressure, and live query behavior across multiple host projects.\\n\\nData strategy note: short-lived live streams must be complemented by durable historical storage in frankensqlite so all key windows (1m, 15m, 1h, 6h, 24h, 3d, 1w) can be queried without recomputation bottlenecks.\\n\\nExecution strategy note: we front-load contract + discovery + storage design to prevent UI rework. We then implement shell/framework before screens, then lock in deterministic tests/snapshots/e2e/perf budgets before rollout.","created_at":"2026-02-13T20:56:47Z"},{"id":220,"issue_id":"bd-2yu","author":"Dicklesworthstone","text":"Priority/due tuning pass (2026-02-13):\\n- Promoted critical unlockers (IA/contracts/discovery/storage core/shell core) to P0.\\n- Kept broad implementation, screens, and quality suites at P1.\\n- Kept host-specific adapters and usability pilot at P2.\\n- Added phase due dates: 2026-02-27 (IA/contracts), 2026-03-13 (core data+shell), 2026-03-27 (mid-layer), 2026-04-10 (screens+host adapters), 2026-04-24 (quality), 2026-05-01 (docs/CI/pilot).\\nThis sequencing is intended to maximize unblock rate while preserving scope fidelity.","created_at":"2026-02-13T21:44:45Z"}]}
{"id":"bd-2yu.1","title":"Workstream: UX architecture and FrankentUI pattern extraction for frankensearch ops TUI","description":"Goal:\nTranslate the strongest patterns from /dp/frankentui and ftui-demo-showcase into a concrete UX and information architecture blueprint for frankensearch operations.\n\nScope:\n- Define user personas (operator, developer, SRE) and high-priority decisions each screen must support.\n- Freeze top-level IA: screen registry, category groupings, navigation semantics, overlay model, command palette action taxonomy.\n- Encode visual direction and interaction patterns to avoid generic/flat dashboards.\n\nOutputs:\n- Reusable design decisions linked to specific ftui-demo-showcase precedents.\n- Screen-level success criteria and acceptance checklist for downstream implementation beads.","acceptance_criteria":"1) Pattern matrix maps concrete ftui-demo-showcase capabilities to frankensearch ops use-cases.\\n2) Final IA/screen registry/navigation model is approved and unambiguous.\\n3) Downstream screen tasks can be implemented without re-litigating UX foundations.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:42.577760820Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:46.893812239Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","frankensearch","phase-ux","tui","ux"],"dependencies":[{"issue_id":"bd-2yu.1","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:42.577760820Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":90,"issue_id":"bd-2yu.1","author":"Dicklesworthstone","text":"Future-self rationale: this workstream exists to prevent random screen sprawl. Every dashboard decision should map to an operator task and to a proven ftui pattern. If a proposed screen element cannot be tied to a decision (detect outage, identify stale index, compare host health, etc.), it should probably not be in v1.","created_at":"2026-02-13T20:56:47Z"}]}
{"id":"bd-2yu.1.1","title":"Extract reusable advanced UX patterns from ftui-demo-showcase into frankensearch TUI blueprint","description":"Task:\nDeeply audit ftui-demo-showcase screens and extract reusable patterns for frankensearch operations UX.\n\nMust capture:\n- Dashboard composition and tile drilldown patterns.\n- Action timeline/event stream patterns.\n- Performance HUD techniques (latency percentiles, sparkline, degradation tiers).\n- Explainability/evidence ledger presentation.\n- Virtualized search + large-list handling.\n- Accessibility panel, keyboard model, and command palette ergonomics.\n\nDeliverable:\nA concrete pattern matrix: source screen -> reusable mechanism -> frankensearch TUI usage.\n\nTesting/logging requirement:\nInclude deterministic reproduction notes for every selected pattern so implementation can be snapshot-tested later without ambiguity.","acceptance_criteria":"1) Pattern extraction document covers dashboard/timeline/perf/evidence/virtualization/a11y/command-palette patterns.\\n2) Each pattern includes reuse guidance and deterministic testing notes.\\n3) At least one explicit anti-pattern to avoid is documented per major category.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:42.674712759Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:21.282025171Z","due_at":"2026-02-27T14:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","showcase-extraction","tui","ux"],"dependencies":[{"issue_id":"bd-2yu.1.1","depends_on_id":"bd-2yu.1","type":"parent-child","created_at":"2026-02-13T20:55:42.674712759Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":174,"issue_id":"bd-2yu.1.1","author":"Dicklesworthstone","text":"REVISION: Pattern Extraction Task Details\n\n1. Source Location:\n   ftui-demo-showcase is at /dp/frankentui (the FrankenTUI demo crate).\n   Key files to audit:\n   - src/screens/ (all screen implementations)\n   - src/app.rs (app shell, registry, navigation)\n   - src/widgets/ (reusable widget library)\n   - src/overlays/ (command palette, help, alerts)\n\n2. Deliverable Format:\n   A markdown document at docs/ux-patterns.md with:\n   - Pattern name, source file reference, and screenshot description\n   - Reusability assessment (copy verbatim / adapt / inspire)\n   - Frankensearch applicability (which screen needs this pattern)\n   Minimum 12 patterns to consider this task complete.\n\n3. Mandatory Patterns to Extract:\n   a) Dashboard tile composition (grid layout with drilldown)\n   b) Action timeline / event stream (scrollable, filterable)\n   c) Performance HUD (real-time metrics overlay)\n   d) Explainability cockpit (evidence ledger visualization)\n   e) Command palette (fuzzy search, categorized actions)\n   f) Screen registry (metadata-driven navigation)\n   g) Status bar chrome (connection, resource, time)\n   h) Accessibility controls (contrast, motion, text size)\n   i) Deterministic replay mode (seeded RNG, tick control)\n   j) Virtualized lists (large dataset scrolling)\n   k) Sparkline/mini-chart widgets (inline trend visualization)\n   l) Alert/notification toast system (severity-colored, auto-dismiss)\n\n4. Done Criteria:\n   - All 12 mandatory patterns documented\n   - Each pattern has a clear \"use in frankensearch ops\" mapping\n   - Patterns reference specific source files in /dp/frankentui\n   - The document is self-contained (no external context needed)\n\n5. Relationship to bd-2yu.1.2:\n   This audit produces raw patterns. bd-2yu.1.2 then organizes them\n   into a concrete screen registry and navigation model for the ops TUI.\n   Keep this task focused on extraction, not design decisions.\n","created_at":"2026-02-13T21:09:21Z"}]}
{"id":"bd-2yu.1.2","title":"Define final screen registry, navigation model, and cross-screen workflows","description":"Task:\nDefine the final screen registry, nav model, and operational IA for frankensearch control plane.\n\nRequired screens (minimum):\n- Fleet overview (all detected instances)\n- Project detail dashboard\n- Live search stream\n- Index + embedding progress\n- Resource trends (CPU/memory/IO)\n- Historical analytics windows\n- Alerts/timeline + explainability panels\n\nMust define:\n- Global keybindings, mouse hit regions, and command palette verbs.\n- Inline vs alt-screen behavior and reconnect semantics.\n- Cross-screen drilldowns and context-preserving navigation.\n\nAcceptance:\nNo ambiguity remains about what each downstream screen bead must implement.","acceptance_criteria":"1) Screen registry is finalized with required screens and purpose statements.\\n2) Global navigation/focus/keybinding/mouse model is specified.\\n3) Cross-screen drilldown and context-preservation behavior is defined for implementation.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:42.768962179Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:21.282382179Z","due_at":"2026-02-27T14:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","ia","navigation","tui"],"dependencies":[{"issue_id":"bd-2yu.1.2","depends_on_id":"bd-2yu.1","type":"parent-child","created_at":"2026-02-13T20:55:42.768962179Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.1.2","depends_on_id":"bd-2yu.1.1","type":"blocks","created_at":"2026-02-13T20:56:21.725231615Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":192,"issue_id":"bd-2yu.1.2","author":"Dicklesworthstone","text":"Planning note: this IA/spec bead is the contract for all downstream screen work; if a screen behavior is not defined here, add it before implementation to avoid UX drift.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.2","title":"Workstream: Telemetry, event, and evidence contracts","description":"Goal:\nDefine stable machine contracts for telemetry, streaming events, and evidence logs so all host integrations can emit consistent data.\n\nScope:\n- Event taxonomy for search/index/embed/resource lifecycle.\n- Control-plane snapshot + stream subscription interfaces.\n- Evidence/diagnostic JSONL schema for replay and debugging.\n\nOutput quality bar:\nContracts must be self-documenting, versioned, and testable without external narrative context.","acceptance_criteria":"1) Canonical telemetry and control-plane contracts are versioned and documented.\\n2) Evidence JSONL contract includes replay + redaction strategy.\\n3) Contract tests can validate producer and consumer conformance.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:42.867410089Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:47.171466371Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["contracts","frankensearch","phase-contracts","telemetry"],"dependencies":[{"issue_id":"bd-2yu.2","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:42.867410089Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2","depends_on_id":"bd-2yu.1.2","type":"blocks","created_at":"2026-02-13T20:56:20.664842357Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":91,"issue_id":"bd-2yu.2","author":"Dicklesworthstone","text":"Future-self rationale: unstable data contracts are the fastest way to derail this project. The UI, simulator, integration adapters, and tests all depend on clear event semantics and replay-safe evidence logs. Treat schema quality and versioning as a first-class product feature, not admin overhead.","created_at":"2026-02-13T20:56:47Z"}]}
{"id":"bd-2yu.2.1","title":"Define canonical telemetry event taxonomy and versioned payload schema","description":"Task:\nDefine canonical telemetry event taxonomy and payload schema.\n\nMust include fields for:\n- Instance identity and host project attribution.\n- Search requests/results/latency/memory use.\n- Embedding job progress and queue states.\n- Index inventory snapshots (words/tokens/lines/bytes/docs).\n- Resource footprint samples (cpu, rss, io read/write).\n\nRequirements:\n- Schema versioning strategy.\n- Correlation IDs for linking related events.\n- Explicit nullability and compatibility policy.","acceptance_criteria":"1) Event schema includes required identity/search/embed/index/resource fields.\\n2) Correlation ID and schema version rules are explicit.\\n3) JSON schema validation fixtures exist for representative event families.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:42.964014177Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:21.282619433Z","due_at":"2026-02-27T14:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["contracts","frankensearch","schema","telemetry"],"dependencies":[{"issue_id":"bd-2yu.2.1","depends_on_id":"bd-2yu.1.1","type":"blocks","created_at":"2026-02-13T20:56:21.821457925Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.1","depends_on_id":"bd-2yu.1.2","type":"blocks","created_at":"2026-02-13T20:56:21.917197155Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.1","depends_on_id":"bd-2yu.2","type":"parent-child","created_at":"2026-02-13T20:55:42.964014177Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":170,"issue_id":"bd-2yu.2.1","author":"Dicklesworthstone","text":"REVISION: Telemetry Event Taxonomy Implementation Details\n\n1. Concrete Rust Type Structure:\n   Use a tagged enum with per-variant structs:\n\n   pub enum TelemetryEvent {\n       Search(SearchEvent),\n       Embedding(EmbeddingEvent),\n       Index(IndexEvent),\n       Resource(ResourceEvent),\n       Lifecycle(LifecycleEvent),\n   }\n\n   pub struct SearchEvent {\n       pub correlation_id: Ulid,\n       pub instance_id: Ulid,\n       pub project: String,\n       pub query: String, // canonicalized, truncated to 500 chars\n       pub query_class: QueryClass,\n       pub phase: SearchPhase,\n       pub latency_us: u64,\n       pub result_count: u32,\n       pub memory_bytes: u64,\n       pub timestamp: chrono::DateTime<chrono::Utc>,\n   }\n\n   Similar structs for Embedding (job_id, doc_count, embedder, duration_ms,\n   queue_depth), Index (operation: Build|Rebuild|Repair, doc_count, dimension,\n   duration_ms), Resource (cpu_pct, rss_bytes, io_read_bytes, io_write_bytes),\n   Lifecycle (state: Started|Stopped|Healthy|Degraded|Stale).\n\n2. Serialization:\n   Use serde with JSON. Envelope format:\n   { \"v\": 1, \"ts\": \"ISO8601\", \"event\": { \"type\": \"search\", ...fields } }\n   This allows schema evolution: consumers check \"v\" and handle unknown fields.\n   JSONL for evidence logs (one event per line, bd-2yu.2.3).\n\n3. Correlation IDs:\n   Use ULID (monotonic, sortable, 128-bit, crate: ulid).\n   Topology: request_id (from TwoTierSearcher) -> search_id -> embed_id.\n   Each search creates a correlation tree. The request_id is the root.\n   Stored in tracing span context, extracted by collectors.\n\n4. Schema Versioning:\n   Integer monotonic version in the envelope \"v\" field.\n   Breaking changes increment version. New optional fields do NOT increment.\n   Consumers must handle: known version = parse fully, unknown version = store raw.\n   Migration: none needed (append-only JSONL, old events stay at their version).\n\n5. Nullability Policy:\n   Fields that may be unavailable (e.g., memory_bytes on platforms without /proc):\n   Use Option<T> with #[serde(skip_serializing_if = \"Option::is_none\")].\n   Required fields (correlation_id, timestamp, instance_id): never None.\n   Document which fields are optional in the struct doc comments.\n","created_at":"2026-02-13T21:09:17Z"}]}
{"id":"bd-2yu.2.2","title":"Define control-plane snapshot and streaming interface","description":"Task:\nDefine control-plane interface for snapshot queries and live stream subscription.\n\nMust support:\n- Enumerating detected instances and their health/attribution confidence.\n- Pulling latest per-instance metrics + SLO/anomaly status.\n- Subscribing to live search/embedding/index/resource/anomaly event streams.\n- Backpressure, reconnect, and lag-reporting semantics for bursty hosts.\n\nAcceptance intent:\nA client can render dashboards and live feeds using only this interface + frankensqlite historical reads.","acceptance_criteria":"1) Snapshot and stream interfaces cover all required dashboard data paths including anomaly/SLO state.\n2) Backpressure/reconnect/lag semantics are explicit and testable.\n3) Client implementation can consume interface without undocumented assumptions.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.059316659Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:21.282843974Z","due_at":"2026-02-27T14:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","contracts","frankensearch","streaming"],"dependencies":[{"issue_id":"bd-2yu.2.2","depends_on_id":"bd-2yu.2","type":"parent-child","created_at":"2026-02-13T20:55:43.059316659Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.2","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:22.009775696Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":193,"issue_id":"bd-2yu.2.2","author":"Dicklesworthstone","text":"Contract intent: snapshot + stream APIs must be sufficient to render all screens without hidden side channels; include lag/reconnect semantics as first-class fields.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.2.3","title":"Define evidence JSONL schema, replay metadata, and redaction policy","description":"Task:\nDefine evidence/logging schema and redaction rules.\n\nMust include:\n- Deterministic replay fields (seed/tick/frame sequence where relevant).\n- Sensitive field classification and redaction policy.\n- Human-readable reason codes for alerts/degradation/decisions.\n- JSONL shape validation strategy used by e2e scripts.\n\nOutcome:\nDetailed logs are safe to persist and rich enough for postmortem + explainability screens.","acceptance_criteria":"1) JSONL schema includes replay metadata and reason-code semantics.\\n2) Redaction policy classifies sensitive fields and required transformations.\\n3) Logging contract tests fail on missing/unsafe fields.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.154194104Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:21.283073974Z","due_at":"2026-02-27T14:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["contracts","evidence","frankensearch","privacy"],"dependencies":[{"issue_id":"bd-2yu.2.3","depends_on_id":"bd-2yu.1.2","type":"blocks","created_at":"2026-02-13T20:56:22.201874546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.3","depends_on_id":"bd-2yu.2","type":"parent-child","created_at":"2026-02-13T20:55:43.154194104Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.3","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:22.105405511Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":194,"issue_id":"bd-2yu.2.3","author":"Dicklesworthstone","text":"Privacy intent: evidence logs should preserve diagnostic utility while enforcing redaction boundaries by default, not as an optional post-process.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.2.4","title":"Define SLO/error-budget semantics and anomaly signal contract","description":"Task:\nDefine the canonical SLO/error-budget and anomaly signal contract consumed by dashboards, alerts, and test harnesses.\n\nMust define:\n- SLO metrics for search latency, query failure rate, stale-index lag, and embedding backlog age.\n- Error-budget burn formulas over 1m/15m/1h/6h/24h/3d/1w windows.\n- Alert signal taxonomy (info/warn/critical) with stable reason codes and confidence semantics.\n- Payload shape for anomaly events including baseline, deviation, and suppression metadata.\n\nWhy this matters:\nWithout explicit SLO/anomaly semantics, screens become pretty but operationally ambiguous; this task makes fleet health interpretation objective and comparable across host projects.","acceptance_criteria":"1) SLO + error-budget formulas are versioned and machine-testable across all required windows.\n2) Anomaly payload schema includes reason codes, baseline context, suppression metadata, and confidence fields.\n3) Dashboard and alert consumers can use the contract with no undocumented assumptions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:05:40.176376166Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.159372091Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alerts","contracts","frankensearch","phase-contracts","schema","slo","telemetry"],"dependencies":[{"issue_id":"bd-2yu.2.4","depends_on_id":"bd-2yu.2","type":"parent-child","created_at":"2026-02-13T21:05:40.176376166Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.4","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T21:05:40.176376166Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.4","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T21:05:40.176376166Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.2.4","depends_on_id":"bd-2yu.2.3","type":"blocks","created_at":"2026-02-13T21:05:40.176376166Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":175,"issue_id":"bd-2yu.2.4","author":"Dicklesworthstone","text":"Revision rationale: elevated SLO/error-budget semantics to a first-class contract so health interpretation is consistent across hosts and windows. This prevents each screen/adapter from inventing its own thresholds.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.3","title":"Workstream: Instance discovery, project attribution, and lifecycle tracking","description":"Goal:\nAutomatically discover running frankensearch-enabled instances on a machine and reliably determine host project identity.\n\nScope:\n- Runtime instance detection mechanisms.\n- Project attribution/resolution logic.\n- Lifecycle/health state machine for start/stop/restart/stale.","acceptance_criteria":"1) Discovery engine can enumerate active instances reliably.\\n2) Project attribution/lifecycle states are surfaced with confidence metadata.\\n3) Discovery/lifecycle outputs are consumable by dashboards and alerts.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.256253436Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:47.546449219Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["discovery","frankensearch","lifecycle","phase-discovery"],"dependencies":[{"issue_id":"bd-2yu.3","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:43.256253436Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T20:56:20.761323155Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":92,"issue_id":"bd-2yu.3","author":"Dicklesworthstone","text":"Future-self rationale: detection and attribution quality directly determine trust in the dashboard. False positives/negatives here will make all downstream charts misleading. Invest in confidence scoring and explicit reasons so operators understand why an instance was mapped to a project.","created_at":"2026-02-13T20:56:47Z"}]}
{"id":"bd-2yu.3.1","title":"Implement multi-source frankensearch instance discovery engine","description":"Task:\nImplement host-level discovery engine for frankensearch instances.\n\nCandidate sources:\n- Process inspection signatures.\n- Domain sockets / control endpoints.\n- Heartbeat files or registration records.\n\nMust handle:\n- Multiple versions simultaneously.\n- Duplicate signal reconciliation.\n- Low overhead polling/refresh cadence.","acceptance_criteria":"1) Multi-source discovery works across process/socket/control endpoint signals.\\n2) Duplicate sightings reconcile to stable instance identities.\\n3) Refresh cadence and overhead stay within defined operational budget.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.351867781Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:24.053765528Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["discovery","frankensearch","instances"],"dependencies":[{"issue_id":"bd-2yu.3.1","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T20:56:22.297990180Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.1","depends_on_id":"bd-2yu.3","type":"parent-child","created_at":"2026-02-13T20:55:43.351867781Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":195,"issue_id":"bd-2yu.3.1","author":"Dicklesworthstone","text":"Discovery strategy should combine multiple weak signals into one stable instance identity, with explicit duplicate reconciliation and stale expiry behavior.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.3.2","title":"Implement project attribution resolver and lifecycle state tracker","description":"Task:\nImplement project attribution resolver + lifecycle tracker used by fleet dashboards and alerts.\n\nRequirements:\n- Resolve host project identity (coding_agent_session_search, xf, mcp_agent_mail_rust, frankenterm, unknown/custom).\n- Track state transitions with heartbeat-gap detection and restart classification.\n- Emit lifecycle events for timeline/alerts with confidence + reason fields.\n- Surface attribution uncertainty and collision states explicitly (never silently discard).\n\nOutcome:\nOperators can trust instance identity and quickly diagnose attribution ambiguity.","acceptance_criteria":"1) Resolver maps instances to known projects or explicit unknown bucket with confidence metadata.\n2) Lifecycle tracker emits start/stop/restart/stale transitions with deterministic semantics.\n3) Attribution reasons and uncertainty states are queryable for troubleshooting.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:43.451112793Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.159712478Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["attribution","discovery","frankensearch","health"],"dependencies":[{"issue_id":"bd-2yu.3.2","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:22.486785404Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.2","depends_on_id":"bd-2yu.3","type":"parent-child","created_at":"2026-02-13T20:55:43.451112793Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.2","depends_on_id":"bd-2yu.3.1","type":"blocks","created_at":"2026-02-13T20:56:22.393663956Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":196,"issue_id":"bd-2yu.3.2","author":"Dicklesworthstone","text":"Lifecycle transitions should be deterministic and explainable so alerting and dashboards do not oscillate under noisy heartbeats.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.3.3","title":"Implement host identity handshake and fallback attribution heuristics","description":"Task:\nImplement a robust host-identity handshake and fallback attribution heuristics so instance-to-project mapping remains accurate under partial telemetry or mixed-version environments.\n\nMust include:\n- Preferred identity handshake fields exposed by host integrations (project key, binary identity, runtime role, instance UUID).\n- Confidence-scored fallback heuristics when handshake data is incomplete/unavailable.\n- Collision-resolution strategy for duplicated or conflicting identity evidence.\n- Explainable attribution traces for operator troubleshooting.\n\nWhy this matters:\nCross-project dashboards lose trust if attribution is brittle. This task hardens attribution quality and keeps unknown/misclassified instances visible and diagnosable.","acceptance_criteria":"1) Identity handshake spec is implemented for host adapters with explicit required/optional fields.\n2) Fallback attribution heuristics produce confidence-scored results with deterministic tie-break behavior.\n3) Operators can inspect attribution evidence for any instance, including unknown bucket assignments.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:05:48.239341350Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.159947348Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["attribution","discovery","frankensearch","instances","lifecycle","phase-discovery"],"dependencies":[{"issue_id":"bd-2yu.3.3","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T21:05:48.239341350Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.3","depends_on_id":"bd-2yu.2.4","type":"blocks","created_at":"2026-02-13T21:05:48.239341350Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.3","depends_on_id":"bd-2yu.3","type":"parent-child","created_at":"2026-02-13T21:05:48.239341350Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.3","depends_on_id":"bd-2yu.3.1","type":"blocks","created_at":"2026-02-13T21:05:48.239341350Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.3.3","depends_on_id":"bd-2yu.3.2","type":"blocks","created_at":"2026-02-13T21:18:32.087172465Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":176,"issue_id":"bd-2yu.3.3","author":"Dicklesworthstone","text":"Revision rationale: added explicit handshake + fallback heuristics to harden project attribution in mixed-version or partial-signal environments. Unknown bucket handling is intentional, not an error path.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.4","title":"Workstream: Frankensqlite storage, aggregation, and query plane","description":"Goal:\nUse frankensqlite as the durable historical store for operational metrics and events.\n\nScope:\n- Schema, ingestion path, rolling aggregates, retention policy, and query APIs for dashboards.\n- Performance tuned for frequent writes + fast range queries.","acceptance_criteria":"1) frankensqlite data plane supports durable ingest and fast dashboard reads.\\n2) Rolling windows and retention strategy are implemented and validated.\\n3) Storage model aligns with existing FrankenSQLite integration plan.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.549146137Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:47.823621219Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","frankensqlite","phase-data-plane","storage"],"dependencies":[{"issue_id":"bd-2yu.4","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:43.549146137Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:20.853520793Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":93,"issue_id":"bd-2yu.4","author":"Dicklesworthstone","text":"Future-self rationale: frankensqlite is not just a sink; it is the analytical backbone for trend windows and incident review. Prioritize predictable ingest latency, idempotency, and query indexes early so we do not paint ourselves into a performance corner.","created_at":"2026-02-13T20:56:47Z"}]}
{"id":"bd-2yu.4.1","title":"Design frankensqlite schema for fleet telemetry and timeline data","description":"Task:\nDesign normalized + query-optimized frankensqlite schema.\n\nTables required:\n- instances/projects\n- search events and search summaries\n- embedding jobs/progress snapshots\n- index inventory snapshots\n- resource samples\n- alerts/timeline/evidence links\n\nMust include:\n- migration/versioning strategy\n- indexes tuned for per-project time-window queries\n- integrity constraints preventing duplicate ingestion","acceptance_criteria":"1) Schema includes all required telemetry/timeline entities and constraints.\\n2) Migrations are versioned and reversible for development/testing.\\n3) Query indexes satisfy expected dashboard access patterns.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.648533777Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:24.054103040Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","frankensqlite","schema"],"dependencies":[{"issue_id":"bd-2yu.4.1","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:22.586927537Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.1","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T20:56:22.683535392Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.1","depends_on_id":"bd-2yu.2.3","type":"blocks","created_at":"2026-02-13T20:56:22.776896038Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.1","depends_on_id":"bd-2yu.4","type":"parent-child","created_at":"2026-02-13T20:55:43.648533777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.1","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:57:28.823088610Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.1","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:57:28.919106601Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":173,"issue_id":"bd-2yu.4.1","author":"Dicklesworthstone","text":"REVISION: FrankenSQLite Telemetry Schema Details\n\n1. Core Tables (DDL sketch):\n\n   CREATE TABLE instances (\n     instance_id TEXT PRIMARY KEY,  -- ULID\n     project TEXT NOT NULL,\n     host TEXT,\n     pid INTEGER,\n     state TEXT NOT NULL DEFAULT 'unknown',  -- started|healthy|degraded|stale|stopped\n     first_seen TEXT NOT NULL,  -- ISO8601\n     last_heartbeat TEXT NOT NULL,\n     version TEXT\n   );\n\n   CREATE TABLE search_events (\n     event_id TEXT PRIMARY KEY,  -- ULID (monotonic, serves as dedup key)\n     instance_id TEXT NOT NULL REFERENCES instances(instance_id),\n     correlation_id TEXT NOT NULL,\n     query TEXT,\n     query_class TEXT,\n     phase TEXT,  -- initial|refined|failed\n     latency_us INTEGER NOT NULL,\n     result_count INTEGER,\n     memory_bytes INTEGER,\n     ts TEXT NOT NULL  -- ISO8601\n   );\n   CREATE INDEX idx_search_ts ON search_events(instance_id, ts);\n\n   CREATE TABLE embedding_events (\n     event_id TEXT PRIMARY KEY,\n     instance_id TEXT NOT NULL REFERENCES instances(instance_id),\n     job_id TEXT,\n     doc_count INTEGER,\n     embedder TEXT,\n     duration_ms INTEGER,\n     queue_depth INTEGER,\n     ts TEXT NOT NULL\n   );\n\n   CREATE TABLE resource_samples (\n     sample_id INTEGER PRIMARY KEY AUTOINCREMENT,\n     instance_id TEXT NOT NULL REFERENCES instances(instance_id),\n     cpu_pct REAL,\n     rss_bytes INTEGER,\n     io_read_bytes INTEGER,\n     io_write_bytes INTEGER,\n     ts TEXT NOT NULL\n   );\n   CREATE INDEX idx_resource_ts ON resource_samples(instance_id, ts);\n\n2. Aggregate Tables (materialized by bd-2yu.4.3):\n   CREATE TABLE search_summaries (\n     instance_id TEXT NOT NULL,\n     window TEXT NOT NULL,  -- '1m'|'15m'|'1h'|'6h'|'24h'\n     window_start TEXT NOT NULL,\n     count INTEGER,\n     p50_latency_us INTEGER,\n     p95_latency_us INTEGER,\n     p99_latency_us INTEGER,\n     avg_result_count REAL,\n     PRIMARY KEY (instance_id, window, window_start)\n   );\n\n3. Relationship to Document Metadata (bd-3w1.2):\n   Separate databases. Document metadata lives in {data_dir}/frankensearch.db.\n   Telemetry data lives in {data_dir}/frankensearch-ops.db.\n   Separate DBs avoid WAL contention between search writes and telemetry writes.\n   Both use the same FrankenSQLite Connection API.\n\n4. Deduplication:\n   Event dedup key: event_id (ULID). INSERT OR IGNORE on duplicate.\n   This handles retries from the ingestion writer (bd-2yu.4.2).\n   Resource samples use AUTOINCREMENT (no dedup needed, append-only).\n\n5. Retention:\n   Default retention: raw events 7 days, summaries 90 days.\n   Retention enforced by a periodic DELETE WHERE ts < threshold.\n   Run retention cleanup once per hour (not on every write).\n   Configurable via TwoTierConfig or env var FRANKENSEARCH_OPS_RETENTION_DAYS.\n","created_at":"2026-02-13T21:09:21Z"}]}
{"id":"bd-2yu.4.2","title":"Implement ingestion writer with batching/idempotency/backpressure","description":"Task:\nImplement ingestion writer with batching, idempotency, and backpressure controls.\n\nRequirements:\n- Handle bursty event rates without dropping critical events.\n- Deterministic ordering guarantees where required.\n- Structured error paths and recovery logging.\n- Observable write latency and queue depth metrics.","acceptance_criteria":"1) Ingestion path is idempotent and handles burst load with backpressure.\\n2) Write failures have deterministic retry/failure accounting behavior.\\n3) Ingestion metrics (latency/queue depth/errors) are emitted for observability.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.747475602Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:24.054340605Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","frankensqlite","ingestion"],"dependencies":[{"issue_id":"bd-2yu.4.2","depends_on_id":"bd-2yu.3.1","type":"blocks","created_at":"2026-02-13T20:56:22.967814187Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.2","depends_on_id":"bd-2yu.4","type":"parent-child","created_at":"2026-02-13T20:55:43.747475602Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.2","depends_on_id":"bd-2yu.4.1","type":"blocks","created_at":"2026-02-13T20:56:22.873810968Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.2","depends_on_id":"bd-3w1.11","type":"blocks","created_at":"2026-02-13T20:57:29.159230902Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.2","depends_on_id":"bd-3w1.3","type":"blocks","created_at":"2026-02-13T20:57:29.015679070Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":197,"issue_id":"bd-2yu.4.2","author":"Dicklesworthstone","text":"Ingestion path must degrade gracefully under burst load: keep critical events, account for drops explicitly, and expose queue/write pressure metrics.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.4.3","title":"Implement rolling-window aggregates, retention, and dashboard query API","description":"Task:\nImplement aggregate windows and retention/downsampling for dashboard and analytics queries.\n\nMust support windows:\n- 1m, 15m, 1h, 6h, 24h, 3d, 1w\n\nMust compute:\n- search count and latency/memory distributions\n- embedding throughput/progress rates\n- resource footprint trends\n\nAlso implement:\n- retention policy + compaction/downsampling strategy\n- fast query layer for dashboard reads\n- alignment hooks consumed by SLO/anomaly rollup materialization","acceptance_criteria":"1) Windowed aggregates (1m/15m/1h/6h/24h/3d/1w) are available via query API.\n2) Retention/downsampling preserves trend fidelity for target horizons.\n3) Dashboard query latency meets agreed budget under realistic load and supports SLO rollup dependencies.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.849163909Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:24.054571417Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["aggregates","frankensearch","frankensqlite","retention"],"dependencies":[{"issue_id":"bd-2yu.4.3","depends_on_id":"bd-2yu.4","type":"parent-child","created_at":"2026-02-13T20:55:43.849163909Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.3","depends_on_id":"bd-2yu.4.2","type":"blocks","created_at":"2026-02-13T20:56:23.063347621Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.3","depends_on_id":"bd-3w1.13","type":"blocks","created_at":"2026-02-13T20:57:29.257792465Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":198,"issue_id":"bd-2yu.4.3","author":"Dicklesworthstone","text":"Aggregate windows are core product semantics (not just reporting); keep formulas stable and test-validated because many screens depend on them.","created_at":"2026-02-13T21:10:34Z"},{"id":216,"issue_id":"bd-2yu.4.3","author":"Dicklesworthstone","text":"REVISION (review pass 5 - dependency corrections):\n\n1. REMOVED bd-3w1.17 DEPENDENCY: bd-3w1.17 is an integration TEST bead. Production code (this bead) should never be blocked by tests. The dependency direction was inverted. Tests depend on implementations, not vice versa. If the intent was to ensure the storage pipeline works first, bd-3w1.13 (the pipeline implementation) is already listed as a blocker and is sufficient.\n\n2. bd-3w1.13 DEPENDENCY RATIONALE: This dependency is correct. bd-3w1.13 wires FrankenSQLite into the pipeline, providing the Connection lifecycle and schema initialization APIs that the aggregation engine builds upon. The aggregation engine uses the same FrankenSQLite Connection (from bd-3w1.1) to create telemetry-specific tables alongside the document/embedding tables.\n","created_at":"2026-02-13T21:23:14Z"}]}
{"id":"bd-2yu.4.4","title":"Implement anomaly materialization and SLO rollup tables in frankensqlite","description":"Task:\nImplement storage-level anomaly materialization and SLO rollup tables so dashboards can query health state cheaply and consistently.\n\nMust include:\n- Precomputed error-budget burn and SLO health status per project + fleet scope.\n- Materialized anomaly rows with baseline/deviation/reason metadata and severity.\n- Incremental rollup jobs aligned to 1m/15m/1h/6h/24h/3d/1w windows.\n- Query APIs optimized for alert/timeline and SLO dashboards.\n\nWhy this matters:\nComputing anomaly semantics on every render is expensive and brittle; persistent rollups keep the UI fast and deterministic under load.","acceptance_criteria":"1) SLO rollup tables persist error-budget and health state for all required windows and scopes.\n2) Anomaly materialization records include baseline, deviation, severity, reason code, and correlation metadata.\n3) Query performance for alert/SLO dashboards meets target latency under representative load.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:05:59.815454927Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.160186285Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["aggregates","alerts","frankensearch","frankensqlite","phase-data-plane","slo"],"dependencies":[{"issue_id":"bd-2yu.4.4","depends_on_id":"bd-2yu.2.4","type":"blocks","created_at":"2026-02-13T21:05:59.815454927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.4","depends_on_id":"bd-2yu.4","type":"parent-child","created_at":"2026-02-13T21:05:59.815454927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.4","depends_on_id":"bd-2yu.4.2","type":"blocks","created_at":"2026-02-13T21:05:59.815454927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.4.4","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T21:05:59.815454927Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":177,"issue_id":"bd-2yu.4.4","author":"Dicklesworthstone","text":"Revision rationale: anomaly/SLO materialization is persisted in frankensqlite to keep dashboards fast and deterministic under load. Runtime recomputation is intentionally avoided for critical health views.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.5","title":"Workstream: Frankensearch instrumentation and host-project adapters","description":"Goal:\nInstrument frankensearch and host applications so the control plane receives complete, comparable, and evolution-safe telemetry.\n\nScope:\n- core collectors + live stream emitters\n- pipeline instrumentation hooks\n- host integration adapters for coding_agent_session_search/xf/mcp_agent_mail_rust/frankenterm\n- reusable adapter SDK + conformance harness for future hosts\n\nQuality bar:\nNo host should require bespoke interpretation logic; attribution, redaction, and lifecycle semantics must remain consistent across integrations.","acceptance_criteria":"1) Core instrumentation emits complete, consistent telemetry for control-plane use.\n2) Host adapters exist for coding_agent_session_search/xf/mcp_agent_mail_rust/frankenterm and pass conformance checks.\n3) Adapter SDK + conformance harness enable future integrations without schema drift.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:43.948235657Z","created_by":"ubuntu","updated_at":"2026-02-13T21:07:31.916931427Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","instrumentation","integrations","phase-adapters"],"dependencies":[{"issue_id":"bd-2yu.5","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:43.948235657Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5","depends_on_id":"bd-2yu.4.2","type":"blocks","created_at":"2026-02-13T20:56:20.950354922Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":94,"issue_id":"bd-2yu.5","author":"Dicklesworthstone","text":"Future-self rationale: instrumentation must be consistent across host projects or comparisons become meaningless. Prefer one canonical emitter path + thin host adapters over bespoke per-project telemetry logic. Correlation IDs and stage markers are mandatory for explainability and timeline usefulness.","created_at":"2026-02-13T20:56:47Z"}]}
{"id":"bd-2yu.5.1","title":"Implement core frankensearch metrics collectors (index/embed/search/resource)","description":"Task:\nImplement core runtime collectors in frankensearch.\n\nMetrics required:\n- index inventory (words/tokens/lines/bytes/docs)\n- embedding queue/progress\n- search latency + memory usage\n- CPU/memory/IO footprint sampling\n\nMust emit according to canonical telemetry schema.","acceptance_criteria":"1) Collectors emit index/embed/search/resource metrics with canonical fields.\\n2) Sampling cadence and overhead are documented and bounded.\\n3) Collector outputs pass contract validation fixtures.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:44.049016074Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:24.054799323Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core-metrics","frankensearch","instrumentation"],"dependencies":[{"issue_id":"bd-2yu.5.1","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:23.159022139Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.1","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.049016074Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.1","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T20:56:23.255153011Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.1","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T20:56:23.349017590Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.1","depends_on_id":"bd-3un.39","type":"blocks","created_at":"2026-02-13T21:22:25.654330968Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":171,"issue_id":"bd-2yu.5.1","author":"Dicklesworthstone","text":"REVISION: Core Metrics Collector Architecture\n\n1. Collector Architecture:\n   A MetricsCollector struct that holds Arc references to the instrumented types\n   and produces snapshots on demand:\n\n   pub struct MetricsCollector {\n       searcher: Option<Arc<TwoTierSearcher>>,\n       index: Option<Arc<TwoTierIndex>>,\n       resource_sampler: ResourceSampler,\n       collection_interval: Duration, // default: 5 seconds\n   }\n\n   pub trait MetricsSource: Send + Sync {\n       fn snapshot(&self) -> MetricsSnapshot;\n   }\n\n   MetricsSnapshot is a struct with all current metric values.\n   The collector does NOT poll — it exposes snapshot() for the data plane\n   (bd-2yu.4.2) to call on its own schedule.\n\n2. Extraction Mechanism:\n   Use the structured tracing spans already defined in bd-3un.39.\n   The TwoTierSearcher emits tracing spans with latency, result_count, phase.\n   A custom tracing::Subscriber captures these into an in-memory ring buffer.\n   The collector reads from the ring buffer to build snapshots.\n\n   This avoids modifying the core frankensearch types — the instrumentation\n   is already there via tracing, we just need a subscriber that aggregates.\n\n   Ring buffer: bounded at 10K events. Oldest events dropped on overflow.\n   Lock-free: use crossbeam::queue::ArrayQueue or similar.\n\n3. Resource Sampling:\n   Use the sysinfo crate (cross-platform, maintained, no unsafe):\n   - CPU: sysinfo::System::refresh_cpu() -> cpu_usage()\n   - Memory: sysinfo::System::refresh_memory() -> used_memory()\n   - IO: /proc/self/io on Linux (read_bytes, write_bytes)\n   - Fallback: return None for unavailable metrics (don't panic)\n\n   Sampling interval: configurable, default 5 seconds.\n   Each sample produces a ResourceEvent for the telemetry stream.\n\n4. Threading Model:\n   The collector itself is stateless and thread-safe (Send + Sync).\n   No dedicated thread — the ingestion writer (bd-2yu.4.2) calls\n   collector.snapshot() from its own worker thread on each batch cycle.\n   The tracing subscriber runs in-process, no additional threads needed.\n\n5. Registration:\n   The collector is created during TwoTierSearcher::auto() or manually:\n   let collector = MetricsCollector::new()\n       .with_searcher(searcher.clone())\n       .with_index(index.clone())\n       .with_resource_sampling(Duration::from_secs(5));\n\n   The collector is optional — if not created, no overhead.\n   Feature-gated behind #[cfg(feature = \"telemetry\")] in the facade.\n","created_at":"2026-02-13T21:09:18Z"},{"id":215,"issue_id":"bd-2yu.5.1","author":"Dicklesworthstone","text":"REVISION (review pass 5 - missing dependency fix):\n\nCRITICAL: Added bd-3un.39 as blocking dependency. The core metrics collector strategy is to capture structured tracing spans from frankensearch into an in-memory ring buffer via a custom tracing::Subscriber. Without bd-3un.39 (which defines the span hierarchy and mandatory fields), there are no spans to capture. This dependency was implicit in the enrichment comments but missing from the dependency graph.\n\nDependency chain: bd-3un.39 (defines spans) -> bd-2yu.5.1 (captures spans into metrics) -> bd-2yu.5.2/5.3 (uses metrics).\n","created_at":"2026-02-13T21:23:14Z"}]}
{"id":"bd-2yu.5.2","title":"Implement live search stream emitter and correlation plumbing","description":"Task:\nImplement live search stream emitter with correlation IDs and bounded buffering.\n\nRequirements:\n- Stream active searches as they occur.\n- Include per-search timing + memory fields.\n- Support lossy/non-lossy modes with explicit accounting.\n- Emit health counters used in timeline and live-feed UI.","acceptance_criteria":"1) Live stream includes per-search latency/memory and correlation IDs.\\n2) Buffering/backpressure behavior is explicit and measurable.\\n3) Stream semantics support timeline and live-feed screens without gaps.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:44.148189292Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:24.055024044Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","instrumentation","search-stream"],"dependencies":[{"issue_id":"bd-2yu.5.2","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:23.443295113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.2","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T20:56:23.541614482Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.2","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.148189292Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.2","depends_on_id":"bd-2yu.5.1","type":"blocks","created_at":"2026-02-13T20:56:23.636863323Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":199,"issue_id":"bd-2yu.5.2","author":"Dicklesworthstone","text":"Live stream emitter should expose backpressure/drop accounting clearly so operators can distinguish data-plane loss from real workload changes.","created_at":"2026-02-13T21:10:34Z"}]}
{"id":"bd-2yu.5.3","title":"Integrate instrumentation hooks across core search/index/embed pipeline","description":"Task:\nWire instrumentation hooks into the core frankensearch execution pipeline.\n\nMust instrument:\n- query intake/classification\n- lexical/vector retrieval/fusion\n- embedding generation/index updates\n- error/degradation paths\n\nAcceptance:\nEvery major pipeline stage appears in timeline/evidence views with actionable context.","acceptance_criteria":"1) Core pipeline stages emit telemetry/evidence hooks consistently.\\n2) Errors/degradation paths are instrumented, not silent.\\n3) Timeline/explainability UIs can reconstruct stage-level story for a query.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:44.246784277Z","created_by":"ubuntu","updated_at":"2026-02-13T21:54:47.076307961Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","instrumentation","pipeline-hooks"],"dependencies":[{"issue_id":"bd-2yu.5.3","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.246784277Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.3","depends_on_id":"bd-2yu.5.1","type":"blocks","created_at":"2026-02-13T20:56:23.730830996Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.3","depends_on_id":"bd-2yu.5.2","type":"blocks","created_at":"2026-02-13T20:56:23.828378891Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.3","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T20:56:23.924332802Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.3","depends_on_id":"bd-3un.39","type":"blocks","created_at":"2026-02-13T21:53:46.173831Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":200,"issue_id":"bd-2yu.5.3","author":"Dicklesworthstone","text":"Instrumentation hooks should cover success + degraded + failure paths uniformly; silent failures are unacceptable for explainability workflows.","created_at":"2026-02-13T21:10:35Z"},{"id":258,"issue_id":"bd-2yu.5.3","author":"Dicklesworthstone","text":"REVISION (review pass 7 - tracing dependency):\n\nADDED bd-3un.39 (structured tracing) as a blocking dependency. bd-2yu.5.3 integrates instrumentation hooks across every pipeline stage. Those hooks must consume the tracing spans defined in bd-3un.39.\n\nSCOPE CLARIFICATION: bd-2yu.5.3's scope is NOT defining new tracing spans (that is bd-3un.39's job). Its scope is:\n1. Verifying bd-3un.39 spans cover all stages needed by the TUI\n2. Adding TUI-specific correlation metadata that bd-3un.39 does not include (e.g., instance_id, project_key)\n3. Wiring the MetricsCollector subscriber from bd-2yu.5.1 into the pipeline startup path\n4. Ensuring error/degradation paths emit tracing events (not just return errors)\n","created_at":"2026-02-13T21:54:47Z"}]}
{"id":"bd-2yu.5.4","title":"Add frankensearch telemetry adapter for coding_agent_session_search","description":"Task:\nIntegrate telemetry adapter for /dp/coding_agent_session_search using the shared adapter SDK/conformance harness.\n\nRequirements:\n- Implement identity handshake fields and lifecycle hooks.\n- Forward search/index/embedding/resource telemetry with canonical schema.\n- Validate attribution and lifecycle transitions via deterministic fixtures.\n- Record host-specific caveats in integration notes.","acceptance_criteria":"1) coding_agent_session_search emits canonical telemetry and registers identity correctly.\n2) Adapter passes shared conformance harness and deterministic attribution/lifecycle fixtures.\n3) Integration docs capture host-specific caveats and config requirements.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:55:44.350332867Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:31.509037759Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cass","frankensearch","integration"],"dependencies":[{"issue_id":"bd-2yu.5.4","depends_on_id":"bd-2yu.3.3","type":"blocks","created_at":"2026-02-13T21:07:04.742193022Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.4","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.350332867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.4","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:24.022781654Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.4","depends_on_id":"bd-2yu.5.8","type":"blocks","created_at":"2026-02-13T21:07:04.640938748Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.4","depends_on_id":"bd-3un.36","type":"blocks","created_at":"2026-02-13T20:56:24.122798311Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":201,"issue_id":"bd-2yu.5.4","author":"Dicklesworthstone","text":"Adapter implementation must be schema-first and conformance-driven to prevent host-specific field drift.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.5.5","title":"Add frankensearch telemetry adapter for xf","description":"Task:\nIntegrate telemetry adapter for /dp/xf using the shared adapter SDK/conformance harness.\n\nRequirements:\n- Implement identity handshake fields and lifecycle hooks.\n- Forward search/index/embedding/resource telemetry with canonical schema.\n- Validate attribution and lifecycle transitions via deterministic fixtures.\n- Record host-specific caveats in integration notes.","acceptance_criteria":"1) xf emits canonical telemetry and registers identity correctly.\n2) Adapter passes shared conformance harness and deterministic attribution/lifecycle fixtures.\n3) Integration docs capture host-specific caveats and config requirements.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:55:44.449104232Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:31.509298397Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","integration","xf"],"dependencies":[{"issue_id":"bd-2yu.5.5","depends_on_id":"bd-2yu.3.3","type":"blocks","created_at":"2026-02-13T21:07:04.942669807Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.5","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.449104232Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.5","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:24.216598049Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.5","depends_on_id":"bd-2yu.5.8","type":"blocks","created_at":"2026-02-13T21:07:04.842473143Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.5","depends_on_id":"bd-3un.35","type":"blocks","created_at":"2026-02-13T20:56:24.314957343Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":202,"issue_id":"bd-2yu.5.5","author":"Dicklesworthstone","text":"Adapter implementation must be schema-first and conformance-driven to prevent host-specific field drift.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.5.6","title":"Add frankensearch telemetry adapter for mcp_agent_mail_rust","description":"Task:\nIntegrate telemetry adapter for /dp/mcp_agent_mail_rust using the shared adapter SDK/conformance harness.\n\nRequirements:\n- Implement identity handshake fields and lifecycle hooks.\n- Forward search/index/embedding/resource telemetry with canonical schema.\n- Validate attribution and lifecycle transitions via deterministic fixtures.\n- Record host-specific caveats in integration notes.","acceptance_criteria":"1) mcp_agent_mail_rust emits canonical telemetry and registers identity correctly.\n2) Adapter passes shared conformance harness and deterministic attribution/lifecycle fixtures.\n3) Integration docs capture host-specific caveats and config requirements.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:55:44.549756489Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:31.509535921Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","integration","mcp-agent-mail"],"dependencies":[{"issue_id":"bd-2yu.5.6","depends_on_id":"bd-2yu.3.3","type":"blocks","created_at":"2026-02-13T21:07:05.150785082Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.6","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.549756489Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.6","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:24.411122390Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.6","depends_on_id":"bd-2yu.5.8","type":"blocks","created_at":"2026-02-13T21:07:05.042769870Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.6","depends_on_id":"bd-3un.37","type":"blocks","created_at":"2026-02-13T20:56:24.508041277Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":203,"issue_id":"bd-2yu.5.6","author":"Dicklesworthstone","text":"Adapter implementation must be schema-first and conformance-driven to prevent host-specific field drift.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.5.7","title":"Add frankensearch telemetry adapter for frankenterm","description":"Task:\nIntegrate telemetry adapter for /dp/frankenterm using the shared adapter SDK/conformance harness.\n\nRequirements:\n- Implement identity handshake fields and lifecycle hooks.\n- Forward search/index/embedding/resource telemetry with canonical schema.\n- Validate attribution and lifecycle transitions via deterministic fixtures.\n- Record host-specific caveats in integration notes.","acceptance_criteria":"1) frankenterm emits canonical telemetry and registers identity correctly.\n2) Adapter passes shared conformance harness and deterministic attribution/lifecycle fixtures.\n3) Integration docs capture host-specific caveats and config requirements.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:55:44.652869814Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:31.509858115Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","frankenterm","integration"],"dependencies":[{"issue_id":"bd-2yu.5.7","depends_on_id":"bd-2yu.3.3","type":"blocks","created_at":"2026-02-13T21:07:05.359026052Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.7","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T20:55:44.652869814Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.7","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:24.606956552Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.7","depends_on_id":"bd-2yu.5.8","type":"blocks","created_at":"2026-02-13T21:07:05.256588232Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":204,"issue_id":"bd-2yu.5.7","author":"Dicklesworthstone","text":"Adapter implementation must be schema-first and conformance-driven to prevent host-specific field drift.","created_at":"2026-02-13T21:10:35Z"},{"id":218,"issue_id":"bd-2yu.5.7","author":"Dicklesworthstone","text":"REVISION (review pass 5 - forward-looking adapter):\n\nNOTE: Unlike the other three host adapters (bd-2yu.5.4 cass, bd-2yu.5.5 xf, bd-2yu.5.6 agent-mail), this frankenterm adapter has NO corresponding migration bead in bd-3un. The other adapters depend on bd-3un.35/36/37 (migrate host project to use frankensearch crate).\n\nThis is because frankenterm integration is FORWARD-LOOKING -- frankenterm does not yet use frankensearch. This adapter should be treated as lower priority than the other three, and will require either:\na) A new migration bead (bd-3un.xx: Integrate frankensearch into frankenterm) created when ready\nb) Or this adapter being deferred until frankenterm integration scope is defined\n\nFor now, this bead can be built against the adapter SDK (bd-2yu.5.8) using mock/stub data, but full integration depends on frankenterm actually consuming frankensearch.\n","created_at":"2026-02-13T21:23:35Z"}]}
{"id":"bd-2yu.5.8","title":"Build host-adapter SDK and conformance harness for future integrations","description":"Task:\nCreate a reusable adapter SDK and conformance harness so additional host projects can integrate with frankensearch telemetry without bespoke glue every time.\n\nMust include:\n- Adapter trait/interface for identity handshake, telemetry emission, and lifecycle hooks.\n- Shared validation helpers for schema/version compliance and redaction rules.\n- Contract test harness that can be executed by each host integration repo.\n- Golden fixtures + failure diagnostics for compatibility drift.\n\nWhy this matters:\nCurrent scope names four host projects, but the system is intended to expand. This task prevents integration entropy and keeps data quality consistent.","acceptance_criteria":"1) Adapter SDK exposes stable interfaces for identity, telemetry emission, and lifecycle hooks.\n2) Conformance harness validates schema/version/redaction compliance with actionable diagnostics.\n3) At least one sample host integration passes conformance tests using golden fixtures.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.160631018Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","frankensearch","instrumentation","integrations","phase-adapters","testing"],"dependencies":[{"issue_id":"bd-2yu.5.8","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.8","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.8","depends_on_id":"bd-2yu.2.3","type":"blocks","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.8","depends_on_id":"bd-2yu.2.4","type":"blocks","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.8","depends_on_id":"bd-2yu.5","type":"parent-child","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.5.8","depends_on_id":"bd-2yu.5.1","type":"blocks","created_at":"2026-02-13T21:06:11.907529124Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":178,"issue_id":"bd-2yu.5.8","author":"Dicklesworthstone","text":"Revision rationale: adapter SDK + conformance harness turns one-off integrations into a scalable onboarding path while preserving telemetry/redaction/attribution consistency.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.6","title":"Workstream: Frankensearch ops TUI shell, overlays, and interaction framework","description":"Goal:\nBuild the FrankenTUI application shell and cross-cutting UX systems used by all operational dashboards.\n\nScope:\n- app shell, nav, command palette, overlays, a11y/perf controls, deterministic mode\n- inline/alt-screen resilience\n- operator view presets, density modes, and progressive-disclosure controls\n\nQuality bar:\nInteraction must stay fast and predictable under high update rates while remaining accessible and discoverable.","acceptance_criteria":"1) App shell and global interaction framework are stable and reusable across all screens.\n2) Command palette/overlays/a11y controls + view presets are integrated and coherent.\n3) Deterministic + inline-mode behavior is robust, testable, and resilient under stream load.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:44.752399119Z","created_by":"ubuntu","updated_at":"2026-02-13T21:23:14.575534612Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","phase-shell","shell","tui"],"dependencies":[{"issue_id":"bd-2yu.6","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:44.752399119Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6","depends_on_id":"bd-2yu.1.2","type":"blocks","created_at":"2026-02-13T20:56:21.050566224Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":95,"issue_id":"bd-2yu.6","author":"Dicklesworthstone","text":"Future-self rationale: the app shell and cross-cutting interactions are shared infrastructure. If we rush into screen code before this is stable, we will duplicate navigation/focus/overlay logic and create UX inconsistency. Build shell discipline first, then screens.","created_at":"2026-02-13T20:56:48Z"},{"id":217,"issue_id":"bd-2yu.6","author":"Dicklesworthstone","text":"REVISION (review pass 5 - stale dependency removal):\n\nREMOVED bd-2yu.4.3 DEPENDENCY from workstream bead. The bd-2yu.6.1 enrichment comment explicitly states: \"DEPENDENCY NOTE: Removed bd-2yu.6.1 -> bd-2yu.4.3. Rationale: The app shell should be buildable and testable with a MockDataSource before the full FrankenSQLite query API exists.\"\n\nThe leaf bead (6.1) was correctly decoupled, but the parent workstream bead (bd-2yu.6) still retained the dependency, creating unnecessary serialization. The individual screen beads (bd-2yu.7.x) have their own dependencies on bd-2yu.4.3 where actually needed.\n","created_at":"2026-02-13T21:23:14Z"}]}
{"id":"bd-2yu.6.1","title":"Implement app shell with registry-driven navigation and status chrome","description":"Task:\nImplement app shell with screen registry, category tabs, status bar, and context-preserving navigation.\n\nMust reuse proven FrankenTUI patterns:\n- registry-driven screen metadata\n- global keybindings and mouse hit regions\n- robust focus/input routing","acceptance_criteria":"1) Registry-driven shell supports tab/category navigation and status chrome.\\n2) Focus/input routing is deterministic for keyboard and mouse flows.\\n3) Shared shell primitives are reused by all screen implementations.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:44.851332568Z","created_by":"ubuntu","updated_at":"2026-02-13T21:54:42.825243181Z","due_at":"2026-03-13T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","navigation","shell","tui"],"dependencies":[{"issue_id":"bd-2yu.6.1","depends_on_id":"bd-2yu.1.2","type":"blocks","created_at":"2026-02-13T20:56:24.705466168Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.1","depends_on_id":"bd-2yu.6","type":"parent-child","created_at":"2026-02-13T20:55:44.851332568Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":172,"issue_id":"bd-2yu.6.1","author":"Dicklesworthstone","text":"REVISION: App Shell Implementation Details\n\n1. TUI Framework:\n   Use ratatui + crossterm (consistent with FrankenTUI heritage).\n   ratatui for layout/rendering, crossterm for terminal I/O.\n   Both are in the existing dependency ecosystem.\n\n2. Core Types:\n   pub trait Screen: Send {\n       fn id(&self) -> &'static str;\n       fn title(&self) -> &str;\n       fn category(&self) -> ScreenCategory;\n       fn render(&mut self, frame: &mut Frame, area: Rect, state: &AppState);\n       fn handle_event(&mut self, event: &Event, state: &mut AppState) -> EventResult;\n       fn on_focus(&mut self, state: &AppState);\n       fn on_blur(&mut self);\n   }\n\n   pub struct ScreenRegistry {\n       screens: IndexMap<&'static str, Box<dyn Screen>>,\n       active: &'static str,\n       history: Vec<&'static str>, // for back-navigation\n   }\n\n   pub enum ScreenCategory { Fleet, Search, Index, Resource, Analytics, Settings }\n\n   pub struct AppShell {\n       registry: ScreenRegistry,\n       status_bar: StatusBar,\n       command_palette: Option<CommandPalette>,\n       data_source: Box<dyn DataSource>,\n   }\n\n3. DataSource Trait (Decoupling from bd-2yu.4.3):\n   pub trait DataSource: Send {\n       fn fleet_snapshot(&self) -> FleetSnapshot;\n       fn search_stream(&self) -> Box<dyn Iterator<Item = SearchEvent>>;\n       fn query_metrics(&self, window: TimeWindow) -> MetricsResult;\n   }\n\n   This trait allows the shell to be developed and tested with a\n   MockDataSource before the real bd-2yu.4.3 query API exists.\n   The concrete FrankenSQLiteDataSource implements DataSource\n   and is wired in when feature = \"storage\" is enabled.\n\n4. Status Bar Content:\n   Left: active screen title + category icon\n   Center: connection status (N instances discovered, M healthy)\n   Right: current time + resource summary (CPU% / RAM MB)\n   Update: refresh on each frame tick (default 4 fps)\n\n5. Navigation:\n   Tab key: cycle through categories\n   Number keys (1-9): jump to screen by position in current category\n   Ctrl+P: open command palette\n   ?: open help overlay\n   Escape: close overlay or go back\n   q: quit (with confirmation if unsaved state)\n","created_at":"2026-02-13T21:09:19Z"},{"id":183,"issue_id":"bd-2yu.6.1","author":"Dicklesworthstone","text":"DEPENDENCY NOTE: Removed bd-2yu.6.1 -> bd-2yu.4.3\n\nRationale: The app shell should be buildable and testable with a MockDataSource\nbefore the full FrankenSQLite query API (bd-2yu.4.3) exists. The DataSource trait\ndefined in the shell enrichment comment enables this decoupling.\n\nThe concrete dependency on bd-2yu.4.3 is moved to the individual dashboard\nscreen beads (bd-2yu.7.1 through bd-2yu.7.4) which actually need real query data.\n\nThis shortens the serial chain from 7-deep to 5-deep, allowing shell and\ndashboard UI development to proceed in parallel with the storage pipeline.\n\nThe bd-2yu.7.* beads already depend on bd-2yu.4.3 through bd-2yu.7's blocked_by,\nso the query API dependency is not lost — it's just correctly placed at the\nscreen level rather than the shell level.\n","created_at":"2026-02-13T21:09:41Z"},{"id":254,"issue_id":"bd-2yu.6.1","author":"Dicklesworthstone","text":"REVISION (review pass 7 - async/sync bridge for TUI):\n\nCRITICAL: The Screen trait and DataSource trait are synchronous but the TwoTierSearcher is async (takes &Cx). The project mandates asupersync exclusively for all async operations.\n\nBRIDGE STRATEGY (required for implementation):\n\nThe ratatui render loop is synchronous (60fps tick loop). The search library is async. The bridge pattern is:\n\n1. Background async region updates shared state:\n   asupersync::scope!(cx, |scope| {\n       scope.spawn(|cx| async move {\n           loop {\n               let snapshot = fleet_monitor.poll(cx).await?;\n               app_state.write(cx).await.update(snapshot);\n               cx.sleep(Duration::from_millis(100)).await;\n           }\n       });\n   });\n\n2. Synchronous render loop reads shared state:\n   let state = app_state.read_blocking();  // Non-blocking read via try_read()\n   screen.render(frame, area, &state);\n\n3. Event handling returns async commands:\n   pub enum EventResult {\n       Consumed,\n       Ignored,\n       AsyncCommand(Box<dyn FnOnce(&Cx) -> BoxFuture<'_, ()> + Send>),\n   }\n\n4. DataSource becomes async with &Cx:\n   pub trait DataSource: Send + Sync {\n       async fn fleet_snapshot(&self, cx: &Cx) -> FleetSnapshot;\n       async fn search_stream(&self, cx: &Cx) -> impl Stream<Item = SearchEvent>;\n       async fn query_metrics(&self, cx: &Cx, window: TimeWindow) -> MetricsResult;\n   }\n\nThe AppState is shared via Arc<asupersync::sync::RwLock<AppState>>. Background tasks write, render loop reads. This is the standard pattern for async-backed TUIs.\n","created_at":"2026-02-13T21:54:42Z"}]}
{"id":"bd-2yu.6.2","title":"Implement command palette, help/alerts overlays, and accessibility controls","description":"Task:\nImplement command palette, contextual help/alerts overlays, and accessibility controls as first-class interaction primitives.\n\nMust include:\n- Command palette with ranked actions, fuzzy filtering, and per-screen command namespaces.\n- Non-disruptive overlay stack for help, alerts, and critical-state notices.\n- Keyboard-only parity for all high-frequency actions.\n- Accessibility controls for contrast, motion, and focus visibility with persisted preferences.\n\nOutcome:\nAdvanced workflows remain discoverable without overwhelming routine triage.","acceptance_criteria":"1) Command palette surfaces relevant actions with predictable ranking and low interaction latency.\n2) Help/alert overlays are discoverable, context-aware, and non-disruptive.\n3) Accessibility controls alter UI behavior consistently and emit telemetry for auditability.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:44.950827779Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.160851831Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["a11y","command-palette","frankensearch","tui"],"dependencies":[{"issue_id":"bd-2yu.6.2","depends_on_id":"bd-2yu.1.1","type":"blocks","created_at":"2026-02-13T20:56:24.997096106Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.2","depends_on_id":"bd-2yu.6","type":"parent-child","created_at":"2026-02-13T20:55:44.950827779Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.2","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:24.902184145Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":205,"issue_id":"bd-2yu.6.2","author":"Dicklesworthstone","text":"Interaction policy: command palette and overlays should speed up operations, not obscure data; optimize discoverability and keyboard-first usage.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.6.3","title":"Implement deterministic replay hooks and inline/alt-screen resilience","description":"Task:\nImplement deterministic/replay mode + inline/alt-screen resilience.\n\nRequirements:\n- deterministic seeds/tick controls for tests\n- JSONL evidence hooks for replay/debug\n- stable behavior in inline mode with reconnect/restart events","acceptance_criteria":"1) Deterministic mode supports reproducible replay for tests/incidents.\\n2) Inline and alt-screen behavior handles reconnect/restart cleanly.\\n3) Evidence logging hooks are available for debugging and explainability.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.053689652Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.161088364Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["determinism","frankensearch","inline-mode","tui"],"dependencies":[{"issue_id":"bd-2yu.6.3","depends_on_id":"bd-2yu.2.3","type":"blocks","created_at":"2026-02-13T20:56:25.188467634Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.3","depends_on_id":"bd-2yu.6","type":"parent-child","created_at":"2026-02-13T20:55:45.053689652Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.3","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:25.093740389Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":206,"issue_id":"bd-2yu.6.3","author":"Dicklesworthstone","text":"Replay hooks must remain deterministic across terminal modes; this is foundational for debugging e2e failures and incident retrospectives.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.6.4","title":"Implement operator view presets, density modes, and progressive-disclosure controls","description":"Task:\nImplement advanced usability controls that keep the dashboard high-signal for both novice and expert operators.\n\nMust include:\n- Saved view presets (fleet triage, project deep-dive, incident mode, low-noise mode).\n- Density modes and optional detail panes for progressive disclosure.\n- Keyboard-first toggles for high-traffic workflows.\n- Accessibility-aware defaults that respect reduced motion/contrast needs.\n\nWhy this matters:\nA single rigid layout cannot satisfy all operational contexts. This task improves adoption and reduces cognitive load during real incidents.","acceptance_criteria":"1) Operators can switch between predefined views/density modes without losing context.\n2) Progressive-disclosure controls expose advanced details on demand while preserving low-noise defaults.\n3) Accessibility settings (reduced motion/high contrast/keyboard-only) are honored consistently across screens.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:06:23.179243317Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:27.161310450Z","due_at":"2026-03-27T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["a11y","frankensearch","phase-shell","shell","tui","ux"],"dependencies":[{"issue_id":"bd-2yu.6.4","depends_on_id":"bd-2yu.1.2","type":"blocks","created_at":"2026-02-13T21:06:23.179243317Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.4","depends_on_id":"bd-2yu.6","type":"parent-child","created_at":"2026-02-13T21:06:23.179243317Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.4","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T21:06:23.179243317Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.6.4","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:06:23.179243317Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":179,"issue_id":"bd-2yu.6.4","author":"Dicklesworthstone","text":"Revision rationale: added presets/density/progressive disclosure to reduce cognitive load and support both novice and expert operators during incidents.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.7","title":"Workstream: Operational dashboard screens for frankensearch fleet","description":"Goal:\nImplement operational dashboards that make frankensearch fleet behavior obvious, actionable, and trustworthy.\n\nScope:\n- real-time and historical views\n- project-specific and fleet-wide insights\n- alerts/timeline/explainability context\n- dedicated SLO/error-budget and capacity-forecast views\n\nQuality bar:\nEvery screen should optimize time-to-diagnosis and preserve context across drilldowns.","acceptance_criteria":"1) Required operational screens are implemented and integrated with live + historical data.\n2) Cross-screen drilldowns support practical triage workflows without context loss.\n3) Visual hierarchy emphasizes high-signal health state, including SLO/alert/capacity indicators.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:45.155102734Z","created_by":"ubuntu","updated_at":"2026-02-13T21:11:38.440819164Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["dashboards","frankensearch","phase-screens","tui"],"dependencies":[{"issue_id":"bd-2yu.7","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:45.155102734Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:21.341503434Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:21.439430800Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:21.242418562Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":96,"issue_id":"bd-2yu.7","author":"Dicklesworthstone","text":"Future-self rationale: these screens should answer operations questions in seconds. Optimize for triage flow: overview -> drilldown -> live evidence -> historical context -> actionable next step. Avoid decorative widgets that do not improve diagnostic speed.","created_at":"2026-02-13T20:56:48Z"},{"id":210,"issue_id":"bd-2yu.7","author":"Dicklesworthstone","text":"Graph optimization: removed unnecessary serial deps among 7.x screen tasks so Fleet, Stream, Resource, and Explainability screens can progress in parallel after shell/data prerequisites are met.","created_at":"2026-02-13T21:11:38Z"}]}
{"id":"bd-2yu.7.1","title":"Implement Fleet Overview and Project Detail dashboards","description":"Task:\nImplement Fleet Overview + Project Detail dashboards as primary operational landing pages.\n\nMust include:\n- auto-detected instance inventory with project attribution confidence\n- per-project index/embedding/search/resource summary cards\n- health badges tied to SLO/anomaly state\n- drilldowns to live stream, timeline, and deep analytics screens\n\nOutcome:\nUsers get immediate situational awareness across all running frankensearch integrations.","acceptance_criteria":"1) Fleet and project dashboards accurately represent detected instances and attribution confidence.\n2) Summary cards surface index/embed/search/resource health with clear state semantics.\n3) Drilldowns preserve context and support rapid transition to deeper diagnostic screens.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.255341697Z","created_by":"ubuntu","updated_at":"2026-02-13T21:54:43.952700479Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["dashboards","fleet","frankensearch","tui"],"dependencies":[{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.3.2","type":"blocks","created_at":"2026-02-13T20:56:25.566809820Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.3.3","type":"blocks","created_at":"2026-02-13T21:07:19.465768399Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:25.377554564Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.5.1","type":"blocks","created_at":"2026-02-13T20:56:25.473151657Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:25.282938378Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:07:19.368539741Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.1","depends_on_id":"bd-2yu.7","type":"parent-child","created_at":"2026-02-13T20:55:45.255341697Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":207,"issue_id":"bd-2yu.7.1","author":"Dicklesworthstone","text":"Fleet overview should prioritize immediate triage signal (health/attribution/freshness) before deep metrics to reduce first-look cognitive load.","created_at":"2026-02-13T21:10:35Z"},{"id":255,"issue_id":"bd-2yu.7.1","author":"Dicklesworthstone","text":"REVISION (review pass 7 - degraded state rendering):\n\nCRITICAL: No screen bead specifies how to render degraded search pipeline states. The TwoTierSearcher can produce:\n- SearchPhase::RefinementFailed { error } — quality model failed\n- SearchError::Cancelled — asupersync cancellation\n- SearchError::EmbeddingError — embedder unavailable\n- SearchError::IndexError — index corrupted\n- SearchError::DurabilityDisabled — durability feature off\n\nREQUIRED ACCEPTANCE CRITERIA for Fleet Overview (bd-2yu.7.1):\n1. Pipeline Health panel showing: which embedders are loaded, whether quality refinement is active or skipped, index health status, durability status\n2. Visual states: GREEN (all healthy), YELLOW (degraded — fast-only mode), RED (error — index corrupted, embedder unavailable)\n3. Tooltip/detail showing the specific SearchError variant and recovery guidance from bd-3un.2\n\nREQUIRED ACCEPTANCE CRITERIA for Live Search Stream (bd-2yu.7.2):\n1. Each search result row shows its SearchPhase: Initial (fast icon), Refined (quality icon), RefinementFailed (warning icon)\n2. Failed refinement rows show the SkipReason from TwoTierMetrics\n3. Cancelled searches appear as dimmed rows with cancellation reason\n\nEMPTY STATE UX (first-run scenario):\n1. Zero instances discovered → onboarding panel: \"No frankensearch instances found. Start a frankensearch-enabled application to begin monitoring.\"\n2. Discovery in progress → spinner with \"Scanning for instances...\"\n3. Discovery failed → error panel with diagnostic info\n","created_at":"2026-02-13T21:54:43Z"}]}
{"id":"bd-2yu.7.2","title":"Implement Live Search Stream and Action Timeline screens","description":"Task:\nImplement Live Search Stream + Action Timeline screens for high-velocity operational triage.\n\nMust include:\n- streaming list of active/recent searches with correlation IDs\n- per-search latency/memory fields and degradation markers\n- event/timeline filters (project, severity, rule/reason code, host)\n- rolling counters for 1m/15m/1h/6h/24h/3d/1w windows\n- explicit stream-health indicators (lag, drops, reconnect state)\n\nOutcome:\nOperators can observe current workload and immediately pivot from aggregate anomalies to concrete search events.","acceptance_criteria":"1) Live search stream updates continuously with bounded UI latency under burst load.\n2) Timeline filters/severity markers preserve operator context during drilldown.\n3) Rolling counters (1m..1w) and stream-health indicators remain accurate and auditable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.359506149Z","created_by":"ubuntu","updated_at":"2026-02-13T21:55:45.113575645Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["dashboards","frankensearch","streaming","timeline","tui"],"dependencies":[{"issue_id":"bd-2yu.7.2","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:25.759926676Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.2","depends_on_id":"bd-2yu.5.2","type":"blocks","created_at":"2026-02-13T20:56:25.853094451Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.2","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:25.660167010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.2","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:07:19.563274776Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.2","depends_on_id":"bd-2yu.7","type":"parent-child","created_at":"2026-02-13T20:55:45.359506149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.2","depends_on_id":"bd-2yu.7.1","type":"blocks","created_at":"2026-02-13T21:55:45.113529498Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":103,"issue_id":"bd-2yu.7.2","author":"Dicklesworthstone","text":"UX intent for live stream/timeline: optimize operator triage speed. Keep filtering fast, preserve context while drilling down, and include high-signal summaries (count/latency/memory) alongside raw stream rows.","created_at":"2026-02-13T20:57:29Z"},{"id":186,"issue_id":"bd-2yu.7.2","author":"Dicklesworthstone","text":"Operational UX intent: pair raw stream rows with stream-health diagnostics (lag/drop/reconnect) so operators can distinguish product issues from observability pipeline issues.","created_at":"2026-02-13T21:09:43Z"}]}
{"id":"bd-2yu.7.3","title":"Implement Index/Embedding/Resource monitoring screens","description":"Task:\nImplement Index + Embedding + Resource monitoring screens with fast project/fleet comparison workflows.\n\nMust show:\n- index inventory (words/tokens/lines/bytes/docs) with freshness indicators\n- embedding queue/progress/throughput and lag projections\n- CPU/memory/IO trends with baseline deltas and anomaly badges\n- per-project vs fleet percentile comparisons\n\nOutcome:\nOperators can quickly identify whether slow search comes from stale index state, embedding backlog, or host resource pressure.","acceptance_criteria":"1) Index inventory, embedding progress, and resource trend views are complete and coherent.\n2) Baseline deltas and percentile comparisons support anomaly spotting across projects.\n3) Screen remains responsive and legible under high-cardinality data.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.463294828Z","created_by":"ubuntu","updated_at":"2026-02-13T21:55:45.984326194Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["dashboards","frankensearch","index","resources","tui"],"dependencies":[{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:26.047603122Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.4.4","type":"blocks","created_at":"2026-02-13T21:07:19.759964921Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.5.1","type":"blocks","created_at":"2026-02-13T20:56:26.142822027Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:25.948667489Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:07:19.661201030Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.7","type":"parent-child","created_at":"2026-02-13T20:55:45.463294828Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.3","depends_on_id":"bd-2yu.7.1","type":"blocks","created_at":"2026-02-13T21:55:45.984257866Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":104,"issue_id":"bd-2yu.7.3","author":"Dicklesworthstone","text":"UX intent for resource/index/embedding screens: emphasize trend readability over visual noise. Every chart/table should support a concrete decision (capacity risk, stale index, embedding bottleneck, or anomalous host behavior).","created_at":"2026-02-13T20:57:29Z"},{"id":187,"issue_id":"bd-2yu.7.3","author":"Dicklesworthstone","text":"Comparison intent: include baseline and fleet-percentile deltas so resource/index anomalies are obvious without manual cross-project arithmetic.","created_at":"2026-02-13T21:09:43Z"}]}
{"id":"bd-2yu.7.4","title":"Implement Historical Analytics and Explainability cockpit screens","description":"Task:\nImplement Historical Analytics + Explainability cockpit screens for deep postmortem and root-cause workflows.\n\nMust include:\n- time-windowed trend analysis with latency/memory percentiles\n- correlation between anomalies/alerts and underlying event streams\n- evidence log visualization with reason codes, attribution confidence, and replay handles\n- export-friendly incident review snapshots\n\nOutcome:\nOperators can move from symptom to root cause with minimal context switching.","acceptance_criteria":"1) Historical analytics expose latency/memory distributions and trend shifts by window.\n2) Explainability cockpit links alerts/decisions to replayable evidence records.\n3) Export and replay pathways are reliable enough for incident review and debugging.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.563923581Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:07.211489573Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["analytics","dashboards","explainability","frankensearch","tui"],"dependencies":[{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.2.3","type":"blocks","created_at":"2026-02-13T20:56:26.424376021Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:26.330374224Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.4.4","type":"blocks","created_at":"2026-02-13T21:07:19.949975101Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T20:56:26.235964544Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:07:19.855038244Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.6.3","type":"blocks","created_at":"2026-02-13T21:07:20.055818326Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.7","type":"parent-child","created_at":"2026-02-13T20:55:45.563923581Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.7.2","type":"blocks","created_at":"2026-02-13T20:56:26.520435339Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.4","depends_on_id":"bd-2yu.7.3","type":"blocks","created_at":"2026-02-13T21:56:07.211428058Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":208,"issue_id":"bd-2yu.7.4","author":"Dicklesworthstone","text":"Explainability screen should join anomalies, evidence logs, and replay handles into one operator flow so root-cause steps are auditable.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.7.5","title":"Implement Alerts/SLO health and capacity-forecast screens","description":"Task:\nImplement dedicated Alerts/SLO and capacity-forecast views for proactive operations across all frankensearch host projects.\n\nMust include:\n- Active alerts panel with severity, confidence, suppression state, and reason-code drilldown.\n- SLO/error-budget status by project and fleet aggregate.\n- Capacity forecast indicators for embedding backlog growth and search saturation risk.\n- Fast drilldown path into live stream, timeline, and project detail screens.\n\nWhy this matters:\nOperators need an explicit health cockpit for decision-making under pressure; this screen converts raw telemetry into action priorities.","acceptance_criteria":"1) Alerts/SLO screen presents actionable severity and error-budget burn state per project and fleet.\n2) Capacity forecast indicators provide early warning for backlog/saturation risk with explainable inputs.\n3) Drilldowns into timeline/live/project views preserve context and complete within target interaction latency.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:06:32.722361012Z","created_by":"ubuntu","updated_at":"2026-02-13T22:01:43.865579105Z","due_at":"2026-04-10T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alerts","analytics","dashboards","fleet","frankensearch","phase-screens","slo","tui"],"dependencies":[{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.2.4","type":"blocks","created_at":"2026-02-13T22:01:43.865505227Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.3.2","type":"blocks","created_at":"2026-02-13T21:06:32.722361012Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.4.4","type":"blocks","created_at":"2026-02-13T21:06:32.722361012Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.6.1","type":"blocks","created_at":"2026-02-13T21:06:32.722361012Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:06:32.722361012Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.7","type":"parent-child","created_at":"2026-02-13T21:06:32.722361012Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.7.1","type":"blocks","created_at":"2026-02-13T22:01:20.900373744Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.7.5","depends_on_id":"bd-2yu.7.2","type":"blocks","created_at":"2026-02-13T21:07:20.174850363Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":180,"issue_id":"bd-2yu.7.5","author":"Dicklesworthstone","text":"Revision rationale: dedicated Alerts/SLO/Capacity screen makes proactive triage possible; timeline + stream views remain necessary but are insufficient as the only health surface.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.8","title":"Workstream: Comprehensive testing, e2e logging, and performance validation","description":"Goal:\nGuarantee reliability with comprehensive unit/integration/e2e/perf coverage and detailed diagnostics.\n\nScope:\n- deterministic simulator\n- unit tests for discovery/data-plane/instrumentation\n- snapshot + PTY e2e suites\n- load/perf/fault and long-duration soak tests\n\nQuality bar:\nFailures must be reproducible via captured seeds/evidence and diagnosable from CI artifacts alone.","acceptance_criteria":"1) Deterministic simulator and test harnesses provide repeatable validation.\n2) Unit/snapshot/e2e/perf/fault/soak suites provide broad coverage with rich logs/artifacts.\n3) CI gates fail fast on correctness, regression, reliability, and budget breaches.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:45.668685983Z","created_by":"ubuntu","updated_at":"2026-02-13T21:07:47.244244777Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","frankensearch","phase-quality","quality","testing"],"dependencies":[{"issue_id":"bd-2yu.8","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:45.668685983Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8","depends_on_id":"bd-2yu.7","type":"blocks","created_at":"2026-02-13T20:56:21.537221570Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":97,"issue_id":"bd-2yu.8","author":"Dicklesworthstone","text":"Future-self rationale: this quality workstream is intentionally heavy. The product is an observability surface; false metrics or flaky UI behavior are high-severity failures. Deterministic simulator + snapshot/e2e + perf/fault suites are non-negotiable.","created_at":"2026-02-13T20:56:48Z"}]}
{"id":"bd-2yu.8.1","title":"Build deterministic multi-instance telemetry simulator","description":"Task:\nBuild deterministic multi-instance simulator for frankensearch fleet telemetry.\n\nPurpose:\n- Reproduce realistic search/embed/resource workloads.\n- Drive snapshot/e2e/perf tests with deterministic seeds.\n- Validate attribution/discovery logic across multiple host projects.","acceptance_criteria":"1) Simulator can emulate multiple host projects and workload profiles.\\n2) Deterministic seeds produce reproducible event streams.\\n3) Simulator is integrated into e2e and performance test entrypoints.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.771807283Z","created_by":"ubuntu","updated_at":"2026-02-13T21:55:10.911437334Z","due_at":"2026-04-24T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","simulator","testing"],"dependencies":[{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-2yu.2.1","type":"blocks","created_at":"2026-02-13T20:56:26.615687646Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-2yu.2.2","type":"blocks","created_at":"2026-02-13T20:56:26.708006793Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-2yu.2.3","type":"blocks","created_at":"2026-02-13T20:56:26.800824893Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-2yu.3.1","type":"blocks","created_at":"2026-02-13T20:56:26.896204609Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-2yu.3.2","type":"blocks","created_at":"2026-02-13T21:22:29.110599007Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-2yu.8","type":"parent-child","created_at":"2026-02-13T20:55:45.771807283Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.1","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:53:46.939526473Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":99,"issue_id":"bd-2yu.8.1","author":"Dicklesworthstone","text":"Simulator intent: model realistic mixed workloads (steady traffic, burst search storms, embedding backlog waves, and host restarts). Keep deterministic mode first-class (fixed seeds + scripted timelines) so failures are exactly replayable in CI and locally.","created_at":"2026-02-13T20:57:29Z"},{"id":260,"issue_id":"bd-2yu.8.1","author":"Dicklesworthstone","text":"REVISION (review pass 7 - simulator type dependency):\n\nADDED bd-3un.2 (core error types) as a blocking dependency. The telemetry simulator must use actual frankensearch types (SearchError variants, SearchPhase enum, SkipReason) to generate realistic events. Without this dependency, the simulator would need to re-invent these types, and any divergence would cause integration failures with the real library.\n\nThe simulator should import and use:\n- SearchError variants for realistic error event generation\n- SearchPhase (Initial, Refined, RefinementFailed) for search stream simulation\n- SkipReason for quality model skip scenarios\n- QueryClass for query classification distribution modeling\n","created_at":"2026-02-13T21:55:10Z"}]}
{"id":"bd-2yu.8.2","title":"Write unit tests for discovery/data-plane/instrumentation engines","description":"Task:\nImplement comprehensive unit tests for discovery, attribution, storage, aggregation, instrumentation, and SLO/anomaly derivation engines.\n\nRequirements:\n- cover normal + edge + failure cases\n- validate schema invariants, rollup correctness, and alert reason-code semantics\n- assert correlation-ID and redaction/privacy rules\n- include deterministic fixtures for cross-project attribution edge cases","acceptance_criteria":"1) Unit suites cover discovery, attribution, storage, aggregation, instrumentation, and anomaly derivation.\n2) Edge/failure cases are explicitly asserted (clock skew, duplicates, stale detection, schema errors, identity conflicts).\n3) Test diagnostics are sufficiently detailed for rapid root-cause analysis.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.878455876Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:37.563267320Z","due_at":"2026-04-24T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensearch","testing","unit"],"dependencies":[{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.2.4","type":"blocks","created_at":"2026-02-13T21:07:20.275518059Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.3.2","type":"blocks","created_at":"2026-02-13T20:56:26.993155997Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.3.3","type":"blocks","created_at":"2026-02-13T21:07:20.372890696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:27.088019657Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.4.4","type":"blocks","created_at":"2026-02-13T21:07:20.470662100Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:27.184615540Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.2","depends_on_id":"bd-2yu.8","type":"parent-child","created_at":"2026-02-13T20:55:45.878455876Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":100,"issue_id":"bd-2yu.8.2","author":"Dicklesworthstone","text":"Unit-test quality bar: each engine test set must include happy path, edge limits, malformed telemetry payloads, missing fields, clock skew, duplicate event ingestion, and stale-instance transitions. Prefer precise invariants over broad smoke tests.","created_at":"2026-02-13T20:57:29Z"}]}
{"id":"bd-2yu.8.3","title":"Create snapshot + PTY e2e suite with rich diagnostic logging","description":"Task:\nImplement dashboard snapshot tests and PTY e2e scripts with detailed diagnostic artifact capture.\n\nMust include:\n- multi-size snapshots (desktop/compact) including accessibility and density modes\n- PTY e2e flows for discovery, triage, drilldown, and recovery scenarios\n- artifact bundle per run: structured logs, evidence JSONL, replay seed, failing snapshot diff, terminal transcript\n- deterministic replay entrypoint for failed runs\n\nOutcome:\nAny e2e failure can be reproduced and diagnosed without ad-hoc local debugging.","acceptance_criteria":"1) Snapshot coverage includes key screens across size/a11y/density variants.\n2) PTY e2e scripts exercise realistic operator flows and emit rich diagnostics.\n3) Every failure artifact bundle contains sufficient data for deterministic replay.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:45.978092291Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:37.563504023Z","due_at":"2026-04-24T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","frankensearch","logging","snapshots","testing"],"dependencies":[{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.6.2","type":"blocks","created_at":"2026-02-13T21:07:20.566387072Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.6.3","type":"blocks","created_at":"2026-02-13T20:56:27.380627866Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.6.4","type":"blocks","created_at":"2026-02-13T21:07:20.663637831Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.7.1","type":"blocks","created_at":"2026-02-13T20:56:27.474804179Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.7.2","type":"blocks","created_at":"2026-02-13T20:56:27.573407881Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.7.3","type":"blocks","created_at":"2026-02-13T20:56:27.671866702Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.7.4","type":"blocks","created_at":"2026-02-13T20:56:27.772558212Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.7.5","type":"blocks","created_at":"2026-02-13T21:07:20.760356253Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.8","type":"parent-child","created_at":"2026-02-13T20:55:45.978092291Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.3","depends_on_id":"bd-2yu.8.1","type":"blocks","created_at":"2026-02-13T20:56:27.278895798Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":101,"issue_id":"bd-2yu.8.3","author":"Dicklesworthstone","text":"E2E logging requirements: scripts must emit machine-readable JSONL + concise human summaries. On failure, persist artifacts for screen snapshots, evidence logs, and timeline excerpts so triage does not require rerunning flaky scenarios blindly.","created_at":"2026-02-13T20:57:29Z"},{"id":185,"issue_id":"bd-2yu.8.3","author":"Dicklesworthstone","text":"Artifact policy: every failing e2e run must emit replay seed, evidence JSONL, terminal transcript, and snapshot diff. This is required to keep debugging deterministic and fast.","created_at":"2026-02-13T21:09:43Z"}]}
{"id":"bd-2yu.8.4","title":"Add load, performance, and fault-injection regression tests","description":"Task:\nImplement load/performance/fault-injection regression validation for telemetry, storage, and UI pipelines.\n\nMust test:\n- high search throughput bursts and backpressure behavior\n- heavy embedding backlogs with resource saturation\n- instance crash/restart, telemetry gaps, and stream reconnects\n- DB contention/recovery and alert materialization lag\n\nOutput:\nPerformance budgets + reliability thresholds wired into CI with clear failure diagnostics.","acceptance_criteria":"1) Load tests validate behavior under high search and embedding throughput.\n2) Fault tests cover crash/restart, telemetry gaps, ingestion contention, and recovery semantics.\n3) CI enforces explicit latency/memory/error-budget thresholds with actionable logs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:46.078642397Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:37.563731199Z","due_at":"2026-04-24T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fault-injection","frankensearch","performance","testing"],"dependencies":[{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.4.2","type":"blocks","created_at":"2026-02-13T21:07:20.857329402Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.4.4","type":"blocks","created_at":"2026-02-13T21:07:20.952360045Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T20:56:27.968572232Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.7.2","type":"blocks","created_at":"2026-02-13T20:56:28.069037699Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.7.3","type":"blocks","created_at":"2026-02-13T20:56:28.166077653Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.7.5","type":"blocks","created_at":"2026-02-13T21:07:21.048575465Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.8","type":"parent-child","created_at":"2026-02-13T20:55:46.078642397Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.4","depends_on_id":"bd-2yu.8.1","type":"blocks","created_at":"2026-02-13T20:56:27.874634656Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":102,"issue_id":"bd-2yu.8.4","author":"Dicklesworthstone","text":"Perf/fault test intent: validate graceful degradation and recovery, not just raw throughput. Explicitly assert UI responsiveness under overload and confirm no silent data loss when backpressure or storage contention is triggered.","created_at":"2026-02-13T20:57:29Z"}]}
{"id":"bd-2yu.8.5","title":"Add long-duration soak tests with leak detection and drift diagnostics","description":"Task:\nAdd long-duration soak/stress tests that validate the fleet dashboard and telemetry pipeline under sustained mixed workloads.\n\nMust include:\n- 6h/24h soak profiles with variable search + embedding rates and intermittent host restarts.\n- Memory leak/drift checks for control-plane process, ingestion queues, and renderer state.\n- Log artifact capture (structured logs, anomaly traces, replay seeds, resource curves).\n- Automatic failure triage summary pointing to first divergence signal.\n\nWhy this matters:\nShort tests miss slow regressions. Soak coverage is required to trust this as a daily operations console.","acceptance_criteria":"1) Soak suites run sustained mixed workloads (including restarts) and assert stability for target durations.\n2) Leak/drift detection flags memory, queue, or renderer growth beyond explicit budgets.\n3) Failure artifacts include replay seed, first divergence marker, and resource trend logs for diagnosis.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T21:06:41.445247423Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:37.563954627Z","due_at":"2026-04-24T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","fault-injection","logging","performance","phase-quality","quality","testing"],"dependencies":[{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.4.4","type":"blocks","created_at":"2026-02-13T21:06:41.445247423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.5.3","type":"blocks","created_at":"2026-02-13T21:06:41.445247423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.6.4","type":"blocks","created_at":"2026-02-13T21:07:21.145492459Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.7.5","type":"blocks","created_at":"2026-02-13T21:06:41.445247423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.8","type":"parent-child","created_at":"2026-02-13T21:06:41.445247423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.8.1","type":"blocks","created_at":"2026-02-13T21:06:41.445247423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.8.3","type":"blocks","created_at":"2026-02-13T21:18:29.969273543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.8.5","depends_on_id":"bd-2yu.8.4","type":"blocks","created_at":"2026-02-13T21:18:31.387554974Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":181,"issue_id":"bd-2yu.8.5","author":"Dicklesworthstone","text":"Revision rationale: soak coverage catches slow memory/drift regressions that short perf tests miss. Artifact requirements ensure failures are reproducible.","created_at":"2026-02-13T21:09:35Z"}]}
{"id":"bd-2yu.9","title":"Workstream: Documentation, CI gates, and rollout operations","description":"Goal:\nOperationalize the system with strong docs, CI gates, rollout controls, and operator-validated usability.\n\nScope:\n- architecture docs, operator runbooks, host integration guide\n- CI workflows and release/rollout checklists\n- usability pilot and feedback loop into UX defaults/docs\n\nQuality bar:\nA new operator should be able to triage incidents and validate rollouts using docs + tooling only.","acceptance_criteria":"1) Operator and integration documentation is complete and self-contained.\n2) CI/release gates enforce required validation suites and artifact capture.\n3) Rollout + usability validation enables safe staged adoption across target host projects.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T20:55:46.187587960Z","created_by":"ubuntu","updated_at":"2026-02-13T21:07:51.441423513Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["ci","docs","frankensearch","phase-ops","rollout"],"dependencies":[{"issue_id":"bd-2yu.9","depends_on_id":"bd-2yu","type":"parent-child","created_at":"2026-02-13T20:55:46.187587960Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9","depends_on_id":"bd-2yu.8","type":"blocks","created_at":"2026-02-13T20:56:21.631834290Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":98,"issue_id":"bd-2yu.9","author":"Dicklesworthstone","text":"Future-self rationale: without CI gates and rollout docs, even a technically strong implementation will fail during adoption. This workstream ensures we can deploy safely across cass/xf/mcp_agent_mail_rust/frankenterm and diagnose regressions quickly.","created_at":"2026-02-13T20:56:48Z"}]}
{"id":"bd-2yu.9.1","title":"Write architecture docs, operator runbook, and host integration guide","description":"Task:\nWrite self-contained architecture and operations documentation for the frankensearch control-plane TUI.\n\nMust include:\n- data-flow/contracts docs including SLO/anomaly semantics\n- screen guide + operator workflows + keyboard model\n- troubleshooting playbook, deterministic replay workflow, and incident-response steps\n- host integration guide using adapter SDK + conformance harness\n- rollout verification and rollback procedures","acceptance_criteria":"1) Docs cover architecture, workflows, troubleshooting, and incident handling end-to-end.\n2) Integration guide is actionable for both known and future host projects via SDK/conformance steps.\n3) Runbook includes concrete verification, replay, and rollback procedures.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:46.292310959Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:41.528705966Z","due_at":"2026-05-01T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","docs","frankensearch","runbook"],"dependencies":[{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.4.3","type":"blocks","created_at":"2026-02-13T20:56:28.260262553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.5.4","type":"blocks","created_at":"2026-02-13T20:56:28.731793043Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.5.5","type":"blocks","created_at":"2026-02-13T20:56:28.825423844Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.5.6","type":"blocks","created_at":"2026-02-13T20:56:28.919856678Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.5.7","type":"blocks","created_at":"2026-02-13T20:56:29.013673828Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.5.8","type":"blocks","created_at":"2026-02-13T21:07:21.243189073Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.6.4","type":"blocks","created_at":"2026-02-13T21:07:21.342162998Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.7.1","type":"blocks","created_at":"2026-02-13T20:56:28.353680195Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.7.2","type":"blocks","created_at":"2026-02-13T20:56:28.447616839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.7.3","type":"blocks","created_at":"2026-02-13T20:56:28.541563392Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.7.4","type":"blocks","created_at":"2026-02-13T20:56:28.638025164Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.7.5","type":"blocks","created_at":"2026-02-13T21:07:21.442452436Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.8.5","type":"blocks","created_at":"2026-02-13T21:07:21.542454285Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.1","depends_on_id":"bd-2yu.9","type":"parent-child","created_at":"2026-02-13T20:55:46.292310959Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":209,"issue_id":"bd-2yu.9.1","author":"Dicklesworthstone","text":"Docs should be sufficient for a new operator to run triage and rollback with no tribal context; include concrete commands/checklists.","created_at":"2026-02-13T21:10:35Z"}]}
{"id":"bd-2yu.9.2","title":"Implement CI quality gates and phased rollout checklist","description":"Task:\nImplement CI gates and phased rollout checklist for safe production-style adoption.\n\nMust gate on:\n- unit/integration/snapshot/e2e/perf/fault/soak suites\n- contract/schema and conformance validation\n- artifact publishing (logs, replay seeds, benchmark summaries)\n\nMust include:\n- phased rollout checklist across coding_agent_session_search/xf/mcp_agent_mail_rust/frankenterm\n- post-rollout health verification and rollback triggers\n- CI matrix guidance for fast pre-merge and full nightly validation modes","acceptance_criteria":"1) CI runs required validation suites and publishes diagnostic artifacts for failures.\n2) Rollout checklist defines staged validation for all target host projects.\n3) Post-rollout health checks and rollback triggers are documented and executable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:55:46.396518061Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:41.529024232Z","due_at":"2026-05-01T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["ci","frankensearch","release","rollout"],"dependencies":[{"issue_id":"bd-2yu.9.2","depends_on_id":"bd-2yu.7.5","type":"blocks","created_at":"2026-02-13T21:07:21.638842830Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.2","depends_on_id":"bd-2yu.8.2","type":"blocks","created_at":"2026-02-13T20:56:29.106888441Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.2","depends_on_id":"bd-2yu.8.3","type":"blocks","created_at":"2026-02-13T20:56:29.203419923Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.2","depends_on_id":"bd-2yu.8.4","type":"blocks","created_at":"2026-02-13T20:56:29.300981825Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.2","depends_on_id":"bd-2yu.8.5","type":"blocks","created_at":"2026-02-13T21:07:21.733463605Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.2","depends_on_id":"bd-2yu.9","type":"parent-child","created_at":"2026-02-13T20:55:46.396518061Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":184,"issue_id":"bd-2yu.9.2","author":"Dicklesworthstone","text":"Optimization note: CI gating was intentionally decoupled from docs completion so validation can be enabled earlier and catch regressions while documentation is still being finalized.","created_at":"2026-02-13T21:09:43Z"}]}
{"id":"bd-2yu.9.3","title":"Run operator usability pilot and feed findings back into docs/UX defaults","description":"Task:\nRun a structured operator usability pilot across target host projects and convert findings into concrete UX/default tuning updates.\n\nMust include:\n- Scenario-based pilot script (incident triage, index lag diagnosis, throughput spike analysis).\n- Quantitative usability checkpoints (time-to-diagnosis, navigation errors, confidence score).\n- Captured feedback mapped to specific screens, shortcuts, labels, and defaults.\n- Follow-up checklist ensuring docs/runbook reflect the final tuned workflows.\n\nWhy this matters:\nThis ensures the TUI is not only feature-complete but genuinely intuitive and reliable for real operators.","acceptance_criteria":"1) Usability pilot covers all critical operator scenarios across target host projects.\n2) Findings are translated into concrete UI/default/doc updates with traceability.\n3) Post-pilot runbook and UX defaults demonstrably improve diagnosis speed and operator confidence.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T21:06:50.544811645Z","created_by":"ubuntu","updated_at":"2026-02-13T21:44:41.529273849Z","due_at":"2026-05-01T13:00:00Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","frankensearch","rollout","runbook","ux"],"dependencies":[{"issue_id":"bd-2yu.9.3","depends_on_id":"bd-2yu.6.4","type":"blocks","created_at":"2026-02-13T21:06:50.544811645Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.3","depends_on_id":"bd-2yu.7.5","type":"blocks","created_at":"2026-02-13T21:06:50.544811645Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.3","depends_on_id":"bd-2yu.8.3","type":"blocks","created_at":"2026-02-13T21:06:50.544811645Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.3","depends_on_id":"bd-2yu.9","type":"parent-child","created_at":"2026-02-13T21:06:50.544811645Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.3","depends_on_id":"bd-2yu.9.1","type":"blocks","created_at":"2026-02-13T21:06:50.544811645Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yu.9.3","depends_on_id":"bd-2yu.9.2","type":"blocks","created_at":"2026-02-13T21:07:21.829837953Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":182,"issue_id":"bd-2yu.9.3","author":"Dicklesworthstone","text":"Revision rationale: explicit usability pilot closes the loop between technical correctness and operator effectiveness, then feeds findings back into docs/defaults.","created_at":"2026-02-13T21:09:36Z"}]}
{"id":"bd-3st","title":"Progressive PRF Query Expansion","description":"Implement pseudo-relevance feedback (PRF) query expansion between Phase 1 (Initial) and Phase 2 (Refined) of the TwoTierSearcher. After the fast tier returns initial results, extract the top-k result embeddings and compute a centroid that \"nudges\" the quality-tier query embedding toward the neighborhood of relevant documents.\n\n## Background\n\nFrankensearch uses a two-tier search architecture: Phase 1 uses fast (smaller) embeddings for initial retrieval, and Phase 2 uses quality (larger) embeddings for refinement. These two embedding models encode different aspects of semantic meaning, and the same query may map to different neighborhoods in each embedding space. PRF bridges this gap by using Phase 1's relevance signal to inform Phase 2's query.\n\nThis is an adaptation of the Rocchio algorithm (1971), one of the most well-established techniques in information retrieval, adapted for frankensearch's two-tier neural architecture.\n\n## Algorithm\n\n1. Phase 1 completes with fast-tier results (top-k fast embeddings + RRF scores)\n2. Compute centroid of top-k fast embeddings, weighted by RRF score\n3. Quality-tier query embedding = alpha * original_quality_embedding + (1-alpha) * centroid\n4. Phase 2 uses this expanded embedding for quality-tier search\n5. alpha in [0.5, 1.0], default 0.8 (mostly original query, slight nudge toward relevant neighborhood)\n\n### Key Insight\n\nFrankensearch's two-tier design provides a \"free\" feedback signal: Phase 1 results are fast to compute and provide a relevance neighborhood before Phase 2 even starts. This is unlike traditional PRF which requires a separate retrieval pass.\n\n## Implementation\n\n```rust\npub struct PrfConfig {\n    pub enabled: bool,           // Default: false\n    pub alpha: f64,              // Interpolation weight (default: 0.8)\n    pub top_k_feedback: usize,   // Number of Phase 1 results to use (default: 5)\n    pub score_weighted: bool,    // Weight centroid by RRF scores (default: true)\n}\n\npub fn prf_expand(\n    original_embedding: &[f32],\n    feedback_embeddings: &[(&[f32], f64)],  // (embedding, weight)\n    alpha: f64,\n) -> Vec<f32>;\n```\n\nThe `prf_expand` function:\n1. Computes the weighted centroid of feedback_embeddings (weight = RRF score if score_weighted, else uniform)\n2. Interpolates: result = alpha * original + (1-alpha) * centroid\n3. L2-normalizes the result (for cosine similarity compatibility)\n\n## Guard Rails\n\n- Only expand if Phase 1 returns >= min_feedback_docs (default: 3). With fewer feedback documents, the centroid is noisy and may hurt relevance.\n- Only expand for NaturalLanguage queries (not Identifier/ShortKeyword). Short/keyword queries don't benefit from PRF because their embedding is already precise.\n- Configurable: consumers can disable PRF entirely (enabled=false, the default).\n- alpha is clamped to [0.5, 1.0] to prevent the centroid from overwhelming the original query intent.\n\n## Justification\n\nQuality-tier embeddings and fast-tier embeddings live in different embedding spaces. A query that maps cleanly in the fast space may land in a suboptimal neighborhood in the quality space. PRF bridges this gap by using Phase 1's relevance signal to guide Phase 2's query. Empirically, this technique improves recall@10 by 5-15% for ambiguous or multi-faceted queries where the fast and quality embeddings disagree on the relevant neighborhood.\n\n## Dependencies Rationale\n\n- Depends on bd-3un.43 (query classification) because PRF should only activate for NaturalLanguage-classified queries\n- Depends on bd-3un.24 (TwoTierSearcher) because it modifies the TwoTierSearcher flow between Phase 1 and Phase 2\n- Depends on bd-3un.3 (Embedder trait) because it operates on embedding vectors produced by the Embedder\n\n## Considerations\n\n- Embedding dimensionality: fast and quality embeddings may have different dimensions. PRF operates within the quality embedding space, using Phase 1 results to look up their corresponding quality embeddings (if available) or to project from fast to quality space.\n- Centroid quality: if Phase 1 results are poor (low scores), the centroid may be noisy. The alpha parameter mitigates this (0.8 = 80% original query).\n- Computational cost: centroid computation is O(k*d) where k=feedback docs, d=embedding dimension. For k=5, d=768, this is ~3840 FP multiplications = <0.1ms.\n\n## Testing\n\n- [ ] Unit: alpha=1.0 produces no expansion (original embedding unchanged within epsilon)\n- [ ] Unit: alpha=0.0 produces pure centroid (degenerate case, but mathematically correct)\n- [ ] Unit: score-weighted centroid vs uniform centroid produce different results\n- [ ] Unit: insufficient feedback docs (< min_feedback_docs) skips expansion\n- [ ] Unit: QueryClass guard (only NaturalLanguage triggers expansion)\n- [ ] Unit: output embedding is L2-normalized\n- [ ] Integration: verify recall improvement on test corpus with clustered documents\n- [ ] Benchmark: PRF overhead (centroid computation should be <0.1ms for typical parameters)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T21:59:59.613087171Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:33.434178969Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3st","depends_on_id":"bd-1do","type":"blocks","created_at":"2026-02-13T22:05:33.434131681Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3st","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T22:01:19.539309784Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3st","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T22:02:15.766257662Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3st","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T22:01:19.310830651Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3st","depends_on_id":"bd-3un.43","type":"blocks","created_at":"2026-02-13T22:01:19.192824589Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3st","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T22:01:19.423950567Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":295,"issue_id":"bd-3st","author":"Dicklesworthstone","text":"REVISION (review pass 7 - formal dependencies added):\n\nAdded formal dependencies that were described in body text but not wired:\n- bd-3un.43 (query classification): PRF guard — only activate for NaturalLanguage queries\n- bd-3un.3 (Embedder trait): PRF operates on embedding vectors\n- bd-3un.5 (result types): PRF uses VectorHit/FusedHit from Phase 1 results\n- bd-3un.2 (error types): PRF returns SearchResult\n\nNOTE: bd-3st does NOT formally depend on bd-3un.24 (TwoTierSearcher) despite the body text suggesting it. The prf_expand() function is a pure computation on embedding vectors — it doesn't need the searcher to exist. The TwoTierSearcher calls PRF, not the other way around. The integration point is in bd-3un.24's implementation, which optionally calls prf_expand() if PrfConfig.enabled is true.\n\nFEATURE FLAG: PRF should be behind a `prf` feature flag (disabled by default). Add to bd-3un.29's feature map:\n  prf = []  # Pseudo-relevance feedback query expansion\n  full = [..., \"prf\"]  # Include in full bundle\n","created_at":"2026-02-13T22:02:05Z"}]}
{"id":"bd-3un","title":"Epic: Create frankensearch standalone crate","description":"Extract the 2-tier hybrid search system from cass, xf, and mcp_agent_mail_rust into a standalone, reusable Rust crate called frankensearch. This crate should be drop-in for any future Rust project needing high-quality local semantic+lexical hybrid search with progressive refinement.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-13T17:46:43.968487926Z","created_by":"ubuntu","updated_at":"2026-02-13T20:47:52.741945809Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["epic","meta"],"comments":[{"id":1,"issue_id":"bd-3un","author":"Dicklesworthstone","text":"PROJECT VISION: frankensearch is a standalone, reusable Rust crate that extracts the 2-tier hybrid search system currently duplicated across 3 projects (cass, xf, mcp_agent_mail_rust). The name 'frankensearch' reflects its Frankenstein-like assembly from the best parts of each codebase.\n\nKEY INSIGHT (from the X post): The user wanted the best of both worlds — sub-millisecond response time (potion-128M) AND high-quality semantic understanding (MiniLM-L6-v2). Rather than choosing one, the solution is a 2-tier progressive system: show fast results immediately, then upgrade them in the background when the quality model finishes. This creates a smooth UX where results appear instantly and improve within ~150ms.\n\nBAKEOFF RESULTS (motivation for model choices):\n- FNV-1a hash: 0.07ms, no semantic meaning (fallback only)\n- potion-multilingual-128M: 0.57ms, decent semantics (223x faster than MiniLM)\n- all-MiniLM-L6-v2: 128ms, excellent semantics (baseline quality)\n- The 223x speed gap between potion and MiniLM is exactly why the 2-tier design exists.\n\nHYBRID SEARCH: Combines lexical (Tantivy BM25) with semantic (vector cosine similarity) via Reciprocal Rank Fusion (RRF, K=60). Documents appearing in both lexical AND semantic results get boosted scores, which empirically produces better results than either alone.\n\nGOAL: Drop-in crate for any Rust project needing local search. Feature-gated so consumers pay only for what they use.","created_at":"2026-02-13T17:56:21Z"},{"id":11,"issue_id":"bd-3un","author":"Dicklesworthstone","text":"TESTING POLICY: Every component bead (bd-3un.6 through bd-3un.28) MUST include inline #[cfg(test)] unit tests alongside the implementation. These inline tests should cover: (a) happy path, (b) edge cases (empty input, max values, boundary conditions), (c) error conditions. The dedicated testing beads (bd-3un.31, bd-3un.32, bd-3un.40) are for CROSS-COMPONENT tests, integration tests, and e2e validation scripts -- not for basic per-component unit testing.\n\nLOGGING POLICY: All public functions should use the tracing crate for structured logging:\n- ERROR: unrecoverable failures (model load failed, index corrupted)\n- WARN: degraded operation (quality model unavailable, fallback to hash)\n- INFO: significant lifecycle events (index opened, model loaded, search completed)\n- DEBUG: operational details (query embedding latency, candidate counts, blend scores)\n- TRACE: hot-path internals (individual dot products, per-record scores) -- gated behind cfg\n\nEvery search operation should emit a tracing span with: query length, k, phase, result count, latency_ms, embedder used.\n","created_at":"2026-02-13T20:11:19Z"},{"id":80,"issue_id":"bd-3un","author":"Dicklesworthstone","text":"CROSS-REFERENCE: FrankenSQLite Integration Epic (bd-3w1)\n\nThe FrankenSQLite integration (bd-3w1) is a sibling epic that adds:\n- Persistent document storage via FrankenSQLite (replaces in-memory state)\n- Self-healing indices via pervasive RaptorQ erasure coding\n- FTS5 as alternative lexical engine (alongside Tantivy)\n\nKey cross-epic dependencies:\n- bd-3w1.1 (storage crate) depends on bd-3un.1 (scaffold) and bd-3un.2 (errors)\n- bd-3w1.5 (durability crate) depends on bd-3un.1 (scaffold) and bd-3un.2 (errors)\n- bd-3w1.7 (FSVI RaptorQ) depends on bd-3un.13 (FSVI format)\n- bd-3w1.8 (Tantivy RaptorQ) depends on bd-3un.17 (Tantivy schema)\n- bd-3w1.10 (FTS5) depends on bd-3un.18 (Tantivy queries, for LexicalIndex trait)\n- bd-3w1.13 (pipeline) depends on bd-3un.27 (embedding job runner)\n- bd-3w1.12 (staleness) depends on bd-3un.41 (staleness detection)\n- bd-3w1.14 (features) depends on bd-3un.29 (feature flags)\n- bd-3w1.21 (facade) depends on bd-3un.30 (public API)\n\nThe two epics can be worked in parallel: bd-3un tasks build the core search engine,\nbd-3w1 tasks add persistence and durability on top.\n","created_at":"2026-02-13T20:47:52Z"}]}
{"id":"bd-3un.1","title":"Scaffold Cargo workspace and crate structure","description":"Create the frankensearch Cargo workspace with the following structure:\n\nfrankensearch/\n├── Cargo.toml (workspace root)\n├── crates/\n│   ├── frankensearch-core/      # Traits, types, error types, scoring primitives\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-embed/     # All embedder implementations (hash, model2vec, fastembed)\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-index/     # Vector index, SIMD, ANN\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-lexical/   # Tantivy integration (feature-gated)\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-fusion/    # RRF, blending, two-tier orchestration\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   └── frankensearch-rerank/    # Reranker trait + implementations\n│       ├── Cargo.toml\n│       └── src/lib.rs\n├── frankensearch/               # Facade crate re-exporting everything\n│   ├── Cargo.toml\n│   └── src/lib.rs\n├── tests/\n├── benches/\n└── examples/\n\nDesign decisions:\n- Workspace-level dep management (workspace = true pattern)\n- Feature flags: 'full' (everything), 'semantic' (embedders), 'lexical' (tantivy), 'hybrid' (both), 'rerank' (rerankers)\n- Rust edition 2024 (nightly), matching existing projects\n- Release profile: opt-level='z', lto=true, codegen-units=1, strip=true\n- unsafe code: forbidden (#\\![forbid(unsafe_code)])\n\nKey workspace deps (from cross-referencing cass/xf/agent-mail):\n- half = '2.4' (f16 quantization)\n- wide = '0.7' (SIMD f32x8)\n- fastembed = '4.9' (ONNX embeddings)\n- tantivy = '0.22' (full-text search)\n- tokenizers = '0.21' (HuggingFace BPE)\n- safetensors = '0.5' (Model2Vec weights)\n- ort = '2.0.0-rc.9' (ONNX Runtime)\n- rayon = '1.10' (data parallelism)\n- serde + serde_json (serialization)\n- thiserror (error types)\n- tracing (structured logging)\n- memmap2 (memory-mapped files)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:01.537714547Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:12.891095505Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation","phase1","setup"],"dependencies":[{"issue_id":"bd-3un.1","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:01.537714547Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":4,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"CRATE STRUCTURE RATIONALE: The workspace is split into multiple sub-crates to enable:\n1. Selective compilation: consumers only compile what they need\n2. Parallel compilation: independent crates build concurrently\n3. Clear boundaries: traits in -core, impls in domain crates\n4. Testing isolation: each crate testable independently\n\nThe facade crate (frankensearch/) re-exports everything so consumers can just 'use frankensearch::*' in simple cases, or import sub-crates directly for finer control.\n\nThe 6 sub-crates map to the logical architecture:\n- core: zero-dep traits/types (everything depends on this)\n- embed: all embedder implementations (feature-gated per model)\n- index: vector storage and search (SIMD, mmap)\n- lexical: Tantivy full-text search\n- fusion: RRF, blending, two-tier orchestration\n- rerank: cross-encoder reranking","created_at":"2026-02-13T17:56:48Z"},{"id":153,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — WORKSPACE DEPENDENCY UPDATE:\n\nAdd asupersync as a workspace-level dependency. Update feature flags accordingly.\n\nBEFORE (Cargo.toml workspace deps):\n  rayon = \"1.10\"\n  crossbeam-channel = \"0.5\"\n  reqwest = { version = \"0.12\", features = [\"rustls-tls\"], optional = true }\n\nAFTER:\n  rayon = \"1.10\"                    # RETAINED for data parallelism (bd-3un.15)\n  asupersync = { path = \"/dp/asupersync\", features = [\"proc-macros\"] }\n  # crossbeam-channel REMOVED (replaced by asupersync::channel)\n  # reqwest REMOVED (replaced by asupersync::http + asupersync::tls)\n\nFEATURE FLAGS for asupersync in frankensearch:\n  - \"proc-macros\": scope!, spawn!, join! macros (quality of life)\n  - \"tls\": only needed when 'download' feature is enabled\n  - \"sqlite\": only if bd-3w1 (FrankenSQLite) integration uses asupersync's sqlite bridge\n  - \"metrics\": optional, for OpenTelemetry integration\n  - \"test-internals\": for LabRuntime in tests\n\nPER-CRATE DEPENDENCIES:\n  - frankensearch-core: asupersync (for sync primitives, Cx, Outcome, Error)\n  - frankensearch-embed: asupersync (for Mutex, Pool, Cx)\n  - frankensearch-index: rayon (data parallelism), asupersync (Cx for cancel checkpoints)\n  - frankensearch-lexical: asupersync (Cx for cancel-aware Tantivy queries)\n  - frankensearch-fusion: asupersync (Cx, region, scope, join, timeout, channels)\n  - frankensearch-rerank: asupersync (Mutex for ONNX session)\n  - frankensearch (facade): re-exports asupersync::Cx, asupersync::Outcome\n\nZERO-TOKIO GUARANTEE:\n  After this migration, the dependency tree contains ZERO references to tokio, hyper, or any tokio-ecosystem crate. The only external async runtime is asupersync. Rayon is retained for data parallelism (not an async runtime).","created_at":"2026-02-13T21:06:20Z"},{"id":191,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"REVISION (review pass 4 - crate structure update for trait extraction):\n\n1. NEW SUB-CRATE: frankensearch-core must define the LexicalIndex trait (not frankensearch-lexical). This enables both Tantivy (in frankensearch-lexical) and FTS5 (in frankensearch-storage) to implement the same trait without depending on each other. The trait and its associated types go in:\n   frankensearch-core/src/traits/lexical.rs\n\n   pub trait LexicalIndex: Send + Sync {\n       async fn search(&self, cx: &Cx, query: &str, limit: usize, offset: usize) -> SearchResult<Vec<LexicalHit>>;\n       async fn index_document(&self, cx: &Cx, doc: &IndexableDocument) -> SearchResult<()>;\n       async fn index_batch(&self, cx: &Cx, docs: &[IndexableDocument]) -> SearchResult<usize>;\n       async fn delete_document(&self, cx: &Cx, doc_id: &str) -> SearchResult<bool>;\n       fn document_count(&self) -> SearchResult<usize>;\n       async fn optimize(&self, cx: &Cx) -> SearchResult<()>;\n   }\n\n2. UPDATED CRATE MAP (add to scaffold):\n   frankensearch-core/src/\n   ├── lib.rs\n   ├── error.rs           (bd-3un.2)\n   ├── types.rs            (bd-3un.5)\n   ├── canonicalize.rs     (bd-3un.42)\n   └── traits/\n       ├── mod.rs\n       ├── embedder.rs     (bd-3un.3: Embedder trait)\n       ├── reranker.rs     (bd-3un.4: Reranker trait)\n       └── lexical.rs      (LexicalIndex trait, shared by Tantivy + FTS5)\n\n3. frankensearch-storage and frankensearch-durability sub-crates (from bd-3w1.1, bd-3w1.5) should also be listed in the workspace scaffold, even though they're feature-gated.\n","created_at":"2026-02-13T21:10:27Z"},{"id":221,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"REVIEW FIX — opt-level and workspace dependency corrections:\n\n1. RELEASE PROFILE: The body specifies opt-level='z' (optimize for binary size). This CONFLICTS with the project's performance goals — frankensearch is a search library where SIMD throughput (bd-3un.14), vector search latency (bd-3un.15), and embedding speed are critical. Fix:\n\n   [profile.release]\n   opt-level = 3       # Maximum runtime performance (NOT 'z' which optimizes for size)\n   lto = true           # Link-time optimization for cross-crate inlining\n   codegen-units = 1    # Single codegen unit for better optimization opportunities\n   strip = true         # Remove debug symbols from release binary\n\n   This matches what AGENTS.md already specifies. The bead body should be updated to match.\n\n2. MISSING WORKSPACE DEPENDENCY: Add `dirs` crate (used by bd-3un.9 for platform-specific model data directories):\n   dirs = \"6\"    # Platform-standard data directories for model storage\n\n3. ASUPERSYNC BEFORE/AFTER CLARIFICATION: The ASUPERSYNC comment references removing `crossbeam-channel` and `reqwest` in its BEFORE/AFTER diff, but neither appears in the original body's dependency list. Clarification: these were implicit dependencies that would have been added during implementation of bd-3un.27 (crossbeam) and bd-3un.11 (reqwest). The ASUPERSYNC comment correctly prevents them from ever being added. No action needed beyond this clarification note.\n\n4. WORKSPACE-LEVEL DEV-DEPENDENCIES: Add for LabRuntime testing:\n   [workspace.dev-dependencies]\n   asupersync = { path = \"/dp/asupersync\", features = [\"test-internals\"] }\n   # Provides LabRuntime, oracles, DPOR schedule exploration for deterministic tests","created_at":"2026-02-13T21:46:23Z"},{"id":236,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"REVISION (review pass 7 - opt-level correction):\n\nThe bead body says \"opt-level='z'\" but AGENTS.md mandates opt-level=3. This is a search library where performance is the primary concern — opt-level=3 (maximum performance) is correct, NOT opt-level='z' (minimum size).\n\nCORRECTED release profile:\n  [profile.release]\n  opt-level = 3       # Maximum performance optimization (NOT 'z')\n  lto = true          # Link-time optimization\n  codegen-units = 1   # Single codegen unit for better optimization\n  strip = true        # Remove debug symbols\n\nThis matches the release profile specified in AGENTS.md exactly.\n","created_at":"2026-02-13T21:49:01Z"},{"id":238,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"CORRECTNESS FIX: opt-level discrepancy\n\nThe description says `opt-level='z'` but AGENTS.md correctly specifies `opt-level = 3`.\n\n- opt-level='z': optimizes for binary SIZE (smaller output)\n- opt-level=3: optimizes for execution SPEED (maximum performance)\n\nFor a search library where sub-millisecond SIMD dot products matter,\nopt-level=3 is the correct choice. The description's 'z' is an error.\n\nCorrect release profile (per AGENTS.md):\n  [profile.release]\n  opt-level = 3       # Maximum performance optimization\n  lto = true          # Link-time optimization\n  codegen-units = 1   # Single codegen unit for better optimization\n  strip = true        # Remove debug symbols\n\nImplementers: use opt-level = 3, NOT 'z'.\n","created_at":"2026-02-13T21:50:12Z"}]}
{"id":"bd-3un.10","title":"Implement model manifest and SHA256 verification","description":"Implement the model manifest system that tracks required model files, their SHA256 checksums, and HuggingFace repo details. This ensures reproducible, verifiable model installations.\n\npub struct ModelManifest {\n    pub id: String,               // e.g., 'all-minilm-l6-v2'\n    pub repo: String,             // HuggingFace repo path\n    pub revision: String,         // Pinned commit SHA for reproducibility\n    pub files: Vec<ModelFile>,    // Required files with checksums\n    pub license: String,          // SPDX identifier\n}\n\npub struct ModelFile {\n    pub name: String,             // Path in repo (e.g., 'onnx/model.onnx')\n    pub sha256: String,           // Expected SHA256 hex string\n    pub size: u64,                // Expected file size in bytes\n}\n\npub enum ModelState {\n    NotInstalled,\n    NeedsConsent,                 // User must approve before download\n    Downloading { progress_pct: u8, bytes_downloaded: u64, total_bytes: u64 },\n    Verifying,\n    Ready,\n    Disabled { reason: String },\n    VerificationFailed { reason: String },\n    UpdateAvailable { current_revision: String, latest_revision: String },\n    Cancelled,\n}\n\nKey principles (from cass src/search/model_download.rs):\n- NO network calls without explicit user consent (consent-gated downloads)\n- Placeholder checksum constant: 'PLACEHOLDER_VERIFY_AFTER_DOWNLOAD'\n- Production-ready = has_verified_checksums() && has_pinned_revision()\n- Atomic installation: download to temp dir, verify, then rename into place\n\nBuilt-in manifests:\n- ModelManifest::minilm_v2() - the baseline quality model\n- ModelManifest::potion_128m() - the fast tier model\n\nVerification:\n- SHA256 streaming verification during download (sha2 crate)\n- Post-install verification on model load\n- Version upgrade detection (compare revisions)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:49:26.536014967Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:34.040450500Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["model-mgmt","phase3"],"dependencies":[{"issue_id":"bd-3un.10","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:49:26.536014967Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.10","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:12.408296236Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":111,"issue_id":"bd-3un.10","author":"Dicklesworthstone","text":"REVISION: Model Manifest and Consent Flow\n\n1. Consent-Gated Downloads:\n   The manifest system enforces NO network access without explicit user consent.\n   State machine: NotInstalled -> NeedsConsent -> [user approves] -> Downloading -> ...\n\n   Consent mechanisms (in priority order):\n   a) Programmatic: ModelManifest::set_consent(true) in code\n   b) Environment: FRANKENSEARCH_ALLOW_DOWNLOAD=1\n   c) Interactive: prompt on stderr if TTY detected\n   d) Config file: frankensearch.toml consent = true\n\n   Without consent, auto_detect() returns HashOnly (hash embedder always works).\n   This is critical for CI/CD environments where network access is unexpected.\n\n2. Built-in Manifests:\n   Two models ship as compile-time constants:\n   - MiniLM-L6-v2: sentence-transformers/all-MiniLM-L6-v2\n     SHA256 for model.onnx: [computed at implementation time]\n     Total size: ~90MB (5 files)\n   - potion-128M: minishlab/potion-base-128M\n     SHA256 for model.safetensors: [computed at implementation time]\n     Total size: ~32MB (2 files)\n\n3. Manifest Extensibility:\n   Users can register custom manifests via:\n   ModelManifest::register(ModelManifest { id: \"my-model\", ... });\n   This allows custom embedders to participate in the manifest/verification system.\n\n4. Version Pinning:\n   Each manifest includes a HuggingFace revision hash.\n   Verification fails if the downloaded files don't match the pinned revision.\n   This prevents silent model updates from changing search quality.\n\n5. Verification Performance:\n   SHA256 of a 90MB file: ~200ms (streaming, not loaded into memory).\n   Verification runs at load time, NOT at every search call.\n   Cached verification: write .verified sentinel file after first check.\n","created_at":"2026-02-13T20:57:47Z"},{"id":231,"issue_id":"bd-3un.10","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.10 (Model Manifest):\n- Manifest validation: invalid JSON → clear error\n- SHA256 verification: correct hash passes, wrong hash fails, truncated file fails\n- State machine: Available → Downloading → Available (success), Downloading → Failed (error)\n- progress_pct: values in 0..=100 range\n- Placeholder detection: PLACEHOLDER_VERIFY_AFTER_DOWNLOAD is rejected in release builds\n- Cancelled → recovery: re-download succeeds after previous cancel\n- Empty manifest: no models listed → no crash\n- File permissions: model file not readable → clear error","created_at":"2026-02-13T21:47:34Z"}]}
{"id":"bd-3un.11","title":"Implement model download system with progress reporting","description":"Implement the model download system that handles fetching model files from HuggingFace with progress reporting, resumable downloads, and atomic installation.\n\nDownload pipeline:\n1. Check ModelState → if NotInstalled, transition to NeedsConsent\n2. On consent, transition to Downloading\n3. For each file in manifest:\n   a. HTTP GET from HuggingFace CDN with range headers for resume\n   b. Stream to temp file with progress callbacks\n   c. SHA256 verification during streaming\n4. After all files downloaded, verify checksums (Verifying state)\n5. Atomic rename from temp dir to final location (Ready state)\n\nProgress reporting:\n- AtomicU64 for bytes_downloaded (lock-free progress reads)\n- AtomicBool for cancellation\n- Callback-based progress: Fn(DownloadProgress) for UI integration\n- Rate-limited progress updates (every 100ms or 1% progress)\n\npub struct DownloadProgress {\n    pub file_name: String,\n    pub bytes_downloaded: u64,\n    pub total_bytes: u64,\n    pub files_completed: usize,\n    pub files_total: usize,\n    pub speed_bytes_per_sec: u64,\n    pub eta_seconds: Option<u64>,\n}\n\nNetwork policy: \n- reqwest with rustls (no system SSL deps)\n- Timeout: 30s connect, 300s total per file\n- Retry: 3 attempts with exponential backoff\n- User-Agent: 'frankensearch/{version}'\n\nThis is behind a 'download' feature flag to keep the core crate network-free.\n\nReference: cass src/search/model_download.rs (state machine, atomic install)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:49:26.674121057Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:34.146287791Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["download","model-mgmt","phase3"],"dependencies":[{"issue_id":"bd-3un.11","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:49:26.674121057Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.11","depends_on_id":"bd-3un.10","type":"blocks","created_at":"2026-02-13T17:55:12.493111893Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":112,"issue_id":"bd-3un.11","author":"Dicklesworthstone","text":"REVISION: Model Download System Details\n\n1. Resumable Downloads:\n   On network interruption, the partial file is kept (with .part suffix).\n   On next attempt, send HTTP Range header to resume from byte offset.\n   The streaming SHA256 hasher state is NOT persisted (recompute from start on resume).\n   This means: resume is efficient for download, but verification restarts.\n\n2. Atomic Installation:\n   Download to: {model_dir}/{model_id}/.download/{file_name}.part\n   Complete to: {model_dir}/{model_id}/.download/{file_name}\n   After all files verified: atomic rename .download/ to final location\n   On failure: .download/ directory is left for retry (not cleaned up)\n   On success: .download/ is removed\n\n3. Progress Reporting:\n   DownloadProgress struct:\n   - bytes_downloaded: u64 (AtomicU64, lock-free)\n   - bytes_total: u64\n   - speed_bytes_per_sec: f64 (smoothed exponential moving average)\n   - eta_seconds: f64\n   - is_cancelled: bool (AtomicBool)\n\n   Callback: Box<dyn Fn(DownloadProgress) + Send>\n   Rate-limited to max 10 updates/second (avoid flooding UI)\n   Default callback: log at INFO with progress bar format\n\n4. Cancellation:\n   Set is_cancelled = true via the DownloadProgress handle.\n   Worker checks is_cancelled between chunk reads (every ~64KB).\n   On cancel: state transitions to Cancelled, partial files kept.\n   Next attempt resumes from where it left off.\n\n5. Network Robustness:\n   - Connect timeout: 30s (fail fast on unreachable hosts)\n   - Read timeout: 300s per chunk (handle slow connections)\n   - Retry: 3 attempts with exponential backoff (1s, 2s, 4s)\n   - User-agent: \"frankensearch/{version}\" (for HuggingFace rate limiting)\n   - HTTPS only (rustls, no OpenSSL dependency)\n   - Respect HuggingFace rate limits (429 -> back off per Retry-After header)\n","created_at":"2026-02-13T20:57:48Z"},{"id":149,"issue_id":"bd-3un.11","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MAJOR REVISION (replaces reqwest/tokio with asupersync HTTP):\n\nThe model download system replaces reqwest (which pulls in tokio as a transitive dependency) with asupersync's native HTTP/1.1 client. This eliminates the ONLY tokio transitive dependency in the entire project.\n\nBEFORE (reqwest):\n  - reqwest::blocking::Client for HTTP downloads (pulls in tokio via reqwest)\n  - AtomicU64 for bytes_downloaded progress\n  - AtomicBool for cancellation\n  - Feature-gated: download = ['dep:reqwest']\n\nAFTER (asupersync):\n  - asupersync::http::h1 for HTTP/1.1 client (native, no tokio)\n  - asupersync::net::tcp::TcpStream for connection\n  - asupersync::tls for HTTPS (rustls, same as reqwest used)\n  - Cx cancellation protocol for download abort\n  - asupersync::channel::watch for progress reporting\n  - Budget enforcement: deadline on total download time\n\nREVISED ARCHITECTURE:\n\npub struct ModelDownloader {\n    user_agent: String,\n    connect_timeout: Duration,\n    progress_tx: asupersync::channel::watch::Sender<DownloadProgress>,\n}\n\npub struct DownloadProgress {\n    pub bytes_downloaded: u64,\n    pub total_bytes: Option<u64>,\n    pub speed_bytes_per_sec: f64,\n}\n\nimpl ModelDownloader {\n    /// Download a model file with cancel-correct progress reporting.\n    pub async fn download(\n        &self,\n        cx: &Cx,\n        url: &str,\n        dest: &Path,\n        expected_sha256: &str,\n    ) -> asupersync::Outcome<PathBuf, SearchError> {\n        // Connect with timeout\n        let stream = asupersync::combinator::timeout(\n            |cx| TcpStream::connect(cx, addr),\n            cx.now() + self.connect_timeout,\n        ).await?;\n\n        // TLS handshake (for HTTPS)\n        let tls_stream = asupersync::tls::connect(cx, stream, hostname).await?;\n\n        // Send HTTP/1.1 GET request\n        let response = asupersync::http::h1::request(cx, &tls_stream, request).await?;\n        let total_bytes = response.content_length();\n\n        // Stream response body to temp file with progress\n        let mut temp_file = File::create(dest.with_extension(\"tmp\"))?;\n        let mut hasher = Sha256::new();\n        let mut downloaded: u64 = 0;\n\n        let mut body = response.body_stream();\n        while let Some(chunk) = body.next(cx).await {\n            cx.checkpoint()?;  // Cancel check per chunk\n\n            let chunk = chunk?;\n            temp_file.write_all(&chunk)?;\n            hasher.update(&chunk);\n            downloaded += chunk.len() as u64;\n\n            // Progress reporting via watch channel (latest-value semantics)\n            let _ = self.progress_tx.send(DownloadProgress {\n                bytes_downloaded: downloaded,\n                total_bytes,\n                speed_bytes_per_sec: compute_speed(downloaded, start),\n            });\n        }\n\n        // Verify SHA-256\n        let actual_hash = hex::encode(hasher.finalize());\n        if actual_hash != expected_sha256 {\n            std::fs::remove_file(dest.with_extension(\"tmp\"))?;\n            return Outcome::Err(SearchError::HashMismatch { expected: expected_sha256.into(), actual: actual_hash });\n        }\n\n        // Atomic rename\n        std::fs::rename(dest.with_extension(\"tmp\"), dest)?;\n        Outcome::Ok(dest.to_path_buf())\n    }\n}\n\nCANCEL-CORRECT DOWNLOAD:\n  1. User cancels search / parent region cancelled\n  2. cx.checkpoint() in the chunk loop returns Cancelled\n  3. Temp file is cleaned up (Drop impl or bracket pattern)\n  4. No partial corrupt files left on disk\n\n  // Using bracket for guaranteed cleanup:\n  asupersync::combinator::bracket(\n      |cx| create_temp_file(cx, dest),           // acquire\n      |cx, temp| download_to_file(cx, temp),     // use\n      |cx, temp| cleanup_temp_file(cx, temp),    // release (always runs)\n  ).await\n\nPROGRESS OBSERVATION:\n  - watch channel has \"latest value\" semantics (reader always sees most recent)\n  - No lock contention between download thread and progress display\n  - Replaces AtomicU64 + AtomicBool with structured channel\n\nFEATURE FLAG UPDATE:\n  BEFORE: download = ['dep:reqwest']\n  AFTER:  download = ['asupersync/tls']  (TLS feature for HTTPS downloads)\n  (asupersync itself is already a workspace dep; just need TLS feature for downloads)\n\nCONSENT-GATED DOWNLOAD (preserved from cass pattern):\n  - First-run consent prompt before any network access\n  - Consent stored as sentinel file in model directory\n  - This is orthogonal to asupersync — purely a UX concern\n\nDEPENDENCY CHANGES:\n  - REMOVE: reqwest (and its entire transitive tree including tokio, hyper, etc.)\n  - ADD: asupersync/tls feature (for HTTPS)\n  - This is the change that makes frankensearch ZERO-TOKIO","created_at":"2026-02-13T21:06:11Z"},{"id":232,"issue_id":"bd-3un.11","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.11 (Model Download):\n- Mock HTTP server: download succeeds, progress callbacks fire\n- SHA256 verification: correct hash → atomic rename; wrong hash → temp file deleted\n- Cancellation: cancel mid-download → temp file cleaned up, no partial file remains\n- Resume: partial file exists → download continues from offset (HTTP Range header)\n- Network timeout: server hangs → SearchError::SearchTimeout after deadline\n- Atomic install: crash during rename → no corrupt model file\n- Progress reporting: bytes_downloaded monotonically increases\n- Zero-length file: server returns empty body → error (not corrupt model)","created_at":"2026-02-13T21:47:34Z"}]}
{"id":"bd-3un.12","title":"Implement model registry with bakeoff infrastructure","description":"Implement the model registry that catalogs all supported embedders and rerankers. This is a static registry (compile-time data) enriched with runtime availability checks.\n\npub struct RegisteredEmbedder {\n    pub name: &'static str,           // Short name (e.g., 'minilm')\n    pub id: &'static str,             // Unique ID (e.g., 'minilm-384')\n    pub dimension: usize,\n    pub is_semantic: bool,\n    pub description: &'static str,\n    pub requires_model_files: bool,\n    pub release_date: &'static str,   // ISO 8601\n    pub huggingface_id: &'static str,\n    pub size_bytes: u64,\n    pub is_baseline: bool,            // Baseline for bakeoff comparison\n}\n\npub struct RegisteredReranker {\n    pub name: &'static str,\n    pub id: &'static str,\n    pub description: &'static str,\n    pub requires_model_files: bool,\n    pub release_date: &'static str,\n    pub huggingface_id: &'static str,\n    pub size_bytes: u64,\n    pub is_baseline: bool,\n}\n\nStatic registries (from cross-referencing all 3 codebases):\n\nEMBEDDERS: \n- minilm (384d, semantic, baseline, 2022-08-01)\n- snowflake-arctic-s (384d, semantic, 2025-11-10)\n- nomic-embed (768d, semantic, 2025-11-05)\n- potion-multilingual-128M (256d, semantic/static, 2025+)\n- potion-retrieval-32M (512d, semantic/static, 2025+)\n- hash/fnv1a (384d, non-semantic, always available)\n\nRERANKERS:\n- ms-marco-minilm (baseline)\n- flashrank-nano (~4MB)\n- bge-reranker-v2\n- jina-reranker-turbo\n- mxbai-rerank-xsmall\n\nRuntime registry:\npub struct EmbedderRegistry {\n    data_dir: PathBuf,\n}\n\nimpl EmbedderRegistry {\n    pub fn available(&self) -> Vec<&RegisteredEmbedder>;\n    pub fn get(&self, name: &str) -> Option<&RegisteredEmbedder>;\n    pub fn best_available(&self) -> &RegisteredEmbedder;\n    pub fn bakeoff_eligible(&self) -> Vec<&RegisteredEmbedder>;\n}\n\nBakeoff eligibility cutoff: 2025-11-01 (models released after this date)\n\nReference: cass src/search/embedder_registry.rs, src/search/reranker_registry.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:49:26.791533937Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:34.247707703Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["model-mgmt","phase3","registry"],"dependencies":[{"issue_id":"bd-3un.12","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:49:26.791533937Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.12","depends_on_id":"bd-3un.10","type":"blocks","created_at":"2026-02-13T17:55:12.757907053Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.12","depends_on_id":"bd-3un.7","type":"blocks","created_at":"2026-02-13T17:55:12.592024762Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.12","depends_on_id":"bd-3un.8","type":"blocks","created_at":"2026-02-13T17:55:12.675057419Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":41,"issue_id":"bd-3un.12","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Model Registry & Bakeoff)\n\n## Mathematical Upgrade: From Point Estimates to Bayesian Bakeoff\n\nThe current bakeoff infrastructure compares embedders by average NDCG. This is ad-hoc and doesn't account for variance, sample size, or the multiple comparisons problem.\n\n### 1. Bayesian A/B Testing with Beta Posteriors\n\nFor each embedder pair (A, B), maintain a Beta posterior:\n\n  // For each query in the bakeoff:\n  //   If A's NDCG@10 > B's NDCG@10: A_wins += 1\n  //   If B's NDCG@10 > A's NDCG@10: B_wins += 1\n  //   If tied: both += 0.5\n\n  P(A better than B) = P(Beta(A_wins+1, B_wins+1) > 0.5)\n\nDecision rule:\n  - P(A > B) > 0.95 → declare A the winner (95% Bayesian credibility)\n  - P(A > B) < 0.05 → declare B the winner\n  - Otherwise → need more queries (continue testing)\n\nThis gives a FORMAL stopping criterion — no more arbitrary \"run N queries and take the average.\"\n\n### 2. Multi-Armed Bandit for Model Selection\n\nWhen multiple embedders are available, use Thompson sampling to select the best one adaptively:\n\n  pub struct EmbedderBandit {\n      arms: Vec<(String, Beta)>,  // (embedder_id, Beta posterior)\n  }\n\n  impl EmbedderBandit {\n      pub fn select_embedder(&self) -> &str {\n          // Sample from each Beta posterior, pick highest\n          self.arms.iter()\n              .max_by(|(_, a), (_, b)| a.sample().partial_cmp(&b.sample()).unwrap())\n              .map(|(id, _)| id.as_str())\n              .unwrap()\n      }\n  }\n\nThis automatically explores new models while exploiting the best known one. Provable O(sqrt(T log T)) regret.\n\n### 3. e-Values for Anytime-Valid Testing\n\nUse e-values instead of p-values for the bakeoff. e-values support OPTIONAL STOPPING — you can look at results at any time and make valid decisions:\n\n  e_n = product(likelihood_ratio_i for i in 1..n)\n  If e_n > 1/alpha, reject H0 at level alpha\n\nThis means: you can stop the bakeoff EARLY if one model is clearly better, or CONTINUE if results are ambiguous. Traditional hypothesis testing requires fixed sample sizes.\n\n### 4. FDR Control for Multi-Model Comparisons\n\nWhen comparing K models pairwise (K*(K-1)/2 comparisons), use the e-BH procedure for False Discovery Rate control:\n\n  1. Compute e-values for all pairwise comparisons\n  2. Sort e-values in decreasing order\n  3. Apply BH threshold: reject hypothesis i if e_i > K*(K-1)/(2*i*alpha)\n\nThis controls the expected proportion of false discoveries at level alpha.\n\n### Implementation Priority\n\n1. Beta posterior A/B testing: add to bakeoff report generation\n2. Thompson sampling model selection: add to EmbedderStack\n3. e-values: add to bakeoff infrastructure\n4. FDR control: add when > 4 models are being compared\n","created_at":"2026-02-13T20:33:29Z"},{"id":233,"issue_id":"bd-3un.12","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.12 (Model Registry):\n- Registry lookup: find_embedder(\"potion-128M\") returns correct entry\n- Availability: model files present → is_available() true; missing → false\n- Bakeoff eligibility: filter by date, verify correct models included\n- Unknown model: find_embedder(\"nonexistent\") returns None (not error)\n- Registry is extensible: add_embedder() at runtime works\n- Category classification: each registered model has correct ModelCategory","created_at":"2026-02-13T21:47:34Z"}]}
{"id":"bd-3un.13","title":"Implement vector index binary format and I/O","description":"Implement the binary vector index format for storing and loading embeddings. This is the on-disk format that enables memory-mapped vector search.\n\nWe should unify the formats from all 3 codebases into a single 'FSVI' (FrankenSearch Vector Index) format:\n\nBinary Format (Little-Endian):\n\nHeader (variable length):\n  Offset  Size  Field\n  0       4     magic: 'FSVI' (4 ASCII bytes)\n  4       2     version: u16 (start at 1)\n  6       2     embedder_id_len: u16\n  8       N     embedder_id: UTF-8 bytes\n  8+N     4     dimension: u32\n  12+N    4     quantization: u8 (0=f32, 1=f16)\n  13+N    3     reserved: [u8; 3]\n  16+N    8     record_count: u64\n  24+N    8     vectors_offset: u64 (byte offset to vector slab)\n  32+N    4     header_crc32: u32\n\nRecord Table (fixed-size per record, 16 bytes each):\n  doc_id_hash: u64     (FNV-1a hash of doc_id for fast lookup)\n  doc_id_offset: u32   (offset into string table)\n  doc_id_len: u16      (byte length of doc_id)\n  flags: u16           (reserved for doc_type enum, etc.)\n\nString Table:\n  Concatenated UTF-8 doc_id strings (referenced by offset+len)\n\nVector Slab (32-byte aligned):\n  record_count × dimension × bytes_per_quant (2 for f16, 4 for f32)\n\nDesign decisions:\n- f16 quantization by default (2x memory savings, minimal quality loss)\n- Memory-mapped via memmap2 for zero-copy access\n- 32-byte aligned vector slab for SIMD (AVX2 alignment)\n- CRC32 header checksum for corruption detection\n- Sorted by doc_id_hash for binary search lookup\n\nAPI:\n\npub struct VectorIndex {\n    mmap: Mmap,               // Memory-mapped file\n    metadata: VectorMetadata,\n    record_count: usize,\n    dimension: usize,\n}\n\nimpl VectorIndex {\n    pub fn open(path: &Path) -> SearchResult<Self>;\n    pub fn create(path: &Path, embedder_id: &str, dimension: usize) -> SearchResult<VectorIndexWriter>;\n    pub fn record_count(&self) -> usize;\n    pub fn dimension(&self) -> usize;\n    pub fn embedder_id(&self) -> &str;\n    pub fn vector_at_f16(&self, index: usize) -> &[f16];\n    pub fn vector_at_f32(&self, index: usize) -> Vec<f32>;\n    pub fn doc_id_at(&self, index: usize) -> &str;\n}\n\npub struct VectorIndexWriter {\n    file: BufWriter<File>,\n    dimension: usize,\n    count: u64,\n}\n\nimpl VectorIndexWriter {\n    pub fn write_record(&mut self, doc_id: &str, embedding: &[f32]) -> SearchResult<()>;\n    pub fn finish(self) -> SearchResult<()>;\n}\n\nReference formats:\n- cass: CVVI (src/search/vector_index.rs, 70-byte rows with message metadata)\n- xf: XFVI (src/vector.rs, variable records with doc_type)\n- agent-mail: planned AMVI (simplified from XFVI)\n\nOur format (FSVI) is a clean generalization that doesn't bake in domain-specific fields.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:23.484008880Z","created_by":"ubuntu","updated_at":"2026-02-13T21:46:36.465252952Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["io","phase4","vector-index"],"dependencies":[{"issue_id":"bd-3un.13","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.484008880Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.13","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:17.773296513Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.13","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:17.853723728Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":7,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"VECTOR INDEX FORMAT DESIGN: The FSVI format is a deliberate generalization of the three existing formats:\n- CVVI (cass): 70-byte rows with domain-specific fields (MessageID, AgentID, etc.)\n- XFVI (xf): variable records with doc_type enum (tweet/like/dm/grok)\n- AMVI (agent-mail): simplified from XFVI\n\nFSVI strips all domain-specific fields. The principle: frankensearch stores (doc_id, embedding) pairs. Any domain metadata belongs in the consumer's own storage. This keeps the index format universal.\n\nKey design choice — f16 by default: \n- 384-dim f16: 768 bytes per doc (vs 1536 for f32) = 50% memory savings\n- Quality loss from f16 quantization is < 1% on cosine similarity benchmarks\n- Memory-mapped f16 means the OS page cache holds 2x more vectors\n- For 100K docs × 384 dims: 73MB (f16) vs 147MB (f32)\n\nThe string table design (separate from records) enables fixed-size record entries for binary search lookups while supporting variable-length doc IDs.","created_at":"2026-02-13T17:57:22Z"},{"id":15,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. EMBEDDER REVISION FIELD MISSING: The FSVI header must include an embedder_revision field (variable-length UTF-8 string, like embedder_id). This tracks the model's pinned commit SHA (e.g., \"c9745ed1d9f207416be6d2e6f8de32d1f16199bf\" for MiniLM). Purpose: when a model is updated, the revision changes, and the index must be rebuilt. Without this, stale indices silently return degraded results.\n\nUpdated header layout:\n  8+N     2     embedder_rev_len: u16\n  10+N    M     embedder_revision: UTF-8 bytes\n  (adjust all subsequent offsets by M+2)\n\nReference: cass CVVI header has EmbedderRevision (u16 len + bytes). This was in the original format but was accidentally omitted from the FSVI design.\n\n2. FSYNC ON SAVE: VectorIndexWriter::finish() must call fsync on the file AND fsync the parent directory for write durability. Reference: cass vector_index.rs sync_dir() at line 1538. This prevents data loss on power failure.\n\n3. VECTOR ALIGNMENT: The vector slab must be 32-byte aligned (VECTOR_ALIGN_BYTES = 32) for AVX2 SIMD. Add padding between string table and vector slab as needed.\n","created_at":"2026-02-13T20:24:01Z"},{"id":33,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (FSVI Vector Index Format)\n\n## Profile-Informed I/O and Memory Optimization\n\n### Opportunity Matrix\n\n| Hotspot                    | Impact | Confidence | Effort | Score |\n|----------------------------|--------|------------|--------|-------|\n| 64-byte cache alignment    | 3      | 5          | 1      | 15.0  |\n| madvise(MADV_SEQUENTIAL)   | 4      | 5          | 1      | 20.0  |\n| Huge pages (2MB THP)       | 3      | 4          | 1      | 12.0  |\n| Vectored write (writev)    | 3      | 3          | 2      | 4.5   |\n| fsync strategy             | 2      | 5          | 1      | 10.0  |\n\n### 1. madvise for Sequential Scan (Score: 20.0, MUST DO)\n\nThe vector search does a sequential scan through the entire mmap. Tell the OS:\n\n  // After opening the mmap:\n  #[cfg(unix)]\n  unsafe {\n      libc::madvise(\n          mmap.as_ptr() as *mut libc::c_void,\n          mmap.len(),\n          libc::MADV_SEQUENTIAL,\n      );\n  }\n\nThis enables aggressive readahead — the kernel will prefetch pages before we need them. Measured impact: 2-3x throughput improvement for sequential scans on cold caches.\n\nNOTE: This requires `unsafe`. Since the project forbids unsafe_code, use the memmap2 crate's `advise()` method if available, or document this as a future optimization if memmap2 doesn't expose it safely. Alternative: the `region` crate provides safe madvise wrappers.\n\nSAFE ALTERNATIVE: memmap2::MmapOptions::new().populate() will pre-fault all pages, and memmap2::Mmap::advise(Advice::Sequential) provides a safe wrapper for MADV_SEQUENTIAL in recent versions. Check memmap2 API.\n\n### 2. 64-Byte Cache Line Alignment (Score: 15.0)\n\nCurrent spec says 32-byte alignment for vector slab. Modern CPUs use 64-byte cache lines. Misaligned vector access causes 2 cache line loads per vector start:\n\n  // In FSVI header:\n  vectors_offset: round_up_to(header_size + records_size + strings_size, 64)\n\nChange: \"32-byte aligned\" → \"64-byte aligned\" in the FSVI format spec. This is a one-line change with measurable impact on scan performance.\n\n### 3. Huge Pages for Large Indices (Score: 12.0)\n\nFor indices > 100MB, transparent huge pages (2MB pages) reduce TLB misses by 512x:\n\n  // When creating the mmap:\n  #[cfg(target_os = \"linux\")]\n  {\n      libc::madvise(ptr, len, libc::MADV_HUGEPAGE);\n  }\n\nFor a 100K-doc × 384-dim f16 index (~73MB), this reduces TLB misses from ~18K to ~36. Each TLB miss costs ~100 cycles on modern CPUs.\n\nSAFE ALTERNATIVE: Set the system's transparent_hugepages to \"madvise\" mode and let memmap2 use MAP_HUGETLB flag if available. Or simply document that sysadmins should enable THP for large indices.\n\n### 4. Write Performance: Buffered + Vectored I/O\n\nFor index building, use BufWriter with a large buffer (256KB instead of default 8KB), and batch writes:\n\n  let writer = BufWriter::with_capacity(256 * 1024, file);\n\nFor the final vector slab write, if all vectors are already in memory (they are, during batch embedding), use a single write() call for the entire slab rather than per-record writes. This reduces syscall overhead from N to 1.\n\n### 5. Isomorphism Proof\n\n- Cache alignment: no data change, only padding\n- madvise: kernel hint only, no data change\n- Huge pages: memory mapping only, no data change\n- Buffer size: same bytes written, just fewer syscalls\n- All: sha256(index_file) identical before/after\n","created_at":"2026-02-13T20:29:56Z"},{"id":228,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"REVIEW FIX — FSVI binary format header reconciliation and alignment decision:\n\n1. CANONICAL HEADER LAYOUT (reconciling body + revision, single source of truth):\n\n   Offset  Size  Field                    Description\n   ──────  ────  ─────                    ───────────\n   0       4     magic: [u8; 4]           b\"FSVI\"\n   4       2     version: u16             Format version (1)\n   6       2     dimension: u16           Vector dimension (e.g., 384)\n   8       1     quantization: u8         0=f32, 1=f16, 2=int8, 3=int4 (extensible for bd-qtx)\n   9       1     embedder_id_len: u8      Length of embedder_id string (N)\n   10      N     embedder_id: [u8; N]     UTF-8 embedder identifier\n   10+N    1     embedder_rev_len: u8     Length of embedder_revision string (M)\n   11+N    M     embedder_rev: [u8; M]    UTF-8 embedder revision hash\n   11+N+M  3     reserved: [u8; 3]        Padding to 4-byte boundary (zeros)\n   14+N+M  8     record_count: u64        Number of vectors stored\n   22+N+M  4     header_crc32: u32        CRC32 of all preceding header bytes\n\n   Total header size: 26 + N + M bytes (variable, typically ~60-80 bytes)\n\n   After header:\n   - Record table: record_count entries of { doc_id_offset: u32, doc_id_len: u16 }\n   - String table: concatenated doc_id strings (UTF-8)\n   - Vector slab: record_count × dimension × element_size contiguous vectors\n\n2. ALIGNMENT DECISION: 64-byte (cache line) alignment for the vector slab.\n\n   The vector slab start offset is padded to the next 64-byte boundary after the string table. This ensures:\n   - SIMD loads (f32x8 = 32 bytes) never cross cache line boundaries\n   - mmap access patterns benefit from aligned pages\n   - No unsafe needed — the alignment is in the file format, not pointer arithmetic\n\n   vector_slab_offset = (string_table_end + 63) & !63  // Round up to 64-byte boundary\n\n   Padding bytes between string table and vector slab are filled with zeros.\n\n3. FULL-FILE CHECKSUM: Add an optional footer checksum for the vector slab:\n\n   After the last vector:\n   - slab_crc32: u32 — CRC32 of the entire vector slab\n\n   This is OPTIONAL (checked only when config.verify_slab_integrity == true) because:\n   - CRC32 of a 73MB slab (100K×384×f16) takes ~50ms\n   - For typical usage (trusted local files), header CRC is sufficient\n   - For untrusted files or after crash, enable slab verification\n\n   When bd-3w1.7 (RaptorQ repair) is enabled, the .fec sidecar provides stronger integrity guarantees, making slab CRC redundant.\n\n4. TEST REQUIREMENTS:\n   - Round-trip: write index → read back → all vectors match within quantization tolerance\n   - CRC32 verification: correct file passes, 1-byte corruption in header detected\n   - Slab CRC (when enabled): 1-byte corruption in vector slab detected\n   - Dimension mismatch: open file with wrong expected dimension → clear error\n   - Embedder ID mismatch: open file with wrong embedder → clear warning (not error, for migration)\n   - Empty index (0 records): writes and reads correctly\n   - Large index (100K records): round-trip without data loss\n   - f16 quantization fidelity: max |f32_original - f16_roundtrip| < 0.001 for typical embedding values\n   - 64-byte alignment: verify vector_slab_offset % 64 == 0\n   - Memory-mapped access: mmap the file, read vectors, verify correctness","created_at":"2026-02-13T21:46:36Z"}]}
{"id":"bd-3un.14","title":"Implement SIMD-accelerated dot product (f16/f32)","description":"Implement SIMD-accelerated dot product for cosine similarity computation. This is the performance-critical inner loop of vector search — called once per stored vector per query.\n\nPrimary implementation using wide crate (portable SIMD):\n\npub fn dot_product_f16_f32(stored: &[f16], query: &[f32]) -> f32 {\n    use wide::f32x8;  // 8-wide SIMD, portable across x86/ARM\n    \n    let chunks = stored.len() / 8;\n    let mut sum = f32x8::ZERO;\n    \n    for i in 0..chunks {\n        let base = i * 8;\n        // Convert 8 f16 values to f32\n        let s_f32: [f32; 8] = [\n            f32::from(stored[base]),   f32::from(stored[base+1]),\n            f32::from(stored[base+2]), f32::from(stored[base+3]),\n            f32::from(stored[base+4]), f32::from(stored[base+5]),\n            f32::from(stored[base+6]), f32::from(stored[base+7]),\n        ];\n        let q_arr: [f32; 8] = query[base..base+8].try_into().unwrap();\n        sum += f32x8::from(s_f32) * f32x8::from(q_arr);\n    }\n    \n    let mut result = sum.reduce_add();  // Horizontal sum\n    \n    // Scalar remainder\n    for i in (chunks * 8)..stored.len() {\n        result += f32::from(stored[i]) * query[i];\n    }\n    result\n}\n\nAlso provide:\n- dot_product_f32_f32(a: &[f32], b: &[f32]) -> f32 (for non-quantized indices)\n- cosine_similarity_f16(a: &[f16], b: &[f32]) -> f32 (wrapper assuming L2-normalized inputs)\n\nPerformance targets (from xf benchmarks):\n- 256-dim f16 dot product: < 1μs\n- 384-dim f16 dot product: < 2μs\n- 10K vectors × 384-dim search: < 15ms\n\nDependencies:\n- wide = '0.7' (portable SIMD)\n- half = '2.4' (f16 type)\n\nDesign note: wide::f32x8 is portable across x86 (SSE2/AVX2) and ARM (NEON). No unsafe code needed.\n\nFile: frankensearch-index/src/simd.rs\n\nReference implementations:\n- cass: src/search/two_tier_search.rs lines 785-832 (dot_product_f16)\n- xf: src/embedder.rs (dot_product_simd)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 638-692","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:23.557226468Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:34.347415499Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["performance","phase4","simd","vector-index"],"dependencies":[{"issue_id":"bd-3un.14","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.557226468Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.14","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T17:55:17.936597738Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":3,"issue_id":"bd-3un.14","author":"Dicklesworthstone","text":"PERFORMANCE: This is the hottest inner loop in the entire search pipeline. The SIMD dot product is called once per stored vector per query. For a 10K-document index with 384-dim embeddings, that's 10,000 dot products of 384 floats each.\n\nwide::f32x8 is the right choice because:\n1. Portable: works on x86 (SSE2/AVX2) and ARM (NEON) without #[cfg] branches\n2. Safe: no unsafe code needed (wide handles the intrinsics)\n3. Fast: 8-wide parallelism reduces loop iterations by 8x\n4. Simple: the API is just multiply + horizontal sum\n\nThe f16→f32 conversion before SIMD multiply is a necessary cost. f16 SIMD isn't widely supported on CPU. The 2 bytes per dimension (vs 4 for f32) saves 50% memory, which matters when the entire index is memory-mapped.\n\nBenchmark baseline: 384-dim f16×f32 dot product should be < 2μs.","created_at":"2026-02-13T17:56:21Z"},{"id":28,"issue_id":"bd-3un.14","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (SIMD Dot Product & Vector Search)\n\n## Profiling-Informed Optimization Opportunities\n\nThe SIMD dot product is called once per stored vector per query — the single hottest loop in the entire system. Every microsecond saved here multiplies across thousands of vectors.\n\n### Opportunity Matrix\n\n| Hotspot               | Impact | Confidence | Effort | Score |\n|-----------------------|--------|------------|--------|-------|\n| f16→f32 batch convert | 4      | 5          | 2      | 10.0  |\n| Cache-line prefetch   | 4      | 4          | 2      | 8.0   |\n| SoA vector layout     | 5      | 4          | 3      | 6.7   |\n| ILP interleaving      | 3      | 4          | 2      | 6.0   |\n| int8 quantization     | 4      | 3          | 4      | 3.0   |\n| AVX-512 path          | 3      | 3          | 3      | 3.0   |\n\n### 1. Batch f16→f32 Conversion (Score: 10.0, MUST DO)\n\nThe current design converts f16 values individually inside the dot product loop. The `half` crate provides `half::slice::convert_to_f32_slice()` which uses F16C SIMD instructions (vcvtph2ps on x86) to convert 8 f16 values simultaneously. This should be done OUTSIDE the dot product, converting the entire stored vector in one batch call, then computing the dot product on the resulting f32 slice.\n\n  // BEFORE: Convert inside loop (scalar f16→f32 per element)\n  for i in 0..chunks {\n      let s_f32: [f32; 8] = [f32::from(stored[base]), ...];  // 8 scalar conversions\n  }\n\n  // AFTER: Batch convert first, then pure f32×f32 SIMD\n  let mut buf = [0f32; 384];  // Stack buffer, reuse across queries\n  half::slice::convert_to_f32_slice(stored, &mut buf);\n  dot_product_f32_f32(&buf, query)\n\nExpected speedup: 2-3x for the conversion step (which is ~40% of total dot product time).\n\n### 2. Cache-Line Prefetching (Score: 8.0, MUST DO)\n\nDuring the linear scan of 10K vectors, each vector access is a cache miss (~100ns penalty on L2 miss). Prefetch the NEXT vector while computing the current one:\n\n  for i in 0..record_count {\n      // Prefetch vector i+4 while computing vector i\n      if i + 4 < record_count {\n          std::arch::x86_64::_mm_prefetch(\n              index.vector_ptr(i + 4) as *const i8,\n              std::arch::x86_64::_MM_HINT_T0,\n          );\n      }\n      let score = dot_product(index.vector_at(i), query);\n      ...\n  }\n\nNOTE: This requires `unsafe` for the raw prefetch intrinsic. Since the project forbids unsafe_code, an alternative is to use the `prefetch` crate which provides safe wrappers, or restructure the scan to use iterator patterns that encourage hardware prefetching (sequential access patterns with no branching).\n\nSAFE ALTERNATIVE: Ensure vectors are stored contiguously in memory (they already are in the mmap) and access them strictly sequentially. Modern CPUs have hardware stream prefetchers that detect sequential access patterns and prefetch automatically. The key is to NEVER skip vectors or access them out of order — even filtered vectors should be loaded and immediately discarded rather than skipped via random access.\n\n### 3. Structure-of-Arrays Vector Layout (Score: 6.7)\n\nThe current FSVI format stores vectors as Array-of-Structures (each record is [doc_id_hash, offset, len, flags, vector_data]). For pure scanning workloads, a Structure-of-Arrays layout is superior:\n\n  // SoA layout in FSVI v2:\n  [All doc_id_hashes]  // Compact for binary search\n  [All vectors]        // Contiguous for linear scan + SIMD\n  [All doc_id strings] // Only touched for top-k results\n\nThis gives perfect cache locality during the scan phase — the CPU prefetcher sees a contiguous f16 slab with no interleaved metadata.\n\nHOWEVER: The current FSVI format already separates the vector slab from records. Verify the vector slab is truly contiguous (no padding between vectors) and 64-byte aligned (cache line boundary). The format spec says 32-byte aligned — consider upgrading to 64-byte.\n\n### 4. ILP: Compute 4 Dot Products Simultaneously (Score: 6.0)\n\nModern CPUs have multiple execution ports. Instead of computing one dot product at a time, process 4 vectors simultaneously to maximize instruction-level parallelism:\n\n  // Process 4 vectors per iteration\n  for chunk in vectors.chunks(4) {\n      let mut sums = [f32x8::ZERO; 4];\n      for dim_block in 0..dim/8 {\n          let q = f32x8::from(&query[dim_block*8..]);\n          sums[0] += f32x8::from(&chunk[0][dim_block*8..]) * q;\n          sums[1] += f32x8::from(&chunk[1][dim_block*8..]) * q;\n          sums[2] += f32x8::from(&chunk[2][dim_block*8..]) * q;\n          sums[3] += f32x8::from(&chunk[3][dim_block*8..]) * q;\n      }\n      // Update top-k heap with all 4 scores\n  }\n\nThis keeps the SIMD execution units fully saturated. Expected improvement: 1.5-2x on modern CPUs with multiple FMA ports.\n\n### 5. Buffer Reuse Pattern (CRITICAL)\n\nAllocate the f32 conversion buffer ONCE and reuse across all dot products in a search:\n\n  pub fn search_top_k(index: &VectorIndex, query: &[f32], k: usize) -> Vec<VectorHit> {\n      let mut buf = vec![0f32; index.dimension()];  // Allocate ONCE\n      for i in 0..index.record_count() {\n          half::slice::convert_to_f32_slice(index.vector_at_f16(i), &mut buf);\n          let score = dot_product_f32_f32(&buf, query);\n          // ... heap update\n      }\n  }\n\nThis eliminates record_count × dimension allocations. For 10K × 384: saves 15M f32 writes to fresh memory.\n\n### Isomorphism Proof Template\n\nAll optimizations preserve:\n- Ordering: Same scores → same rankings (f32 arithmetic identical)\n- Tie-breaking: doc_id ordering preserved for equal scores\n- Floating-point: f16→f32 conversion path identical (same precision)\n- Golden outputs: sha256 of top-k results unchanged\n","created_at":"2026-02-13T20:29:51Z"},{"id":234,"issue_id":"bd-3un.14","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.14 (SIMD Dot Product):\n- Correctness: SIMD result matches scalar reference within f32 epsilon\n- f16 precision bounds: max |dot_f32(a,b) - dot_f16(a,b)| < 0.01 for unit vectors\n- SIMD vs scalar equivalence: same inputs produce same output\n- Zero-vector: dot(zero, anything) == 0.0\n- NaN handling: NaN in input → NaN in output (not panic or UB)\n- Dimension mismatch: different-length vectors → error (not UB)\n- Batch f16→f32 conversion: verify against half::f16::to_f32 scalar reference\n- Performance regression: benchmark dot product of 384-dim vectors, assert < 2 microseconds\n- Edge case: dimension not divisible by 8 → correct handling of remainder elements","created_at":"2026-02-13T21:47:34Z"}]}
{"id":"bd-3un.15","title":"Implement brute-force top-k vector search","description":"Implement brute-force exact nearest neighbor search over the vector index. Uses SIMD dot product + binary heap for efficient top-k retrieval.\n\npub fn search_top_k(\n    index: &VectorIndex,\n    query: &[f32],     // L2-normalized query embedding\n    k: usize,\n    filter: Option<&dyn Fn(usize) -> bool>,  // Optional per-record filter\n) -> Vec<VectorHit> {\n    // Use BinaryHeap<Reverse<VectorHit>> for min-heap (tracks worst of top-k)\n    // For each record in index:\n    //   1. Optional filter check (skip if filtered)\n    //   2. Compute dot_product_f16_f32(stored, query)\n    //   3. If score > heap.peek() or heap.len() < k: push to heap\n    // Return sorted Vec<VectorHit> (descending by score)\n}\n\nOptimizations:\n- Two-phase approach (from xf src/vector.rs): Phase 1 stores only indices+scores (no String allocs), Phase 2 extracts doc_ids for top-k only\n- Rayon parallelism for large indices (threshold: 10,000 records)\n  - Split into chunks of 1024, each chunk produces local top-k\n  - Merge chunk results into global top-k\n- Early termination impossible for cosine similarity (no bounds), so we rely on SIMD speed\n\nConstants:\n- PARALLEL_THRESHOLD: 10_000\n- PARALLEL_CHUNK_SIZE: 1_024\n- These match cass's constants from src/search/vector_index.rs\n\nPerformance targets:\n- 1K vectors: < 1ms\n- 10K vectors: < 15ms\n- 100K vectors: < 150ms (with rayon parallelism)\n\nFile: frankensearch-index/src/search.rs\n\nReference: cass src/search/vector_index.rs, xf src/vector.rs (search_top_k)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:23.629532738Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:34.450162244Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase4","search","vector-index"],"dependencies":[{"issue_id":"bd-3un.15","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.629532738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.15","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T17:55:18.017639224Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.15","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T17:55:18.118248211Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":20,"issue_id":"bd-3un.15","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. NaN-SAFE ORDERING: The BinaryHeap-based top-k search MUST use total_cmp() for float comparison, not partial_cmp(). From cass vector_index.rs (ScoredEntry, lines 459-479):\n\nstruct ScoredEntry { score: f32, index: usize }\nimpl Ord for ScoredEntry {\n    fn cmp(&self, other: &Self) -> Ordering {\n        self.score.total_cmp(&other.score)\n            .then(self.index.cmp(&other.index))  // index as tiebreaker\n    }\n}\n\nWithout total_cmp(), NaN values cause panics in BinaryHeap. Using total_cmp + index tiebreaker ensures deterministic results.\n\n2. TWO-PHASE SEARCH (from xf vector.rs): The search should use a two-phase approach:\n   Phase 1: Collect (index, score) pairs only -- NO String allocations\n   Phase 2: Look up doc_ids only for the final top-k winners\n   This avoids N string allocations for N vectors, only doing K allocations for the K results.\n\n3. PARALLEL SEARCH ENV VAR: Add FRANKENSEARCH_PARALLEL_SEARCH env var (default: true) to let users disable parallel search for debugging. Reference: cass CASS_PARALLEL_SEARCH.\n","created_at":"2026-02-13T20:25:17Z"},{"id":31,"issue_id":"bd-3un.15","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (Top-K Vector Search)\n\n## Profile-Informed Optimization for search_top_k\n\n### Opportunity Matrix\n\n| Hotspot                  | Impact | Confidence | Effort | Score |\n|--------------------------|--------|------------|--------|-------|\n| Two-phase alloc strategy | 5      | 5          | 1      | 25.0  |\n| Heap branch elimination  | 3      | 4          | 2      | 6.0   |\n| Rayon chunk merge opt    | 4      | 4          | 3      | 5.3   |\n| Early abandonment        | 3      | 3          | 2      | 4.5   |\n| Bloom pre-filter         | 3      | 3          | 3      | 3.0   |\n\n### 1. Two-Phase Allocation Strategy (Score: 25.0, CRITICAL)\n\nThe current design is described correctly in bd-3un.15: \"Phase 1 stores only indices+scores (no String allocs), Phase 2 extracts doc_ids for top-k only.\" This is the single most important optimization. ENSURE it's implemented:\n\n  // Phase 1: Score-only scan (ZERO allocations in hot loop)\n  struct ScoreHit { index: u32, score: f32 }  // 8 bytes, fits in register\n  let mut heap: BinaryHeap<Reverse<ScoreHit>> = BinaryHeap::with_capacity(k + 1);\n\n  for i in 0..record_count {\n      let score = dot_product(...);\n      if heap.len() < k || score > heap.peek().unwrap().0.score {\n          heap.push(Reverse(ScoreHit { index: i as u32, score }));\n          if heap.len() > k { heap.pop(); }\n      }\n  }\n\n  // Phase 2: String alloc only for top-k (typically 10-20 items)\n  heap.into_sorted_vec().iter().map(|h| VectorHit {\n      index: h.0.index as usize,\n      score: h.0.score,\n      doc_id: index.doc_id_at(h.0.index as usize).to_string(),\n  }).collect()\n\nThis avoids 10,000 String allocations (the doc_id lookup), saving ~200μs for 10K vectors.\n\n### 2. Heap Guard Pattern (Score: 6.0)\n\nAfter the heap is full (len == k), most candidates will be worse than the worst in the heap. Add a \"guard\" score to skip the heap comparison entirely:\n\n  let mut min_score = f32::NEG_INFINITY;\n  for i in 0..record_count {\n      let score = dot_product(...);\n      if score > min_score {  // Branch predicted as NOT TAKEN (99%+ of the time)\n          heap.push(Reverse(ScoreHit { index: i as u32, score }));\n          if heap.len() > k {\n              heap.pop();\n              min_score = heap.peek().unwrap().0.score;\n          }\n      }\n  }\n\nThe key insight: after ~2k vectors, the guard score stabilizes and ~99% of candidates are rejected by a single float comparison (< 1ns). The heap push/pop (~50ns) is almost never reached.\n\n### 3. Rayon Parallel Merge Optimization (Score: 5.3)\n\nFor > 10K vectors with rayon parallelism, the merge of per-chunk heaps matters:\n\n  // Each chunk produces a local top-k heap\n  // Merge: pour all heaps into one, re-heapify to global top-k\n\n  // BAD: Merge one-by-one\n  for chunk_heap in chunk_heaps { global.extend(chunk_heap); }\n\n  // GOOD: Tournament merge (log₂(num_chunks) rounds)\n  while heaps.len() > 1 {\n      heaps = heaps.chunks(2).map(|pair| merge_heaps(pair[0], pair.get(1))).collect();\n  }\n\nAlso: set PARALLEL_CHUNK_SIZE to match L2 cache size / vector_bytes:\n  - 384-dim f16 = 768 bytes per vector\n  - 8MB L2 cache → ~10K vectors per chunk (matches current default)\n  - Adjust at runtime: chunk_size = l2_cache_bytes / (dimension * 2)\n\n### 4. Isomorphism Proof\n\n- Ordering: top-k by descending score, ties broken by index (deterministic)\n- Two-phase: Phase 2 produces identical doc_ids as single-phase\n- Parallel: global top-k identical to sequential (heap is deterministic for same inputs)\n- Guard pattern: only changes branch prediction, not comparison result\n","created_at":"2026-02-13T20:29:54Z"},{"id":152,"issue_id":"bd-3un.15","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — LIGHT REVISION (rayon RETAINED for data parallelism):\n\nRayon is RETAINED for brute-force top-k vector search. This is the correct decision because:\n1. Vector dot products are CPU-bound, embarrassingly parallel data\n2. Rayon's work-stealing scheduler is optimal for this pattern\n3. asupersync handles task/structured concurrency; rayon handles data parallelism\n4. These compose: an asupersync task can use rayon internally for CPU-bound work\n\nNo changes to the core algorithm. The only addition:\n\nADDITION: When called from an asupersync context, the caller should use cx.checkpoint() BEFORE and AFTER the rayon parallel search to respect cancellation boundaries:\n\n  pub async fn search_top_k(cx: &Cx, index: &VectorIndex, query: &[f32], k: usize) -> asupersync::Result<Vec<VectorHit>> {\n      cx.checkpoint()?;  // Cancel check before CPU-bound work\n      let results = rayon_search(index, query, k);  // Rayon data parallelism (synchronous)\n      cx.checkpoint()?;  // Cancel check after CPU-bound work\n      Ok(results)\n  }\n\nNOTE: rayon parallel_iter runs synchronously from the caller's perspective — it just uses multiple threads internally. This means the asupersync task is \"blocked\" during the rayon work, but that's fine because:\n1. The rayon work completes in <15ms for 10K vectors\n2. cx.checkpoint() on either side provides cancel boundaries\n3. For very large indices (100K+), consider splitting into asupersync tasks that each use rayon on a chunk","created_at":"2026-02-13T21:06:19Z"},{"id":235,"issue_id":"bd-3un.15","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.15 (Top-k Search):\n- Correctness: top-k matches linear scan reference on small dataset\n- Ordering: results sorted by score descending\n- k > record_count: returns all records (no error)\n- k == 0: returns empty vec (not error)\n- NaN scores: handled by total_cmp (NaN sorts last, not panic)\n- Empty index: returns empty vec\n- Filter function: only matching records returned; count <= k\n- Parallel vs sequential: same inputs → same outputs (deterministic)\n- Two-phase allocation: Phase 1 stores (index, score); Phase 2 resolves doc_ids\n- Deterministic tie-breaking: equal scores → stable order (by index)\n- 10K vectors: search completes in < 15ms (performance assertion)","created_at":"2026-02-13T21:47:34Z"}]}
{"id":"bd-3un.16","title":"Implement optional HNSW approximate nearest neighbor index","description":"Implement optional HNSW (Hierarchical Navigable Small World) approximate nearest neighbor index for large vector collections (>50K documents). This is an optimization for scale — brute-force is preferred for smaller collections.\n\nBased on cass src/search/ann_index.rs:\n\nBinary format (CHSW magic, persisted to disk):\n- M (max connections per node): 16\n- ef_construction (build-time accuracy): 200\n- ef_search (query-time accuracy): 100 (tunable)\n- MAX_LAYER: 16\n- Distance metric: DistDot (dot product for cosine)\n\npub struct HnswIndex {\n    hnsw: Hnsw<f32, DistDot>,\n    doc_ids: Vec<String>,\n    dimension: usize,\n}\n\nimpl HnswIndex {\n    pub fn build_from_vector_index(vi: &VectorIndex, config: HnswConfig) -> Self;\n    pub fn load(path: &Path) -> SearchResult<Self>;\n    pub fn save(&self, path: &Path) -> SearchResult<()>;\n    pub fn knn_search(&self, query: &[f32], k: usize, ef: usize) -> Vec<VectorHit>;\n}\n\npub struct AnnSearchStats {\n    pub index_size: usize,\n    pub dimension: usize,\n    pub ef_search: usize,\n    pub k_requested: usize,\n    pub k_returned: usize,\n    pub search_time_us: u64,\n    pub is_approximate: bool,\n    pub estimated_recall: f64,  // min(1.0, 0.9 + 0.1 * log2(ef / k))\n}\n\nDependencies: hnsw_rs crate (behind 'ann' feature flag)\n\nPriority P3 because brute-force + SIMD is sufficient for typical use cases (<50K docs). HNSW adds complexity (build time, graph persistence, recall estimation) and is only needed at scale.\n\nReference: cass src/search/ann_index.rs (200+ lines)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T17:50:23.697487665Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:20.478226721Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["ann","optional","phase4","vector-index"],"dependencies":[{"issue_id":"bd-3un.16","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.697487665Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.16","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T21:56:20.478175536Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.16","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T17:55:18.199925117Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.16","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T17:55:18.279814044Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":252,"issue_id":"bd-3un.16","author":"Dicklesworthstone","text":"IMPLEMENTATION GUIDANCE: When to use HNSW vs brute-force\n\nDecision criteria:\n  - < 50K documents: brute-force (always exact, simpler, <150ms with rayon)\n  - 50K-500K documents: HNSW recommended (10-50x faster, ~95% recall)\n  - > 500K documents: HNSW required (brute-force exceeds latency budget)\n\nThe TwoTierSearcher should auto-select based on index size:\n  if index.record_count() > hnsw_threshold { use HNSW } else { use brute-force }\n  hnsw_threshold configurable via TwoTierConfig, default 50_000\n\nHNSW build time:\n  - 50K docs x 384d: ~5 seconds (one-time cost during index build)\n  - 100K docs x 384d: ~12 seconds\n  - Build happens in IndexBuilder, not at search time\n\nHNSW persistence:\n  - Save to {data_dir}/vector.fast.hnsw and vector.quality.hnsw\n  - Load is memory-mapped, instant startup\n  - Rebuild if VectorIndex changes (detected by record count or hash mismatch)\n\nTesting: Compare HNSW vs brute-force results for recall measurement.\nFor top-10 search on 50K docs with ef_search=100: expect recall >= 0.95.\nInclude in benchmarks (bd-3un.33) for regression tracking.\n\nNote: This is P3 and feature-gated behind 'ann'. The core frankensearch\nexperience works perfectly without it. Add when scale demands it.\n","created_at":"2026-02-13T21:54:12Z"},{"id":262,"issue_id":"bd-3un.16","author":"Dicklesworthstone","text":"REVIEW FIX — HNSW dependency, recall formula, and tests:\n\n1. MISSING DEPENDENCY: Add bd-3un.13 (FSVI binary format). HNSW uses the same vector storage layer — it reads f16 vectors from the FSVI slab. Without this dep, HNSW would need to reimplement vector I/O.\n\n2. RECALL FORMULA CORRECTION: The body claims \"recall = |HNSW_results ∩ exact_results| / k\". This is correct as stated (standard recall@k definition). However, it should note that this measures recall AGAINST BRUTE FORCE, not against ground truth relevance. The metric answers \"how many of the true top-k nearest neighbors does HNSW find?\" — it does NOT measure search quality directly.\n\n3. ef_construction vs ef_search: The body sets ef_construction=200, ef_search=100. These are reasonable defaults but should be configurable via TwoTierConfig. Add fields:\n   hnsw_ef_construction: usize (default 200, only used at index build time)\n   hnsw_ef_search: usize (default 100, used at query time)\n   hnsw_m: usize (default 16, max connections per node)\n\n4. TEST REQUIREMENTS:\n   - Recall test: build HNSW on 1000 random 384-dim vectors, verify recall@10 >= 0.95 vs brute force\n   - Determinism: same data + same seed → identical graph structure\n   - Empty index: search returns empty vec, no panic\n   - Single element: search returns that element\n   - ef_search impact: higher ef_search → higher recall (monotonic)\n   - Serialization round-trip: build, save, load, search → same results\n   - Distance metric consistency: HNSW distances match brute-force dot product scores","created_at":"2026-02-13T21:55:45Z"}]}
{"id":"bd-3un.17","title":"Implement Tantivy schema and document indexing","description":"Implement the Tantivy full-text search schema and document indexing in frankensearch-lexical. This provides BM25 keyword matching that complements semantic search in the hybrid pipeline.\n\nSchema design (generalized from all 3 codebases):\n\npub fn build_schema() -> Schema {\n    let mut builder = Schema::builder();\n    \n    // Required fields (all documents must have these)\n    builder.add_text_field('id', STRING | STORED);           // Unique document ID\n    builder.add_text_field('content', TEXT | STORED);         // Main searchable text\n    builder.add_i64_field('created_at', INDEXED | STORED | FAST);  // Timestamp for sorting\n    \n    // Optional metadata fields\n    builder.add_text_field('title', TEXT | STORED);           // Optional title\n    builder.add_text_field('doc_type', STRING | STORED);     // Document type tag\n    builder.add_text_field('source', STRING | STORED);       // Source identifier\n    builder.add_text_field('metadata', TEXT | STORED);       // JSON metadata blob\n    \n    // Prefix search support (edge n-grams)\n    builder.add_text_field('content_prefix', TEXT);           // Edge n-gram tokenized\n    builder.add_text_field('title_prefix', TEXT);\n    \n    builder.build()\n}\n\nCustom tokenizer (from cass src/search/tantivy.rs):\n- Hyphen-aware: prevents splitting 'POL-358' into 'POL' and '358'\n- Edge n-gram: generates prefixes for typeahead (configurable 2..=15 chars)\n- Lowercase normalization\n\npub struct LexicalIndex {\n    index: tantivy::Index,\n    schema: Schema,\n    reader: IndexReader,\n    writer: Option<IndexWriter>,\n}\n\nimpl LexicalIndex {\n    pub fn create(path: &Path) -> SearchResult<Self>;\n    pub fn open(path: &Path) -> SearchResult<Self>;\n    pub fn add_document(&mut self, doc: &IndexableDocument) -> SearchResult<()>;\n    pub fn add_documents_batch(&mut self, docs: &[IndexableDocument]) -> SearchResult<()>;\n    pub fn commit(&mut self) -> SearchResult<()>;\n    pub fn search(&self, query: &str, limit: usize) -> SearchResult<Vec<LexicalHit>>;\n}\n\npub struct IndexableDocument {\n    pub id: String,\n    pub content: String,\n    pub title: Option<String>,\n    pub created_at: i64,\n    pub doc_type: Option<String>,\n    pub source: Option<String>,\n    pub metadata: Option<serde_json::Value>,\n}\n\nSchema versioning: hash-based (e.g., 'tantivy-schema-v1-frankensearch') stored in a sentinel file. If hash changes, index needs rebuild.\n\nMerge strategy (from cass):\n- Merge cooldown: 5 minutes between merges\n- Threshold: merge when >= 4 segments\n\nFeature gating: Behind 'lexical' feature flag\nDependencies: tantivy = '0.22'\n\nReference:\n- cass: src/search/tantivy.rs (schema v6, custom tokenizer, edge n-grams)\n- xf: src/search.rs (simpler schema, separate prefix fields)\n- agent-mail: search-v3 architecture (custom tokenizer)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:54.185243489Z","created_by":"ubuntu","updated_at":"2026-02-13T21:55:52.321851050Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["lexical","phase5","tantivy"],"dependencies":[{"issue_id":"bd-3un.17","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:54.185243489Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.17","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:23.757391712Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.17","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:23.837343877Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":21,"issue_id":"bd-3un.17","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. TOKEN LENGTH FILTER: The custom tokenizer MUST include RemoveLongFilter::limit(256) to prevent pathologically long tokens from causing performance issues. From cass tantivy.rs line 482.\n\n2. WRITER BUFFER SIZE: Use 50MB writer buffer: writer(50_000_000). This matches cass tantivy.rs line 137 and is tuned for typical workloads.\n\n3. CORRUPTED INDEX RECOVERY: If an existing index fails to open despite matching schema hash, fall back to clean rebuild with a WARN log. From cass tantivy.rs lines 110-121. This prevents permanent broken states.\n\n4. PREVIEW FIELD: Add a 'preview' stored field with the first 400 chars of content. This enables result snippets without re-reading the full document from an external store.\n\n5. EDGE N-GRAM PERFORMANCE: Use ArrayVec (stack-allocated) for n-gram index collection to avoid heap allocation during bulk indexing. From cass tantivy.rs MAX_NGRAM_INDICES=21 with ArrayVec.\n\n6. SCHEMA VERSIONING: Store schema version as a string hash (e.g., \"tantivy-schema-v1-frankensearch\") in a sentinel file. On mismatch, wipe and rebuild. From cass \"tantivy-schema-v6-long-tokens\".\n","created_at":"2026-02-13T20:25:32Z"},{"id":263,"issue_id":"bd-3un.17","author":"Dicklesworthstone","text":"REVIEW FIX — metadata field type, struct naming, and LexicalIndex trait:\n\n1. METADATA FIELD TYPE: The body defines metadata as `STORED | TEXT` but metadata is structured key-value data (JSON), not free text for search. It should be `STORED` only (not indexed for full-text search). If users want to search metadata fields, they should define explicit named fields, not dump JSON into a TEXT field.\n\n   RESOLUTION: metadata field = STORED only. For searchable metadata, add a separate `tags` field (TEXT | STORED) for comma-separated searchable tags.\n\n2. STRUCT NAMING: The body defines `struct LexicalIndex` as a concrete implementation. But if we extract a `LexicalIndex` trait to frankensearch-core (as bd-3un.18's revision suggests), the concrete struct needs a different name. \n\n   RESOLUTION: The trait in core is `LexicalSearch` (the capability). The concrete Tantivy implementation is `TantivyIndex` (the implementation). This follows the Embedder/HashEmbedder naming pattern.\n\n3. DOCUMENT STRUCT: The body defines a local Document struct. This should use IndexableDocument from bd-3un.5 (core types) to avoid duplication:\n   TantivyIndex::add_document(cx: &Cx, doc: &IndexableDocument) -> Result<(), SearchError>\n\n4. TEST REQUIREMENTS:\n   - Schema creation: verify all fields present with correct types\n   - Document indexing round-trip: add document, commit, search, verify all fields returned\n   - Metadata storage: store JSON metadata, retrieve it intact\n   - Tags field: add tags, search by tag, verify results\n   - Empty index search: returns empty results, no panic\n   - Unicode text indexing: NFC-normalized text indexes and searches correctly\n   - Concurrent indexing: multiple documents added in parallel (via asupersync) don't corrupt index\n   - Large document: 100KB text indexes without error","created_at":"2026-02-13T21:55:52Z"}]}
{"id":"bd-3un.18","title":"Implement Tantivy query parsing and search execution","description":"Implement query parsing and search execution for the Tantivy lexical index. This handles converting user queries into Tantivy query objects and executing them.\n\nQuery types to support:\n1. Simple term search: 'authentication' → term query on content field\n2. Phrase search: '\"error handling\"' → phrase query (exact sequence)\n3. Boolean: 'rust AND async' → BooleanQuery with AND/OR/NOT\n4. Prefix/wildcard: 'auth*' → prefix query on content_prefix field\n5. Filtered: doc_type:tweet → term query on doc_type field\n6. Date range: created_at > 2025-01-01 → range query on created_at\n\npub struct LexicalQuery {\n    pub text: String,\n    pub fields: Vec<String>,         // Which fields to search (default: content)\n    pub doc_types: Option<Vec<String>>,  // Filter by doc_type\n    pub date_range: Option<(Option<i64>, Option<i64>)>,  // (start, end) timestamps\n    pub limit: usize,\n    pub offset: usize,\n}\n\npub struct LexicalHit {\n    pub doc_id: String,\n    pub score: f32,         // BM25 score\n    pub rank: usize,        // 0-based rank\n    pub highlights: Vec<String>,  // Matched snippets with highlighting\n    pub doc: Option<serde_json::Value>,  // Retrieved stored fields\n}\n\nQuery explanation (for debugging):\npub enum QueryExplanation {\n    Simple,\n    Phrase,\n    Boolean,\n    Wildcard,\n    Filtered,\n    Empty,\n}\n\nSnippet generation:\n- Use Tantivy's built-in snippet generator\n- Configurable max snippet length (default: 200 chars)\n- HTML highlighting tags (configurable)\n\nReference:\n- cass: src/search/tantivy.rs (search method, query builder)\n- xf: src/search.rs (BM25 ranking with phrase/prefix)\n- agent-mail: crates/mcp-agent-mail-search-core/src/lexical_parser.rs (700+ lines), lexical_response.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:54.262211413Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:00.251558050Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["lexical","phase5","query","tantivy"],"dependencies":[{"issue_id":"bd-3un.18","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:54.262211413Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.18","depends_on_id":"bd-3un.17","type":"blocks","created_at":"2026-02-13T17:55:23.920801880Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":48,"issue_id":"bd-3un.18","author":"Dicklesworthstone","text":"REVISION: Tantivy Query Parsing Hardening\n\n1. Query Complexity Limits:\n   - Max boolean clauses: 64 (prevent OOM from deeply nested queries)\n   - Max query length: 10,000 chars (truncate with WARN log)\n   - Max wildcard expansions: 1000 terms (Tantivy default, document explicitly)\n   - Timeout: 500ms per search execution (configurable via TwoTierConfig)\n   - If limits exceeded: return SearchError::QueryError with human-friendly message\n\n2. Integration with Query Classification (bd-3un.43):\n   - QueryClass informs search strategy BEFORE Tantivy parsing\n   - Identifier queries: use exact match on 'id' field first, fall back to content\n   - ShortKeyword queries: boost title field 2x\n   - NaturalLanguage queries: standard BM25 across content + title\n   - Empty queries: return empty results immediately (no Tantivy round-trip)\n\n3. Error Messages:\n   - Parse errors: return the problematic token position and suggestion\n   - Example: \"Unmatched quote at position 15. Did you mean to search for: error handling\"\n   - Tantivy QueryParserError mapped to SearchError::QueryError with context\n   - Log at DEBUG: \"query_parsed input={} type={} clauses={} fields={}\"\n\n4. Performance:\n   - QueryParser is created once per LexicalIndex (not per search)\n   - Reuse Tantivy Searcher via SearcherManager (leased readers)\n   - For repeated queries: caller can cache results (frankensearch doesn't cache internally)\n   - Snippet generation is optional (skip if caller doesn't need highlights)\n\n5. Field Boosting:\n   - Default boost: title 2.0x, content 1.0x, metadata 0.5x\n   - Configurable via LexicalQuery.field_boosts: HashMap<String, f32>\n   - title_prefix and content_prefix fields: boost 1.5x for prefix matches\n","created_at":"2026-02-13T20:44:52Z"},{"id":135,"issue_id":"bd-3un.18","author":"Dicklesworthstone","text":"REVISION (architectural note - LexicalIndex trait location):\n\nThe LexicalIndex trait is currently defined in this bead (bd-3un.18, Tantivy query parsing). However, the FTS5 adapter (bd-3w1.10) also needs to implement this trait. Having the trait defined in the Tantivy crate creates an unnecessary dependency: FTS5 -> Tantivy, which is architecturally wrong since they are ALTERNATIVES.\n\nRECOMMENDED FIX: Extract the LexicalIndex trait to frankensearch-core (as part of bd-3un.5 result types or a new dedicated traits module). This way:\n  - frankensearch-core defines: trait LexicalIndex\n  - frankensearch-lexical (Tantivy) implements: impl LexicalIndex for TantivyIndex\n  - frankensearch-storage (FTS5) implements: impl LexicalIndex for Fts5LexicalIndex\n  - Neither depends on the other\n\nThe trait definition:\n  pub trait LexicalIndex: Send + Sync {\n      fn search(&self, query: &str, limit: usize, offset: usize) -> SearchResult<Vec<LexicalHit>>;\n      fn index_document(&self, doc: &IndexableDocument) -> SearchResult<()>;\n      fn index_batch(&self, docs: &[IndexableDocument]) -> SearchResult<usize>;\n      fn delete_document(&self, doc_id: &str) -> SearchResult<bool>;\n      fn document_count(&self) -> SearchResult<usize>;\n      fn optimize(&self) -> SearchResult<()>;\n  }\n\n  pub struct LexicalHit {\n      pub doc_id: String,\n      pub bm25_score: f32,\n      pub snippet: Option<String>,\n  }\n\n  pub struct IndexableDocument {\n      pub doc_id: String,\n      pub title: Option<String>,\n      pub content: String,\n      pub metadata: Option<serde_json::Value>,\n  }\n\nThis trait and its associated types should live in frankensearch-core/src/traits/lexical.rs.\n\nIMPACT: bd-3w1.10's dependency on bd-3un.18 can then be replaced with a dependency on bd-3un.5 (where the trait lives). The dependency on bd-3un.17 should be REMOVED entirely (FTS5 does not need Tantivy's schema).\n","created_at":"2026-02-13T21:02:29Z"},{"id":264,"issue_id":"bd-3un.18","author":"Dicklesworthstone","text":"REVIEW FIX — conflicting type definitions, trait location, and LexicalSearch trait:\n\n1. TWO CONFLICTING TYPE DEFINITIONS: The body defines `struct QueryParser` with specific Tantivy fields. The revision suggests extracting a `LexicalIndex` trait to core. These are two different things:\n   - QueryParser: Tantivy-specific query construction (belongs in frankensearch-lexical)\n   - LexicalSearch trait: Abstract interface for any lexical search backend (belongs in frankensearch-core)\n\n   RESOLUTION: Keep both. The trait in core defines the interface:\n   pub trait LexicalSearch: Send + Sync {\n       async fn search(&self, cx: &Cx, query: &str, limit: usize) -> Result<Vec<ScoredResult>, SearchError>;\n       async fn index_document(&self, cx: &Cx, doc: &IndexableDocument) -> Result<(), SearchError>;\n       fn doc_count(&self) -> usize;\n   }\n   \n   TantivyIndex (from bd-3un.17) implements LexicalSearch. QueryParser is internal to TantivyIndex.\n\n2. QUERY SYNTAX: The body mentions \"simple query syntax\" but Tantivy supports both simple and advanced query syntax. Clarify:\n   - Default: Tantivy's QueryParser with lenient mode (no syntax errors on user input)\n   - Fields searched: title (boost 2.0), body (boost 1.0)\n   - Conjunction mode: OR (Tantivy default) — this matches BM25 standard practice\n   \n3. SCORE NORMALIZATION NOTE: BM25 scores from Tantivy are unbounded [0, ∞). The normalization step (bd-3un.19) handles converting these to [0, 1] before fusion.\n\n4. TEST REQUIREMENTS:\n   - Simple query: single term returns matching documents\n   - Multi-term query: \"foo bar\" finds documents with either term (OR mode)\n   - Phrase query: \"\\\"exact phrase\\\"\" returns only exact matches\n   - Boosted field: documents matching title rank higher than body-only matches\n   - Empty query: returns empty results\n   - Special characters: query with @, #, etc. doesn't crash (lenient mode)\n   - Query with no results: returns empty vec, no error\n   - Score ordering: results are sorted by descending BM25 score","created_at":"2026-02-13T21:56:00Z"}]}
{"id":"bd-3un.19","title":"Implement score normalization (min-max)","description":"Implement score normalization utilities used throughout the fusion pipeline. Different search sources produce scores on different scales (BM25 scores vs cosine similarity), so normalization is required before combining them.\n\npub fn min_max_normalize(scores: &mut [f32]) {\n    let min = scores.iter().copied().fold(f32::INFINITY, f32::min);\n    let max = scores.iter().copied().fold(f32::NEG_INFINITY, f32::max);\n    let range = max - min;\n    \n    if range.abs() < f32::EPSILON {\n        // All scores equal → set to 1.0 (not 0.0, to avoid suppressing results)\n        for s in scores.iter_mut() { *s = 1.0; }\n        return;\n    }\n    \n    for s in scores.iter_mut() {\n        *s = (*s - min) / range;\n    }\n}\n\npub fn normalize_scores(scores: &[f32]) -> Vec<f32> {\n    let mut result = scores.to_vec();\n    min_max_normalize(&mut result);\n    result\n}\n\nAlso provide z-score normalization as an alternative:\npub fn z_score_normalize(scores: &mut [f32]) { ... }\n\nThese are used in:\n1. Two-tier blending (normalize fast + quality scores to [0,1] before weighted combination)\n2. RRF doesn't need normalization (rank-based, inherently normalized)\n3. Reranker score normalization (cross-encoder scores can be arbitrary scale)\n\nFile: frankensearch-fusion/src/normalize.rs\n\nReference:\n- cass: src/search/two_tier_search.rs lines 750-765 (normalize_scores)\n- xf: src/hybrid.rs (min_max_normalize)\n- agent-mail: same pattern","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:51:31.268523209Z","created_by":"ubuntu","updated_at":"2026-02-13T21:57:57.537037824Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fusion","phase6","scoring"],"dependencies":[{"issue_id":"bd-3un.19","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:51:31.268523209Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.19","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:24.004145339Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":29,"issue_id":"bd-3un.19","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Score Normalization)\n\n## Mathematical Upgrade: From Min-Max to Copula-Based Score Combination\n\nMin-max normalization is the most basic approach and has known failure modes. The alien-artifact approach uses copula theory for principled score combination.\n\n### Problem with Min-Max\n\nMin-max normalization maps scores to [0,1] but:\n1. Is sensitive to outliers (one extreme score compresses all others)\n2. Destroys distributional information (BM25 scores are roughly Gaussian; cosine similarities are Beta-distributed)\n3. Maps different distributions to the same [0,1] range, making them \"look similar\" when they aren't\n\n### 1. Rank-Based Copula Transform (Recommended Upgrade)\n\nTransform each score set to a uniform distribution via rank transform, then to standard normal:\n\n  fn copula_normalize(scores: &[f32]) -> Vec<f32> {\n      // Step 1: Rank transform → Uniform[0,1]\n      let ranks = rank_transform(scores);  // ties get averaged ranks\n      let uniform: Vec<f32> = ranks.iter().map(|&r| r / (scores.len() as f32 + 1.0)).collect();\n\n      // Step 2: Probit transform → Normal(0,1)\n      let normal: Vec<f32> = uniform.iter().map(|&u| probit(u)).collect();\n      normal\n  }\n\nThis preserves the RANKING within each source while making scores comparable across sources. The probit (inverse normal CDF) ensures Gaussian scores, which behave well under linear combination.\n\n### 2. Empirical Bayes Shrinkage for Small Result Sets\n\nWhen a source returns few results (< 20), min-max normalization is noisy. Apply James-Stein shrinkage toward the grand mean:\n\n  shrunk_score = grand_mean + shrinkage_factor × (raw_score - grand_mean)\n  shrinkage_factor = 1 - (n-2) × σ² / Σ(xᵢ - x̄)²\n\nThis is provably better than raw scores (dominates in mean squared error for n ≥ 3). It prevents small result sets from producing extreme normalized scores.\n\n### 3. When to Use Which\n\n| Scenario                        | Method              | Why                                     |\n|---------------------------------|---------------------|-----------------------------------------|\n| RRF fusion                      | None needed         | RRF is rank-based, doesn't use scores   |\n| Two-tier blending               | Copula + shrinkage  | Comparing different embedding spaces    |\n| Reranker integration            | Min-max is fine     | Same model, same score distribution     |\n| Cross-model comparison (bakeoff)| Copula              | Different models, different distributions|\n\n### 4. Keep Min-Max as Default\n\nThe copula transform is strictly better but adds complexity. Keep min-max as the default implementation, provide copula_normalize as an opt-in alternative:\n\n  pub enum NormalizationMethod {\n      MinMax,           // Simple, fast, good enough for most cases\n      CopulaNormal,     // Principled, handles distributional differences\n      JamesSteinShrink, // For small result sets (< 20 items)\n      None,             // Raw scores (for RRF which doesn't need normalization)\n  }\n\nNote: RRF does NOT depend on normalization (rank-based fusion). This was already correctly identified in the review. The normalization improvements apply to the BLENDING step (bd-3un.21) where we combine fast and quality scores.\n","created_at":"2026-02-13T20:29:52Z"},{"id":271,"issue_id":"bd-3un.19","author":"Dicklesworthstone","text":"REVIEW FIX — z-score edge case and test requirements:\n\n1. Z-SCORE UNDEFINED FOR ZERO STDEV: When all scores are identical, standard deviation = 0, and z-score division produces NaN/Inf. \n\n   RESOLUTION: If stdev < epsilon (1e-10), skip normalization and assign all scores 0.5 (midpoint). This handles:\n   - Single result (stdev = 0)\n   - All identical scores (stdev = 0)\n   - Near-identical scores (stdev ≈ 0, numerical instability)\n\n2. NORMALIZATION METHODS: Clarify the available methods and when each is used:\n   - MinMax: (score - min) / (max - min) → [0, 1]. Used for BM25 scores (unbounded).\n   - Z-score: (score - mean) / stdev → approximately [-3, 3], then clamp to [0, 1]. Alternative.\n   - None: Pass-through for scores already in [0, 1] (e.g., cosine similarity).\n   Default: MinMax for BM25 (lexical), None for cosine (semantic).\n\n3. TEST REQUIREMENTS:\n   - MinMax: [1, 2, 3, 4, 5] → [0.0, 0.25, 0.5, 0.75, 1.0]\n   - MinMax single element: [42.0] → [1.0] (or 0.5, define the convention)\n   - MinMax identical scores: [3.0, 3.0, 3.0] → [0.5, 0.5, 0.5]\n   - Z-score zero stdev: all same scores → all 0.5\n   - Z-score normal: verify output mean ≈ 0.5 for symmetric distributions\n   - None pass-through: scores unchanged\n   - NaN handling: NaN scores in input → handled gracefully (filtered or mapped to 0.0)\n   - Ordering preserved: normalization does not change relative order of scores","created_at":"2026-02-13T21:57:57Z"}]}
{"id":"bd-3un.2","title":"Define core error types (SearchError)","description":"Create comprehensive error types in frankensearch-core that cover all failure modes across the search pipeline. These should be ergonomic for consumers and map cleanly to the error types already used in cass/xf/agent-mail.\n\nError hierarchy (from studying all 3 codebases):\n\npub enum SearchError {\n    // Embedding errors\n    EmbedderUnavailable { model: String, reason: String },\n    EmbeddingFailed { model: String, source: Box<dyn Error> },\n    ModelNotFound { name: String },\n    ModelLoadFailed { path: PathBuf, source: Box<dyn Error> },\n    \n    // Index errors\n    IndexCorrupted { path: PathBuf, detail: String },\n    IndexVersionMismatch { expected: u16, found: u16 },\n    DimensionMismatch { expected: usize, found: usize },\n    IndexNotFound { path: PathBuf },\n    \n    // Search errors\n    QueryParseError { query: String, detail: String },\n    SearchTimeout { elapsed_ms: u64, budget_ms: u64 },\n    \n    // Reranker errors\n    RerankerUnavailable { model: String },\n    RerankFailed { source: Box<dyn Error> },\n    \n    // IO errors\n    Io(std::io::Error),\n    \n    // Configuration errors\n    InvalidConfig { field: String, detail: String },\n}\n\nUse thiserror for derive(Error). Implement Display with actionable messages.\n\nAlso define type aliases:\npub type SearchResult<T> = Result<T, SearchError>;\n\nReference implementations:\n- cass: src/search/reranker.rs (RerankerError), src/search/daemon_client.rs (DaemonError)\n- xf: implicit in Result types across modules\n- agent-mail: SearchError in mcp-agent-mail-search-core","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:13.149991088Z","created_by":"ubuntu","updated_at":"2026-02-13T21:46:24.865968633Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1"],"dependencies":[{"issue_id":"bd-3un.2","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:13.149991088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.2","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T17:55:00.517386900Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":106,"issue_id":"bd-3un.2","author":"Dicklesworthstone","text":"REVISION: Error Message UX and Recovery Guidance\n\nEvery SearchError variant MUST include:\n1. A human-readable message (via thiserror #[error(\"...\")])\n2. Recovery guidance (what should the user do?)\n3. Structured context (for programmatic handling)\n\nError variant catalog with recovery guidance:\n\nSearchError::IndexNotFound { path: PathBuf }\n  Message: \"Vector index not found at {path}\"\n  Recovery: \"Run index_documents() to build the index first, or check FRANKENSEARCH_DATA_DIR\"\n\nSearchError::IndexCorrupted { path: PathBuf, expected_crc: u32, actual_crc: u32 }\n  Message: \"Vector index corrupted at {path}: CRC mismatch\"\n  Recovery: \"Delete the index file and rebuild with index_documents()\"\n\nSearchError::ModelNotFound { model_id: String, searched_paths: Vec<PathBuf> }\n  Message: \"Model {model_id} not found in {searched_paths:?}\"\n  Recovery: \"Run download_models() with consent, or set FRANKENSEARCH_MODEL_DIR\"\n\nSearchError::EmbeddingError { model_id: String, source: Box<dyn Error> }\n  Message: \"Embedding failed for model {model_id}: {source}\"\n  Recovery: \"Transient error; retry or fall back to hash embedder\"\n\nSearchError::RerankerError { model_id: String, source: Box<dyn Error> }\n  Message: \"Reranking failed for model {model_id}: {source}\"\n  Recovery: \"Results still valid without reranking; disable rerank or retry\"\n\nSearchError::QueryError { query: String, position: usize, suggestion: Option<String> }\n  Message: \"Query parse error at position {position}: {suggestion}\"\n  Recovery: \"Fix the query syntax. Common issues: unmatched quotes, invalid date range\"\n\nSearchError::ConfigError { field: String, value: String, reason: String }\n  Message: \"Invalid config {field}={value}: {reason}\"\n  Recovery: \"Check TwoTierConfig documentation for valid values\"\n\nSearchError::IoError(std::io::Error)\n  Message: \"I/O error: {0}\"\n  Recovery: \"Check file permissions and disk space\"\n\nSearchError::TimeoutError { operation: String, elapsed_ms: u64, limit_ms: u64 }\n  Message: \"{operation} timed out after {elapsed_ms}ms (limit: {limit_ms}ms)\"\n  Recovery: \"Increase timeout in TwoTierConfig, or reduce query complexity\"\n\nDesign principle: errors should guide the user to a solution, not just report a failure.\nThe TwoTierSearcher should catch transient errors and degrade gracefully\n(EmbeddingError -> fall back to hash, RerankerError -> skip reranking, TimeoutError -> yield Initial).\nOnly IndexNotFound and ConfigError should prevent search from starting at all.\n","created_at":"2026-02-13T20:57:41Z"},{"id":134,"issue_id":"bd-3un.2","author":"Dicklesworthstone","text":"REVISION (FrankenSQLite + durability integration - new error variants):\n\nThe following error variants are needed by the FrankenSQLite storage (bd-3w1) and RaptorQ durability (bd-3w1.5-9) integration. Add these to SearchError:\n\n  // Storage errors (feature = \"storage\")\n  StorageError { detail: String },\n  // Wraps FrankenSQLite errors. Examples:\n  //   \"unexpected NULL in column 'content_hash'\"\n  //   \"transaction conflict (MVCC retry)\"\n  //   Recovery: \"Check FrankenSQLite database integrity, or delete and rebuild\"\n\n  // Durability errors (feature = \"durability\")\n  IndexCorrupted { path: PathBuf, detail: String },\n  // Already listed in the base error enum above, but confirm it includes:\n  //   - Which file is corrupted\n  //   - Nature of corruption (CRC mismatch, truncated, bad magic)\n  //   Recovery: \"If durability is enabled, repair will be attempted automatically.\n  //              Otherwise, delete the index file and rebuild from source.\"\n\n  RepairFailed { path: PathBuf, reason: String },\n  // RaptorQ repair was attempted but corruption exceeds repair capacity\n  //   Recovery: \"Corruption exceeds 20% of file. Rebuild index from document store.\"\n\n  DurabilityDisabled,\n  // Repair was requested but the 'durability' feature is not compiled in\n  //   Recovery: \"Enable the 'durability' Cargo feature to use self-healing indices\"\n\nThese variants should be feature-gated at the ENUM level using cfg attributes:\n\n  pub enum SearchError {\n      // ... existing variants ...\n\n      #[cfg(feature = \"storage\")]\n      StorageError { detail: String },\n\n      #[cfg(feature = \"durability\")]\n      RepairFailed { path: PathBuf, reason: String },\n\n      DurabilityDisabled,  // NOT feature-gated (available always, returned when feature is off)\n  }\n\nNote: IndexCorrupted already exists in the base enum (it's a general index error, not durability-specific). DurabilityDisabled is NOT feature-gated because it's returned precisely when the feature is NOT enabled.\n","created_at":"2026-02-13T21:02:28Z"},{"id":222,"issue_id":"bd-3un.2","author":"Dicklesworthstone","text":"REVIEW FIX — Error type reconciliation and asupersync interop:\n\n1. VARIANT NAME RECONCILIATION: The body and revision comments use different names for the same variants. CANONICAL names (body wins unless revision has good reason):\n   - EmbeddingFailed { source, embedder_id, detail } — KEEP (body name)\n   - QueryParseError { query, detail } — KEEP (body name)\n   - SearchTimeout { phase, elapsed, limit } — KEEP (body name)\n   - InvalidConfig { field, value, reason } — KEEP (body name)\n   - IndexCorrupted { path, detail } — KEEP (body version with String detail, more flexible than CRC-specific fields)\n\n2. MISSING VARIANT — Add `Cancelled`:\n   /// Operation was cancelled via asupersync Cx cancellation protocol.\n   /// This enables conversion from asupersync::Outcome::Cancelled to SearchError.\n   Cancelled {\n       phase: String,      // Which phase was active when cancelled\n       reason: String,     // CancelKind description\n   }\n\n   Plus a From impl:\n   impl From<asupersync::CancelError> for SearchError {\n       fn from(e: asupersync::CancelError) -> Self {\n           SearchError::Cancelled { phase: \"unknown\".into(), reason: e.to_string() }\n       }\n   }\n\n3. MISSING VARIANT — Add `HashMismatch` (needed by bd-3un.11 model downloads):\n   HashMismatch {\n       path: PathBuf,\n       expected: String,\n       actual: String,\n   }\n\n4. FEATURE-GATED VARIANTS: The body uses `#[cfg(feature = \"storage\")]` on individual enum variants. This is fragile — match arms break across feature combinations. FIX: Use a boxed inner error instead:\n\n   // Instead of:\n   #[cfg(feature = \"storage\")]\n   StorageError { source: Box<dyn std::error::Error + Send + Sync> }\n\n   // Use:\n   /// Wraps errors from optional subsystems (storage, durability, etc.)\n   /// Always present in the enum but only constructible when the feature is enabled.\n   SubsystemError {\n       subsystem: &'static str,  // \"storage\", \"durability\", \"fts5\"\n       source: Box<dyn std::error::Error + Send + Sync>,\n   }\n\n   This single variant replaces all feature-gated variants. Match arms are stable regardless of features.\n\n5. ASUPERSYNC INTEROP: Add conversion between SearchError and asupersync error types:\n   impl From<SearchError> for asupersync::Error {\n       fn from(e: SearchError) -> Self { asupersync::Error::custom(e) }\n   }\n\n6. TEST REQUIREMENTS for this bead:\n   - Every variant has a Display message that includes actionable recovery guidance\n   - From<std::io::Error> conversion works correctly\n   - From<asupersync::CancelError> conversion preserves cancel reason\n   - SubsystemError wraps arbitrary inner errors correctly\n   - Error is Send + Sync (required for async contexts)\n   - Serialization round-trip (serde) preserves variant and fields","created_at":"2026-02-13T21:46:24Z"}]}
{"id":"bd-3un.20","title":"Implement Reciprocal Rank Fusion (RRF)","description":"Implement Reciprocal Rank Fusion (RRF) for combining lexical and semantic search results. RRF is the standard fusion algorithm used across all 3 codebases and is well-established in IR literature.\n\nAlgorithm:\n  score(doc) = Σ 1/(K + rank_i + 1)   for each source i where doc appears\n  K = 60 (empirically optimal constant from literature)\n\npub struct RrfConfig {\n    /// RRF constant K (default: 60.0). Higher = more weight to high-ranked docs.\n    pub k: f64,\n    /// Tie-breaking epsilon (default: 1e-9).\n    pub epsilon: f64,\n}\n\npub fn rrf_fuse(\n    lexical: &[LexicalHit],\n    semantic: &[VectorHit],\n    limit: usize,\n    offset: usize,\n    config: &RrfConfig,\n) -> Vec<FusedHit> {\n    // 1. Build HashMap<doc_id, FusedHit>\n    // 2. For each lexical result (rank 0, 1, ...):\n    //    score += 1.0 / (K + rank + 1)\n    //    Record lexical_rank, lexical_score\n    // 3. For each semantic result (rank 0, 1, ...):\n    //    score += 1.0 / (K + rank + 1)\n    //    Record semantic_rank, semantic_score\n    // 4. Sort by: rrf_score (desc) → in_both_sources (tiebreak) → doc_id (stable)\n    // 5. Apply offset and limit\n}\n\nCandidate budget (from xf src/hybrid.rs):\npub const CANDIDATE_MULTIPLIER: usize = 3;  // Fetch 3x limit from each source\n\npub fn candidate_count(limit: usize, offset: usize) -> usize {\n    limit.saturating_add(offset).saturating_mul(CANDIDATE_MULTIPLIER)\n}\n\nThis is the heart of hybrid search — it rewards documents that appear in BOTH lexical and semantic results (they get scores from both sources), which typically produces better results than either source alone.\n\nEnvironment variable: FRANKENSEARCH_RRF_K (override K constant)\n\nFile: frankensearch-fusion/src/rrf.rs\n\nReference:\n- cass: src/search/query.rs (RRF with k=60, candidate multiplier 3x/4x)\n- xf: src/hybrid.rs (rrf_fuse function, K=60, CANDIDATE_MULTIPLIER=3)\n- agent-mail: crates/mcp-agent-mail-search-core/src/fusion.rs (DEFAULT_RRF_K=60)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:51:31.343268779Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:48.546725333Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fusion","phase6","rrf"],"dependencies":[{"issue_id":"bd-3un.20","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:51:31.343268779Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.20","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:24.084932128Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":5,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"RRF ALGORITHM CONTEXT: Reciprocal Rank Fusion was introduced by Cormack et al. (2009) and has become the standard for combining ranked lists from different retrieval systems.\n\nWhy K=60: The K parameter controls how much weight is given to high-ranked vs lower-ranked results. K=60 is the empirically optimal value from the original paper and has been independently validated across many IR systems. Higher K values make the scores more uniform (less distinction between ranks); lower K values amplify the importance of being highly ranked.\n\nWhy RRF over alternatives:\n- Simple: no training data or score calibration needed\n- Robust: works well even when score distributions differ wildly (BM25 scores vs cosine similarities)\n- Principled: theoretically grounded in rank aggregation\n- Proven: used in production at Elastic, Pinecone, Vespa, etc.\n\nThe candidate multiplier (3x) is important: if the user wants 10 results, we fetch 30 from each source. This ensures good coverage for docs that might rank differently across sources.","created_at":"2026-02-13T17:56:49Z"},{"id":18,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. DETERMINISTIC TIE-BREAKING: The RRF output must use a strict multi-level sort for reproducible results across runs. From xf hybrid.rs and agent-mail fusion.rs:\n\nSorting chain for FusedHit (ALL levels must be applied):\n  Level 1: rrf_score DESCENDING (with epsilon=1e-9 for float comparison)\n  Level 2: in_both_sources bonus (true > false) -- docs in both lexical+semantic rank higher on tie\n  Level 3: lexical_score DESCENDING (if available, with epsilon comparison)\n  Level 4: doc_id ASCENDING (absolute determinism -- string comparison)\n\nThis 4-level chain ensures that:\n- Results are reproducible across runs (no HashMap iteration order dependency)\n- Equal-RRF-score docs are ordered by which had better lexical coverage\n- Final doc_id tiebreak prevents any residual ordering ambiguity\n\n2. RANK CONVENTION: Use 0-based ranks internally with the formula:\n   score(doc) = 1.0 / (K + rank + 1.0)\n   This is equivalent to agent-mail's 1-based convention: 1.0 / (K + rank_1based)\n   Document this equivalence explicitly so implementors don't get confused.\n\n3. DEDUPLICATION KEY: After fusion, deduplicate by doc_id. If the same doc_id appears in both sources, its scores are summed (not duplicated). The FusedHit should track both source ranks for explainability.\n","created_at":"2026-02-13T20:24:37Z"},{"id":26,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (RRF Fusion)\n\n## Mathematical Upgrade: From Ad-Hoc RRF to Principled Rank Aggregation\n\nCurrent design uses fixed K=60 RRF. This works, but there's a richer mathematical framework from social choice theory and probabilistic rank aggregation that would make this genuinely alien-artifact quality.\n\n### 1. Plackett-Luce Rank Aggregation (replaces ad-hoc K constant)\n\nRRF's 1/(K+rank) is actually a special case of the Plackett-Luce model for rank aggregation. The PL model assigns probability to a ranking π as:\n\n  P(π) = ∏ᵢ wᵢ / (Σⱼ≥ᵢ wⱼ)\n\nWhere wᵢ are per-item \"worth\" parameters. When we set wᵢ = 1/(K+rankᵢ), we recover RRF. But the PL framework lets us LEARN optimal weights from implicit feedback (clicks, dwell time), giving us adaptive fusion that improves with usage.\n\nImplementation: Start with RRF (K=60) as the uninformative prior. As user interactions accumulate, update PL weights via MM algorithm (minorization-maximization). This converges in O(n·log n) per update and gives provably optimal rank aggregation under the PL model.\n\n### 2. Kemeny Distance for Fusion Quality Monitoring\n\nInstead of just Kendall's tau for rank correlation (already in bd-3un.21), use Kemeny distance to measure how much the fused ranking deviates from each source. Kemeny distance is the MINIMUM number of pairwise swaps to transform one ranking into another. This gives a principled metric for:\n- Detecting when lexical and semantic rankings diverge wildly (→ query is ambiguous)\n- Monitoring whether fusion is actually helping (→ average Kemeny distance should decrease)\n\n### 3. Conformal Prediction Sets for Score Calibration\n\nWrap RRF scores in conformal prediction sets to provide distribution-free coverage guarantees:\n\n  P(doc_relevance ∈ Ĉ(score)) ≥ 1 - α\n\nThis requires a calibration set (ground truth relevance judgments for a sample of queries), but provides FORMAL guarantees that the score-to-relevance mapping is calibrated. No distributional assumptions needed.\n\n### 4. Optimal K via Bayesian Online Learning\n\nInstead of fixed K=60, maintain a Gamma(α,β) posterior over K, updated by implicit relevance signals. The posterior predictive gives E[K] = α/β which adapts per-query-class:\n- Short queries (1-2 words): may benefit from higher K (more uniform weighting)\n- Long queries (5+ words): may benefit from lower K (sharper top-heavy weighting)\n\n### Performance Optimization: Parallel RRF with Sharded HashMap\n\nFor large result sets (1000+ from each source), the HashMap-based fusion becomes the bottleneck. Use a sharded HashMap (DashMap or 16 Mutex<HashMap>) partitioned by doc_id hash. Each shard can be updated independently. For the common case (< 300 results per source), the overhead isn't worth it, so add a threshold check.\n\n### Implementation Priority\n\n1. Implement standard RRF first (as designed) — this is correct and well-tested\n2. Add Kemeny distance monitoring alongside Kendall's tau — pure addition, no API change\n3. Add adaptive K via Beta posterior — simple addition with env var fallback\n4. Add Plackett-Luce upgrade path — optional, behind feature flag\n\nThese upgrades make the fusion system formally principled while keeping the simple RRF as the always-correct baseline.\n","created_at":"2026-02-13T20:29:49Z"},{"id":52,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"CORRECTNESS NOTE: RRF Score Semantics and Edge Cases\n\n1. Zero-Based Rank Convention Documentation:\n   The formula 1/(K + rank + 1) with 0-based ranks means rank=0 gives score 1/(K+1).\n   For K=60: rank 0 -> 1/61 = 0.01639, rank 1 -> 1/62 = 0.01613.\n   This MUST be documented prominently because off-by-one here silently degrades quality.\n   The +1 in the denominator is because the original RRF paper uses 1-based ranks: 1/(K+r).\n   With 0-based ranks, this becomes 1/(K+r+1) to produce identical scores.\n\n2. Score Summation for Multi-Source Documents:\n   When a document appears in both lexical and semantic results, its RRF scores are SUMMED.\n   This means multi-source docs get up to 2x the score of single-source docs.\n   This is the standard RRF formulation and is correct -- being retrieved by multiple\n   systems is strong evidence of relevance.\n\n3. Dedup Semantics:\n   Documents are deduped by doc_id (String equality). When a duplicate is found,\n   scores are summed into the first occurrence. The HashMap insert-or-add pattern\n   must use entry().and_modify().or_insert() for correctness.\n\n4. Empty Input Handling:\n   - Both lists empty -> return empty results\n   - One list empty -> return the non-empty list's RRF scores only\n   - Both lists have same single doc -> sum both contributions\n","created_at":"2026-02-13T20:45:32Z"},{"id":169,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — NOTE (DashMap reference):\n\nThe comment about \"sharded HashMap (DashMap or 16 Mutex<HashMap>)\" for large result sets should prefer asupersync::sync::Mutex if sharding is needed. However, RRF fusion is typically a single-threaded operation on small result sets (< 1000 items), so this optimization is unlikely to be needed.\n\nIf needed: asupersync::sync::Mutex<HashMap> per shard (16 shards) is simpler than DashMap and integrates with cancel-aware locking. DashMap is a third-party dependency that we don't need.","created_at":"2026-02-13T21:07:02Z"},{"id":287,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"REVIEW FIX — Config-only parameter loading and test additions:\n\n1. ENV VAR LOADING: The body may reference loading RRF K from an environment variable. All configuration should flow through TwoTierConfig (bd-3un.22), not direct env var reads. TwoTierConfig can be populated from env vars at the config layer, but RRF code should only read from the config struct.\n\n2. TEST ADDITIONS:\n   - RRF with K=60 (default): verify score formula 1/(K+rank) for known inputs\n   - RRF with K=1: extreme value, verify different ranking behavior\n   - Single-source RRF: only semantic results → RRF = semantic ranking\n   - Both sources same document: document appears once with combined score\n   - 4-level tie-breaking: when RRF scores are identical, verify tie-break order (in_both > source_diversity > original_rank > doc_id)\n   - Empty input: no results from either source → empty output\n   - Score ordering: output is strictly descending by fused score","created_at":"2026-02-13T21:59:48Z"}]}
{"id":"bd-3un.21","title":"Implement two-tier score blending","description":"Implement the score blending algorithm that combines fast-tier and quality-tier semantic scores when the quality model finishes processing.\n\npub fn blend_two_tier(\n    fast_results: &[VectorHit],      // From fast embedder (already normalized)\n    quality_results: &[VectorHit],   // From quality embedder (already normalized)\n    blend_factor: f32,               // 0.0 = fast-only, 1.0 = quality-only\n) -> Vec<VectorHit> {\n    // For each document appearing in either result set:\n    // blended_score = fast_score * (1.0 - blend_factor) + quality_score * blend_factor\n    //\n    // Default blend_factor: 0.7 (70% quality, 30% fast)\n    //\n    // Algorithm:\n    // 1. Normalize both sets to [0, 1] via min_max_normalize\n    // 2. Build HashMap<doc_id, (fast_score, quality_score)>\n    // 3. For docs only in fast: quality_score = 0.0\n    // 4. For docs only in quality: fast_score = 0.0\n    // 5. Compute blended_score for each\n    // 6. Sort by blended_score descending\n}\n\nThe blend_factor of 0.7 was chosen based on empirical testing:\n- 0.7 gives quality model dominant influence while still valuing fast rankings\n- Fast rankings serve as a useful 'prior' that prevents quality model from making radical changes\n- This produces smooth visual transitions (results don't jump around too much)\n\nAlso provide rank correlation metrics for debugging/monitoring:\n\npub fn compute_rank_changes(\n    initial: &[VectorHit],\n    refined: &[VectorHit],\n) -> RankChanges {\n    // Returns counts of promoted/demoted/stable results\n}\n\npub fn kendall_tau(\n    initial: &[VectorHit],\n    refined: &[VectorHit],\n) -> Option<f64> {\n    // Kendall's tau coefficient (-1 to 1)\n    // Values near 1 = rankings barely changed\n    // Values near 0 = significant reordering\n    // Useful for monitoring whether quality model is doing useful work\n}\n\nFile: frankensearch-fusion/src/blend.rs\n\nReference:\n- xf: src/hybrid.rs (blend_two_tier, compute_rank_changes, kendall_tau)\n- cass: src/search/two_tier_search.rs lines 696-712 (weight blending)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 836-902","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:51:31.417801691Z","created_by":"ubuntu","updated_at":"2026-02-13T21:58:04.455332247Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["blending","fusion","phase6","two-tier"],"dependencies":[{"issue_id":"bd-3un.21","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:51:31.417801691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.21","depends_on_id":"bd-3un.19","type":"blocks","created_at":"2026-02-13T17:55:24.327257910Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.21","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:24.244555602Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":27,"issue_id":"bd-3un.21","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (Two-Tier Blending)\n\n## Mathematical Upgrade: From Fixed Blend Factor to Bayesian Adaptive Blending\n\nCurrent design uses a fixed 0.7 blend factor. This is a reasonable starting point but leaves significant quality on the table. The alien-artifact approach uses online Bayesian learning to adapt the blend factor per query characteristics.\n\n### 1. Beta-Bernoulli Adaptive Blend Weight\n\nModel the \"quality model agrees with ground truth\" as a Bernoulli trial. Maintain Beta(α,β) posterior, initialized as Beta(7, 3) to encode the 0.7 prior:\n\n  P(quality_is_better | data) ~ Beta(α + successes, β + failures)\n  E[blend_factor] = α / (α + β)\n\nAfter each query with implicit feedback (click on result = success for the scoring system that ranked it higher), update the posterior. The blend factor naturally adapts:\n- When quality consistently outperforms fast → α grows → blend_factor → 1.0\n- When quality adds noise (e.g., domain mismatch) → β grows → blend_factor → 0.5\n\nThis is a CONJUGATE prior, so updates are O(1) with no MCMC needed.\n\n### 2. Thompson Sampling for Exploration-Exploitation\n\nRather than always using E[blend_factor], occasionally SAMPLE from Beta(α,β) to explore:\n\n  blend = sample(Beta(α, β))\n\nThis automatically balances exploitation (use best known blend) with exploration (try different blends to learn faster). Thompson sampling is provably optimal in the Bayesian regret sense.\n\n### 3. Evidence Ledger for Blend Decisions\n\nFor every search query, log a structured evidence entry:\n\n  {\n    query_hash: u64,\n    fast_ndcg: f32,\n    quality_ndcg: f32,\n    blended_ndcg: f32,\n    blend_factor_used: f32,\n    rank_correlation: f32,  // Kendall's tau between fast and quality\n    quality_latency_ms: u64,\n    decision_reason: \"bayesian_posterior\" | \"fixed_default\" | \"fast_only_timeout\"\n  }\n\nThis provides complete explainability — you can always show exactly WHY the blend factor was what it was.\n\n### 4. Formal Regret Bound\n\nUnder the Beta-Bernoulli model with Thompson sampling, the expected regret after T queries is bounded by:\n\n  E[Regret(T)] ≤ O(√(T · log T))\n\nThis is a formal guarantee that the blend factor converges to optimal at a known rate.\n\n### 5. Performance: Zero-Overhead Adaptive Blending\n\nThe Beta posterior is just two floats (α, β). Sampling from Beta requires one call to the beta distribution RNG (< 1μs). The overhead vs. fixed 0.7 is literally unmeasurable.\n\nThe blend_factor computation itself is the same multiply-add — only the WEIGHT changes.\n\n### Implementation in bd-3un.21\n\nAdd to the blend module:\n\n  pub struct AdaptiveBlender {\n      alpha: AtomicF32,  // Beta posterior param (success count + prior)\n      beta: AtomicF32,   // Beta posterior param (failure count + prior)\n      min_samples: usize, // Don't adapt until N queries observed (default: 50)\n  }\n\n  impl AdaptiveBlender {\n      pub fn new() -> Self { Self { alpha: 7.0, beta: 3.0, min_samples: 50 } }\n      pub fn blend_factor(&self) -> f32 { self.alpha / (self.alpha + self.beta) }\n      pub fn update(&self, quality_was_better: bool) { ... }\n  }\n\nThe fixed 0.7 fallback is the zero-observation case (Beta(7,3) prior). Full backward compatibility.\n","created_at":"2026-02-13T20:29:50Z"},{"id":53,"issue_id":"bd-3un.21","author":"Dicklesworthstone","text":"CORRECTNESS NOTE: Two-Tier Blend Scoring for Partial Coverage\n\nDesign decision documentation for missing score handling:\n\nThe blend function uses 0.0 for missing scores: if a document has a fast_score but no\nquality_score, its blended score = fast_score * (1-blend_factor) + 0.0 * blend_factor.\n\nThis is INTENTIONAL and CORRECT in the two-tier pipeline because:\n\n1. Phase 1 only computes quality embeddings for the top N candidates from Phase 0.\n2. Documents outside top N never enter the blend function.\n3. Within the top N, all documents have fast scores. A missing quality score means\n   the quality model couldn't process it (e.g., timeout, error).\n4. Using 0.0 for missing quality scores penalizes failed-to-embed docs, which is\n   the correct conservative behavior (we don't know their quality score).\n\nAlternative considered and rejected:\n- Imputing missing quality scores from fast scores (e.g., quality = fast * 0.8)\n  This would be an unjustified assumption about score correlation.\n- Keeping fast score unchanged for missing quality\n  This would bypass the blend entirely, giving fast-only docs an unfair advantage\n  when blend_factor > 0.5 (which is the default at 0.7).\n\nIMPORTANT: The blend function MUST document this behavior clearly with an example\nin its doc comment showing the scoring math for a doc with missing quality score.\n","created_at":"2026-02-13T20:45:32Z"},{"id":272,"issue_id":"bd-3un.21","author":"Dicklesworthstone","text":"REVIEW FIX — AtomicF32 does not exist, and test requirements:\n\n1. AtomicF32 DOES NOT EXIST IN STD: If the body references AtomicF32 for the blend weight, this type doesn't exist in Rust's std library. Options:\n   a) Use AtomicU32 with f32::to_bits() / f32::from_bits() bit-cast pattern (same as bd-21g's AtomicF64 fix)\n   b) Use asupersync::sync::Mutex<f32> (simpler, cancel-aware)\n   c) Use a regular f32 behind Arc<asupersync::sync::RwLock<TwoTierConfig>> — the blend weight comes from config\n\n   RESOLUTION: Option (c) — the blend weight is part of TwoTierConfig, which is already behind a shared reference. No atomic needed. If hot-path performance requires avoiding the lock, use option (a):\n   let bits = AtomicU32::load(Ordering::Relaxed);\n   let weight = f32::from_bits(bits);\n\n2. BLEND FORMULA: Clarify the canonical formula:\n   final_score = alpha * quality_score + (1 - alpha) * fast_score\n   where alpha = config.quality_weight (default 0.7)\n   \n   For documents only in one source:\n   - Only in fast: final_score = (1 - alpha) * fast_score\n   - Only in quality: final_score = alpha * quality_score\n   This naturally penalizes single-source results.\n\n3. TEST REQUIREMENTS:\n   - Alpha=0.7: quality_score=1.0, fast_score=0.5 → blended = 0.7*1.0 + 0.3*0.5 = 0.85\n   - Alpha=1.0: only quality score matters\n   - Alpha=0.0: only fast score matters\n   - Single-source (fast only): correct penalty applied\n   - Single-source (quality only): correct penalty applied\n   - Both sources same score: blended = original score\n   - Ordering: higher quality scores promote documents\n   - NaN scores: handled gracefully (skip or default to 0.0)","created_at":"2026-02-13T21:58:04Z"}]}
{"id":"bd-3un.22","title":"Implement TwoTierConfig with presets and env overrides","description":"Implement the configuration struct for the two-tier search system. This controls all tuning knobs for the progressive search experience.\n\npub struct TwoTierConfig {\n    /// Fast embedding model name (default: 'potion-multilingual-128M')\n    pub fast_model: Option<String>,\n    \n    /// Quality embedding model name (default: 'all-MiniLM-L6-v2')\n    pub quality_model: Option<String>,\n    \n    /// Fast embedding dimension (default: 256)\n    pub fast_dimension: usize,\n    \n    /// Quality embedding dimension (default: 384)\n    pub quality_dimension: usize,\n    \n    /// Blend factor: 0.0 = fast-only, 1.0 = quality-only (default: 0.7)\n    pub quality_weight: f32,\n    \n    /// Max documents to refine with quality model (default: 100)\n    pub max_refinement_docs: usize,\n    \n    /// Quality model timeout in ms (default: 500)\n    pub quality_timeout_ms: u64,\n    \n    /// Skip quality refinement (fast results only)\n    pub fast_only: bool,\n    \n    /// Skip fast phase (quality results only, slower but highest quality)\n    pub quality_only: bool,\n    \n    /// RRF K constant for hybrid fusion (default: 60.0)\n    pub rrf_k: f64,\n    \n    /// Candidate multiplier for retrieval (default: 3x)\n    pub candidate_multiplier: usize,\n}\n\nPresets:\n- TwoTierConfig::default() → Full two-tier with 0.7 quality weight\n- TwoTierConfig::fast_only() → No quality refinement, instant results\n- TwoTierConfig::quality_only() → Best quality, no fast phase\n- TwoTierConfig::balanced() → 0.5 quality weight\n\nEnvironment variable overrides (from xf src/config.rs):\n- FRANKENSEARCH_FAST_MODEL\n- FRANKENSEARCH_QUALITY_MODEL\n- FRANKENSEARCH_BLEND_FACTOR (or FRANKENSEARCH_QUALITY_WEIGHT)\n- FRANKENSEARCH_QUALITY_TIMEOUT_MS\n- FRANKENSEARCH_RRF_K\n- FRANKENSEARCH_CANDIDATE_MULTIPLIER\n\nConfig loading priority: explicit code > env vars > defaults\n\nBuilder pattern:\nTwoTierConfig::builder()\n    .fast_model('potion-multilingual-128M')\n    .quality_weight(0.8)\n    .max_refinement_docs(50)\n    .build()\n\nFile: frankensearch-fusion/src/config.rs\n\nReference:\n- xf: src/config.rs (TwoTierConfig with env vars)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 60-109","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:52:20.058628453Z","created_by":"ubuntu","updated_at":"2026-02-13T21:58:11.596841417Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","phase7","two-tier"],"dependencies":[{"issue_id":"bd-3un.22","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:20.058628453Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.22","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:29.667615367Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":45,"issue_id":"bd-3un.22","author":"Dicklesworthstone","text":"REVISION: TwoTierConfig Hardening\n\n1. Validation Logic (enforce at construction time):\n   - blend_factor: must be in [0.0, 1.0], panic-free clamp with WARN log if out of range\n   - rrf_k: must be > 0 (K=0 causes division by 1/(0+rank+1) which is valid but unintended)\n   - quality_timeout_ms: must be > 0, default 200ms, cap at 5000ms\n   - candidate_multiplier: must be >= 1.0, default 3.0\n   - refinement_doc_limit: must be > 0, default 50\n   - Log all overrides at INFO: \"config_override field={} value={} source=env|explicit\"\n\n2. Serde Serialization:\n   - Derive Serialize, Deserialize for config file persistence\n   - Support TOML format (frankensearch.toml) for human-readable config\n   - Round-trip test: serialize -> deserialize -> assert_eq\n\n3. Builder Pattern:\n   - TwoTierConfigBuilder with method chaining\n   - .build() returns Result<TwoTierConfig, ConfigError> with validation\n   - Each setter logs at TRACE level for config debugging\n\n4. Integration with Adaptive Fusion (bd-21g):\n   - TwoTierConfig gains optional field: adaptive_params: Option<AdaptiveParams>\n   - When Some, blend_factor and rrf_k become initial priors, not fixed values\n   - Adaptive mode is opt-in, defaults to None (static parameters)\n\n5. Environment Variable Priority:\n   - Document clearly: explicit code > env vars > defaults\n   - Use a helper fn: fn env_or<T: FromStr>(key: &str, default: T) -> T\n   - All env vars prefixed FRANKENSEARCH_ to avoid collisions\n","created_at":"2026-02-13T20:44:50Z"},{"id":273,"issue_id":"bd-3un.22","author":"Dicklesworthstone","text":"REVIEW FIX — fast_only/quality_only mutual exclusion, timeout default, and test requirements:\n\n1. FAST_ONLY + QUALITY_ONLY MUTUAL EXCLUSION: If both fast_only and quality_only are set to true, the behavior is undefined. Add validation:\n   \n   impl TwoTierConfig {\n       pub fn validate(&self) -> Result<(), SearchError> {\n           if self.fast_only && self.quality_only {\n               return Err(SearchError::InvalidConfig(\"fast_only and quality_only are mutually exclusive\".into()));\n           }\n           if self.quality_weight < 0.0 || self.quality_weight > 1.0 {\n               return Err(SearchError::InvalidConfig(\"quality_weight must be in [0.0, 1.0]\".into()));\n           }\n           if self.rrf_k == 0 {\n               return Err(SearchError::InvalidConfig(\"rrf_k must be > 0\".into()));\n           }\n           Ok(())\n       }\n   }\n\n2. TIMEOUT DEFAULT MISMATCH: If the body says quality_timeout = 500ms but comments say 200ms (or vice versa), reconcile:\n   CANONICAL DEFAULT: quality_timeout = Duration::from_millis(500)\n   Rationale: MiniLM embedding alone takes ~128ms. With quality vector search + optional rerank, 500ms is a reasonable upper bound.\n\n3. quality_only SEMANTICS (from bd-3un.9 review): quality_only was removed. fast_only remains. When fast_only = true: skip quality phase entirely. When fast_only = false: run both phases (normal two-tier behavior).\n\n4. CONFIGURABLE HNSW PARAMETERS: Add fields for bd-3un.16:\n   pub hnsw_ef_search: usize,         // default 100\n   pub hnsw_ef_construction: usize,   // default 200  \n   pub hnsw_m: usize,                 // default 16\n\n5. TEST REQUIREMENTS:\n   - Default config: all defaults are valid (validate() returns Ok)\n   - Mutual exclusion: fast_only + quality_only → Err\n   - Invalid quality_weight: -0.1 → Err, 1.1 → Err\n   - Invalid rrf_k: 0 → Err\n   - Serialization round-trip: serialize to TOML, deserialize, values match\n   - Config from environment: FRANKENSEARCH_QUALITY_WEIGHT=0.8 overrides default\n   - Config from file: frankensearch.toml loaded correctly","created_at":"2026-02-13T21:58:11Z"}]}
{"id":"bd-3un.23","title":"Implement TwoTierIndex (dual-index storage)","description":"Implement the TwoTierIndex that manages two parallel vector indices — one for fast-tier embeddings and one for quality-tier embeddings. This is the data structure that enables progressive search.\n\npub struct TwoTierIndex {\n    fast_index: VectorIndex,             // 256-dim potion embeddings\n    quality_index: Option<VectorIndex>,  // 384-dim MiniLM embeddings (may not exist yet)\n    doc_ids: Vec<String>,                // Shared doc ID list (both indices same order)\n    has_quality: Vec<bool>,              // Track which docs have quality embeddings\n    config: TwoTierConfig,\n}\n\nimpl TwoTierIndex {\n    /// Open from a directory containing vector.fast.idx and optionally vector.quality.idx\n    pub fn open(dir: &Path, config: TwoTierConfig) -> SearchResult<Self>;\n    \n    /// Create a new two-tier index from scratch\n    pub fn create(dir: &Path, config: TwoTierConfig) -> SearchResult<TwoTierIndexBuilder>;\n    \n    /// Search using fast embeddings only\n    pub fn search_fast(&self, query_vec: &[f32], k: usize) -> Vec<VectorHit>;\n    \n    /// Get quality scores for specific document indices (used during refinement)\n    pub fn quality_scores_for_indices(\n        &self,\n        query_vec: &[f32],\n        indices: &[usize],\n    ) -> Vec<f32>;\n    \n    /// Check if quality index is available\n    pub fn has_quality_index(&self) -> bool;\n    \n    /// Number of indexed documents\n    pub fn doc_count(&self) -> usize;\n}\n\nFile naming convention:\n- {dir}/vector.fast.idx  → fast-tier embeddings\n- {dir}/vector.quality.idx → quality-tier embeddings\n- {dir}/vector.idx → fallback single-tier index\n\nCaching (from xf src/main.rs):\npub struct VectorIndexCache {\n    fast: OnceLock<Option<VectorIndex>>,\n    quality: OnceLock<Option<VectorIndex>>,\n    // Staleness detection for auto-rebuild\n}\n\nThe cache uses OnceLock for thread-safe lazy initialization. The index is loaded once and reused for all queries in a session.\n\nReference:\n- xf: src/main.rs lines 50-150 (VectorIndexCache with fast/quality/default)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 199-255 (TwoTierIndex)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:52:20.138236083Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:07.177947217Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["index","phase7","two-tier"],"dependencies":[{"issue_id":"bd-3un.23","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:20.138236083Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.23","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T17:55:29.743275010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.23","depends_on_id":"bd-3un.22","type":"blocks","created_at":"2026-02-13T17:55:29.821622919Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":46,"issue_id":"bd-3un.23","author":"Dicklesworthstone","text":"REVISION: TwoTierIndex Implementation Details\n\n1. Cache Invalidation Strategy:\n   - OnceLock means indices are loaded once and stay resident\n   - Invalidation via TwoTierIndex::reload() which replaces the OnceLock contents\n   - On index rebuild: writer creates new files with .tmp suffix, then atomic rename\n   - Readers see stale data until reload() is called (eventual consistency, not instant)\n   - Integration with bd-3un.41 (staleness): staleness detector triggers reload()\n\n2. Consistency Between Fast and Quality Indices:\n   - CRITICAL: fast and quality indices MUST share the same doc_id set\n   - During incremental indexing: fast index is always updated; quality index may lag\n   - has_quality HashMap tracks which doc_ids have quality embeddings\n   - Documents without quality embeddings get skipped during Phase 1 blend\n   - Consistency check at open(): verify doc_count matches, log WARN if mismatch\n\n3. Memory Pressure:\n   - Two memory-mapped indices (256d fast + 384d quality) for each data directory\n   - For 100K docs: fast ~49MB + quality ~73MB = 122MB total (f16)\n   - OnceLock prevents double-loading but also prevents releasing memory\n   - Future: consider LRU eviction for multi-directory deployments\n   - Document memory requirements in INFO log at open()\n\n4. File Locking:\n   - Use advisory file locks (flock) during write operations\n   - Readers do not lock (mmap provides read isolation)\n   - Writer holds exclusive lock during rebuild, released on commit\n   - Stale lock detection: check lock age, warn if > 5 minutes\n\n5. Error Recovery:\n   - Missing quality index: degrade to fast-only mode (not an error)\n   - Corrupted fast index: return SearchError::IndexError, log ERROR\n   - Partial write (crash during rebuild): .tmp files cleaned up on next open()\n","created_at":"2026-02-13T20:44:50Z"},{"id":157,"issue_id":"bd-3un.23","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — LIGHT REVISION (OnceLock replacement):\n\nBEFORE:\n  - std::sync::OnceLock for thread-safe lazy index initialization\n\nAFTER:\n  - asupersync::sync::OnceCell for cancel-aware lazy initialization\n\nThe functional behavior is identical — initialize once, read many times. The asupersync version integrates with the Cx context:\n  - If cancelled while initializing (e.g., loading a large index), the init is cleanly aborted\n  - The cell remains uninitialized, and the next accessor retries\n\n  pub struct TwoTierIndex {\n      fast: asupersync::sync::OnceCell<Option<VectorIndex>>,\n      quality: asupersync::sync::OnceCell<Option<VectorIndex>>,\n      lexical: asupersync::sync::OnceCell<Option<LexicalIndex>>,\n  }\n\n  impl TwoTierIndex {\n      pub async fn fast_index(&self, cx: &Cx) -> asupersync::Result<Option<&VectorIndex>> {\n          self.fast.get_or_try_init(cx, || async {\n              load_vector_index(self.data_dir.join(\"fast.fsvi\")).await\n          }).await\n      }\n  }","created_at":"2026-02-13T21:06:26Z"},{"id":265,"issue_id":"bd-3un.23","author":"Dicklesworthstone","text":"REVIEW FIX — has_quality contradiction, Vec<bool> inefficiency, and canonical definition:\n\n1. has_quality CONTRADICTION: The body says has_quality is a field on TwoTierIndex. The revision says \"has_quality should be computed from EmbedderStack::quality().is_some(), not stored as a field.\" These contradict.\n\n   RESOLUTION: has_quality is a DERIVED property, not stored. It is computed as:\n   pub fn has_quality(&self) -> bool {\n       self.embedder_stack.quality().is_some()\n   }\n   This ensures consistency with the actual embedder state and avoids stale booleans.\n\n2. Vec<bool> INEFFICIENCY: If the body uses Vec<bool> anywhere for tracking per-document state, use a BitVec (from the `bitvec` crate) or a simple u64 bitfield for small sets. Vec<bool> wastes 7 bits per element.\n\n   RESOLUTION: For the \"which documents have quality embeddings\" tracking, use:\n   - bitvec::BitVec if the set is large (>64 documents)  \n   - u64 bitfield if small (unlikely for real indices)\n   - Or better: just check if the quality vector slab has an entry for that doc_id (presence in the quality FSVI = has quality embedding)\n\n3. OnceCell FOR LAZY INIT: Per asupersync migration, OnceLock → asupersync::sync::OnceCell for lazy initialization of the quality index. The OnceCell holds the quality VectorIndex and is populated on first quality search.\n\n4. CANONICAL TwoTierIndex DEFINITION:\n   pub struct TwoTierIndex {\n       fast_index: VectorIndex,           // Always present (hash or model2vec)\n       quality_index: OnceCell<VectorIndex>, // Lazily built on first quality search\n       lexical_index: Option<Box<dyn LexicalSearch>>, // None if lexical feature disabled\n       embedder_stack: EmbedderStack,     // Manages fast + quality embedders\n       config: TwoTierConfig,\n   }\n\n5. TEST REQUIREMENTS:\n   - Construction with fast-only: quality_index stays empty, has_quality() returns false\n   - Construction with fast+quality: has_quality() returns true after first quality search\n   - Lazy quality init: quality_index not populated until first search requesting quality\n   - Serialization: TwoTierIndex can be saved/loaded (delegates to VectorIndex save/load)\n   - Thread safety: TwoTierIndex is Send + Sync","created_at":"2026-02-13T21:56:07Z"}]}
{"id":"bd-3un.24","title":"Implement TwoTierSearcher with progressive iterator","description":"Implement the TwoTierSearcher — the main orchestrator that ties everything together into the progressive search experience. This is the crown jewel of frankensearch.\n\npub struct TwoTierSearcher<'a> {\n    index: &'a TwoTierIndex,\n    fast_embedder: Arc<dyn Embedder>,\n    quality_embedder: Option<Arc<dyn Embedder>>,\n    lexical_index: Option<&'a LexicalIndex>,\n    reranker: Option<&'a dyn Reranker>,\n    config: TwoTierConfig,\n}\n\nimpl<'a> TwoTierSearcher<'a> {\n    pub fn new(\n        index: &'a TwoTierIndex,\n        embedder_stack: &EmbedderStack,\n        config: TwoTierConfig,\n    ) -> Self;\n    \n    /// Set optional lexical index for hybrid search\n    pub fn with_lexical(self, index: &'a LexicalIndex) -> Self;\n    \n    /// Set optional reranker\n    pub fn with_reranker(self, reranker: &'a dyn Reranker) -> Self;\n    \n    /// Execute progressive search, yielding phases as an iterator.\n    /// \n    /// Usage:\n    ///   for phase in searcher.search('my query', 10) {\n    ///       match phase {\n    ///           SearchPhase::Initial { results, latency_ms } => {\n    ///               // Display fast results immediately (~15ms)\n    ///           }\n    ///           SearchPhase::Refined { results, latency_ms } => {\n    ///               // Update display with refined rankings (~160ms)\n    ///           }\n    ///           SearchPhase::RefinementFailed { error } => {\n    ///               // Keep showing initial results\n    ///           }\n    ///       }\n    ///   }\n    pub fn search(&self, query: &str, k: usize) -> TwoTierSearchIter<'_>;\n}\n\nIterator implementation:\n\nstruct TwoTierSearchIter<'a> {\n    searcher: &'a TwoTierSearcher<'a>,\n    query: String,\n    k: usize,\n    phase: u8,              // 0 = fast, 1 = quality, 2 = done\n    fast_results: Option<Vec<VectorHit>>,\n    lexical_results: Option<Vec<LexicalHit>>,\n}\n\nimpl Iterator for TwoTierSearchIter<'_> {\n    type Item = SearchPhase;\n    \n    fn next(&mut self) -> Option<SearchPhase> {\n        match self.phase {\n            0 => {\n                // Phase 0: Fast tier\n                let start = Instant::now();\n                \n                // 1. Embed query with fast model (~1ms)\n                let query_vec = self.searcher.fast_embedder.embed(&self.query)?;\n                \n                // 2. Search fast index (~10ms)\n                let candidates = candidate_count(self.k, 0);\n                let fast_hits = self.searcher.index.search_fast(&query_vec, candidates);\n                \n                // 3. Optional: lexical search in parallel (if available)\n                let lexical_hits = self.searcher.lexical_index.map(|li| li.search(&self.query, candidates));\n                \n                // 4. Fuse results\n                let results = if let Some(lh) = &lexical_hits {\n                    rrf_fuse(lh, &fast_hits, self.k, 0, &self.searcher.config.rrf_config())\n                } else {\n                    fast_hits.into_scored_results()\n                };\n                \n                self.fast_results = Some(fast_hits);\n                self.lexical_results = lexical_hits;\n                self.phase = if self.searcher.quality_embedder.is_some() && !self.searcher.config.fast_only { 1 } else { 2 };\n                \n                Some(SearchPhase::Initial { results, latency_ms: start.elapsed().as_millis() as u64 })\n            }\n            1 => {\n                // Phase 1: Quality refinement\n                let start = Instant::now();\n                \n                let quality_embedder = self.searcher.quality_embedder.as_ref().unwrap();\n                \n                // 1. Embed query with quality model (~128ms)\n                let query_vec = match quality_embedder.embed(&self.query) {\n                    Ok(v) => v,\n                    Err(e) => {\n                        self.phase = 2;\n                        return Some(SearchPhase::RefinementFailed { error: e });\n                    }\n                };\n                \n                // 2. Get quality scores for top candidates\n                let fast = self.fast_results.as_ref().unwrap();\n                let max_refine = self.searcher.config.max_refinement_docs.min(fast.len());\n                let candidate_indices: Vec<usize> = fast[..max_refine].iter().map(|h| h.index).collect();\n                let quality_scores = self.searcher.index.quality_scores_for_indices(&query_vec, &candidate_indices);\n                \n                // 3. Blend fast + quality scores\n                let blended = blend_scored(fast, &quality_scores, self.searcher.config.quality_weight, max_refine);\n                \n                // 4. Re-fuse with lexical if available\n                let results = if let Some(lh) = &self.lexical_results {\n                    rrf_fuse(lh, &blended, self.k, 0, &self.searcher.config.rrf_config())\n                } else {\n                    blended.into_scored_results()\n                };\n                \n                // 5. Optional reranking\n                // (if reranker available, rerank top results)\n                \n                self.phase = 2;\n                Some(SearchPhase::Refined { results, latency_ms: start.elapsed().as_millis() as u64 })\n            }\n            _ => None,\n        }\n    }\n}\n\nThis is the most complex component. The key insight is that the iterator pattern allows callers to process fast results IMMEDIATELY while the quality refinement continues. A TUI can display initial results and then smoothly update when refinement completes. An API can return fast results first and stream refinements via SSE.\n\nPerformance budget:\n- Phase 0 (Initial): < 15ms total\n- Phase 1 (Refined): < 200ms total\n\nReference:\n- cass: src/search/two_tier_search.rs (TwoTierIndex, TwoTierSearcher, SearchPhase)\n- xf: src/main.rs lines 1538-1705 (two-tier search implementation)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs (TwoTierSearchIter)","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T17:52:20.217498515Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:16.565724675Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase7","progressive","search","two-tier"],"dependencies":[{"issue_id":"bd-3un.24","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:20.217498515Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T17:55:29.973692711Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T21:10:30.392509841Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T17:55:30.048852207Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.21","type":"blocks","created_at":"2026-02-13T17:55:30.129420997Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T17:55:30.208957915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.26","type":"blocks","created_at":"2026-02-13T21:10:29.174881517Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.41","type":"blocks","created_at":"2026-02-13T21:13:28.730825277Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T21:48:41.217584711Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.43","type":"blocks","created_at":"2026-02-13T20:23:47.195037129Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.9","type":"blocks","created_at":"2026-02-13T17:55:29.898210902Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":2,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"CRITICAL PATH: This is the single most important component. The TwoTierSearcher is what consumers interact with. It must be:\n\n1. EASY TO USE: One-liner setup with auto-detection\n2. CORRECT: Iterator contract must be exact (Initial → Refined → done)\n3. FAST: Phase 0 must complete in < 15ms\n4. RESILIENT: Quality failure → graceful degradation to fast results\n5. COMPOSABLE: Optional lexical index, optional reranker\n\nThe iterator pattern is the key innovation — it lets callers process each phase independently:\n- TUI: display fast results, then animate ranking changes\n- HTTP API: return fast results, stream refinements via SSE\n- CLI: print fast results, then update display\n- Batch: just collect all phases\n\nThe blend factor of 0.7 (70% quality, 30% fast) was empirically chosen. Too high and fast rankings are ignored; too low and quality model adds little value. 0.7 is the sweet spot where quality dominates but fast provides a useful prior that smooths rankings.","created_at":"2026-02-13T17:56:21Z"},{"id":17,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. TWOTIERMETRICS: Add comprehensive metrics recording for each search operation. From xf hybrid.rs:\n\npub struct TwoTierMetrics {\n    pub query_len: usize,\n    pub k: usize,\n    pub phase1_latency_ms: u64,\n    pub phase2_latency_ms: Option<u64>,\n    pub phase1_candidates: usize,\n    pub phase2_candidates: Option<usize>,\n    pub blend_factor: f32,\n    pub kendall_tau: Option<f64>,     // Rank correlation between phases\n    pub promoted: usize,              // Docs that moved up in refined ranking\n    pub demoted: usize,               // Docs that moved down\n    pub stable: usize,                // Docs that stayed in place\n    pub skip_reason: Option<SkipReason>,\n    pub embedder_fast: String,\n    pub embedder_quality: Option<String>,\n    pub query_class: QueryClass,      // From bd-3un.43\n}\n\npub enum SkipReason {\n    HighConfidence,       // Fast results already high quality\n    Timeout,              // Quality model exceeded budget\n    IndexNotReady,        // Quality index not yet built\n    EmbedderUnavailable,  // Quality embedder not loaded\n    EmbeddingFailed,      // Quality embedding failed\n    FastOnly,             // Config set to fast-only mode\n}\n\nThe metrics should be:\n- Returned alongside SearchPhase results (added to Refined phase)\n- Optionally logged via tracing at INFO level\n- Optionally appended to a JSONL file for offline analysis (like xf's two_tier_metrics.jsonl)\n\n2. QUERY CANONICALIZATION: Before embedding the query, apply canonicalize_query() from bd-3un.42. This should be configurable (TwoTierConfig.canonicalize_queries: bool, default true).\n\n3. CONTENT HASH: When returning ScoredResult, optionally include the content_hash from the vector index (if available) for downstream dedup.\n","created_at":"2026-02-13T20:24:24Z"},{"id":38,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (TwoTierSearcher)\n\n## The Crown Jewel Deserves Alien-Artifact Quality\n\nThe TwoTierSearcher is the consumer-facing API. It should embody all six alien-artifact characteristics.\n\n### 1. Expected Loss Minimization for Phase Decisions\n\nInstead of always running Phase 1 (quality refinement), use expected loss to decide:\n\n  L(skip_quality, quality_would_help) = quality_improvement_expected\n  L(skip_quality, quality_would_not_help) = 0\n  L(run_quality, quality_would_help) = -quality_improvement_expected + quality_latency_cost\n  L(run_quality, quality_would_not_help) = quality_latency_cost\n\n  P(quality_helps | query_features) = logistic(beta_0 + beta_1*query_len + beta_2*rank_spread + ...)\n\n  action* = argmin_a sum_s L(a,s) * P(s|query_features)\n\nThis means: for queries where the fast tier already produces tight rankings (rank_spread is small), SKIP the quality tier entirely. For queries where rankings are spread out, quality refinement is worth the 128ms.\n\nThis can save 50-70% of quality model invocations with < 1% quality loss.\n\n### 2. Galaxy-Brain Transparency Layer\n\nAdd optional \"explain\" mode that shows the math:\n\n  let results = searcher.search_explained(\"rust async\", 10);\n  // Returns SearchPhaseExplained with:\n  //   fast_embed_latency_us: 570\n  //   fast_scores: [(doc_3, 0.89), (doc_7, 0.85), ...]\n  //   quality_embed_latency_us: 128000\n  //   quality_scores: [(doc_7, 0.92), (doc_3, 0.87), ...]\n  //   blend_computation: \"0.3 * 0.89 + 0.7 * 0.87 = 0.876 (doc_3)\"\n  //   rrf_computation: \"1/(60+0) + 1/(60+2) = 0.0328 (doc_7 from both sources)\"\n  //   rank_changes: [(doc_7, +2), (doc_3, -1)]\n  //   kendall_tau: 0.73\n  //   decision: \"quality refinement improved NDCG by estimated 0.12\"\n\nThis makes the sophisticated math ACCESSIBLE, not intimidating.\n\n### 3. Performance: Speculative Quality Embedding\n\nStart the quality embedding IMMEDIATELY when the query arrives, in parallel with the fast tier:\n\n  // Current (sequential):\n  Phase 0: fast_embed → fast_search → fuse → yield Initial\n  Phase 1: quality_embed → blend → yield Refined\n\n  // Optimized (speculative parallel):\n  Phase 0: fast_embed → fast_search → fuse → yield Initial\n             |\n             + quality_embed starts HERE (spawn_blocking or rayon)\n  Phase 1: quality_embed already done → blend → yield Refined\n\nThis can reduce Phase 1 latency from ~140ms to ~20ms (quality embed runs during Phase 0 search).\n\nImplementation: spawn quality embedding on rayon thread pool in the iterator constructor. By the time Phase 0 finishes (~15ms), quality embedding (~128ms) has been running for 15ms already. Phase 1 just waits for the remaining ~113ms instead of all 128ms.\n\nCAVEAT: This trades latency for CPU usage. Only do this when quality_only is false and quality embedder is available. Add a config flag: speculative_quality (default: true).\n\n### 4. Formal Latency SLO\n\nDefine latency service-level objectives with formal monitoring:\n\n  pub struct LatencySLO {\n      phase_0_p99_ms: u64,  // Default: 20ms\n      phase_1_p99_ms: u64,  // Default: 250ms\n      total_p99_ms: u64,    // Default: 300ms\n  }\n\nTrack percentiles using a P2 quantile estimator (no-alloc, O(1) per observation, 5 markers):\n\n  pub struct P2Quantile {\n      markers: [f64; 5],     // Position markers\n      positions: [f64; 5],   // Desired positions\n      heights: [f64; 5],     // Marker heights (quantile estimates)\n  }\n\nThis gives accurate p50/p90/p99 estimates with ZERO allocations and O(1) per update. The P2 algorithm (Jain & Chlamtac 1985) is provably convergent.\n\n### 5. Isomorphism Note\n\nSpeculative quality embedding does NOT change results — the quality embedding is the same regardless of when it starts. Only latency changes. Golden outputs: sha256 identical.\n","created_at":"2026-02-13T20:32:42Z"},{"id":105,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"ARCHITECTURE NOTE: Lexical Search Integration Across Crate Boundaries\n\nThe TwoTierSearcher lives in frankensearch-fusion. It needs to call lexical search\n(Tantivy) for hybrid mode. But Tantivy is in frankensearch-lexical behind the\n`lexical` feature flag. This creates a cross-crate integration challenge.\n\nSOLUTION: Optional crate dependency with conditional compilation.\n\nIn frankensearch-fusion/Cargo.toml:\n  [dependencies]\n  frankensearch-lexical = { path = \"../frankensearch-lexical\", optional = true }\n\n  [features]\n  lexical = [\"frankensearch-lexical\"]\n\nIn TwoTierSearcher:\n  #[cfg(feature = \"lexical\")]\n  use frankensearch_lexical::LexicalIndex;\n\n  pub struct TwoTierSearcher {\n      // ...\n      #[cfg(feature = \"lexical\")]\n      lexical_index: Option<LexicalIndex>,\n  }\n\nWhen `lexical` feature is OFF:\n  - TwoTierSearcher does semantic-only search (fast + quality, no RRF with lexical)\n  - Phase 0: fast_embed -> fast_search -> yield Initial\n  - Phase 1: quality_embed -> blend -> yield Refined\n  - No Tantivy dependency compiled in\n\nWhen `lexical` feature is ON:\n  - TwoTierSearcher does hybrid search (semantic + lexical + RRF)\n  - Phase 0: fast_embed -> fast_search -> lexical_search -> RRF fuse -> yield Initial\n  - Phase 1: quality_embed -> blend -> re-fuse -> yield Refined\n\nThe facade (bd-3un.30) activates features based on consumer's Cargo.toml.\nThe consumer simply writes: frankensearch = { features = [\"hybrid\"] }\nand gets both semantic and lexical automatically.\n\nIMPORTANT: The auto() constructor must detect whether a Tantivy index exists\nin data_dir and set lexical_index accordingly. If the lexical feature is enabled\nbut no Tantivy index exists, log WARN and proceed with semantic-only mode.\n","created_at":"2026-02-13T20:57:40Z"},{"id":148,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MODERATE REVISION (replaces rayon spawn for quality embedding):\n\nThe TwoTierSearcher's parallel quality embedding changes from rayon::spawn to asupersync structured concurrency with join/race combinators. The progressive iterator pattern is preserved but gains cancel-correctness and deterministic testing.\n\nBEFORE (rayon):\n  - rayon::spawn(|| quality_embedder.embed(query)) during Phase 0\n  - Arc<dyn Embedder> shared across rayon thread pool\n  - No cancellation: if quality embedding is slow, it runs to completion\n  - No budget enforcement on quality embedding\n\nAFTER (asupersync):\n  - cx.region(|scope| { scope.spawn(quality_embed); scope.spawn(fast_search); })\n  - asupersync::combinator::join(fast_search, quality_embed) for parallel execution\n  - asupersync::combinator::timeout(quality_embed, deadline) for bounded latency\n  - Cancel-correct: if quality embedding exceeds budget, it's cancelled cleanly\n  - Deterministic testing: LabRuntime reproduces exact search behavior\n\nREVISED PROGRESSIVE SEARCH:\n\npub struct TwoTierSearcher<'a> {\n    index: &'a TwoTierIndex,\n    fast_embedder: Arc<dyn Embedder>,\n    quality_embedder: Option<Arc<dyn Embedder>>,\n    lexical_index: Option<&'a LexicalIndex>,\n    reranker: Option<&'a dyn Reranker>,\n    config: TwoTierConfig,\n}\n\nimpl<'a> TwoTierSearcher<'a> {\n    /// Execute progressive search with asupersync structured concurrency.\n    pub async fn search(&self, cx: &Cx, query: &str, k: usize) -> impl Stream<Item = SearchPhase> {\n        // Phase 0: Fast search (parallel fast embed + lexical)\n        let (fast_results, lexical_results) = asupersync::combinator::join(\n            |cx| self.fast_search(cx, query, k),\n            |cx| self.lexical_search(cx, query, k),\n        ).await;\n\n        // Yield Initial results immediately\n        yield SearchPhase::Initial { results: fuse(fast_results, lexical_results), latency_ms };\n\n        // Phase 1: Quality refinement with timeout budget\n        if let Some(quality_embedder) = &self.quality_embedder {\n            let quality_budget = Budget {\n                deadline: Some(cx.now() + Duration::from_millis(self.config.quality_timeout_ms)),\n                ..Default::default()\n            };\n\n            match asupersync::combinator::timeout(\n                |cx| self.quality_refine(cx, query, k, &fast_results),\n                quality_budget.deadline.unwrap(),\n            ).await {\n                Outcome::Ok(refined) => yield SearchPhase::Refined { results: refined, latency_ms },\n                Outcome::Cancelled(reason) => yield SearchPhase::RefinementFailed {\n                    error: SearchError::QualityTimeout(reason),\n                },\n                Outcome::Err(e) => yield SearchPhase::RefinementFailed { error: e },\n                Outcome::Panicked(_) => yield SearchPhase::RefinementFailed {\n                    error: SearchError::InternalError(\"quality embedding panicked\"),\n                },\n            }\n        }\n    }\n}\n\nSPECULATIVE PARALLEL QUALITY EMBEDDING (from alien-artifact comment):\n  - Start quality embedding at search entry (not after Phase 0 completes)\n  - Use asupersync::combinator::race semantics: if fast search is sufficient, cancel quality early\n\n  // Speculative version:\n  cx.region(|scope| async {\n      let quality_handle = scope.spawn(|cx| quality_embed(cx, query));\n      let fast_results = fast_search(cx, query, k).await;\n      yield SearchPhase::Initial { results: fast_results };\n\n      // Quality may already be partially done (ran in parallel)\n      match quality_handle.join(cx).await {\n          Outcome::Ok(quality) => { /* blend and yield Refined */ },\n          _ => { /* yield RefinementFailed */ },\n      }\n  }).await;\n\nPHASE GATE INTEGRATION (bd-2ps):\n  - Before spawning quality embedding, check PhaseGate.decision\n  - If SkipQuality: don't spawn quality task at all (saves the region overhead)\n  - If None: spawn speculatively with timeout budget\n\nNOTE ON RAYON:\n  - Rayon is RETAINED for bd-3un.15 (brute-force top-k vector search) because:\n    - Vector dot products are CPU-bound, embarrassingly parallel\n    - Rayon's work-stealing is optimal for data parallelism\n    - asupersync's concurrency is for I/O-bound and structured async work\n  - The key distinction: rayon = data parallelism, asupersync = task/structured concurrency\n  - These compose: asupersync tasks can internally use rayon for CPU-bound work\n\nDEPENDENCY CHANGES:\n  - ADD: asupersync (for Cx, region, scope, join, timeout, Outcome)\n  - KEEP: rayon (for vector search data parallelism in bd-3un.15)\n  - Arc<dyn Embedder> stays (Send + Sync required for cross-task sharing)","created_at":"2026-02-13T21:06:09Z"},{"id":188,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"REVISION (review pass 4 - dependency and architecture verification):\n\n1. MISSING DEPENDENCY: bd-3un.26 (rerank step). The TwoTierSearcher orchestrates the full Phase 1 pipeline: quality_embed -> blend -> RERANK -> yield Refined. Without the rerank step being implemented, the reranking code path in the searcher can't be written. Add dependency on bd-3un.26.\n\n2. MISSING DEPENDENCY: bd-3un.18 (Tantivy query parsing). The TwoTierSearcher performs HYBRID search, which requires lexical results from Tantivy. The search pipeline is:\n   a) Query the lexical index (Tantivy/FTS5) for BM25 results\n   b) Query the vector index for semantic results\n   c) Feed both into RRF fusion\n   Without bd-3un.18, the lexical code path can't be implemented. This dependency should be added even though lexical is feature-gated, because the searcher ORCHESTRATES the lexical path.\n\n3. LEXICAL INDEX INTEGRATION: The TwoTierSearcher needs a reference to the lexical index (Tantivy or FTS5). Currently TwoTierIndex (bd-3un.23) only manages vector indices. The searcher constructor should accept:\n   pub struct TwoTierSearcher {\n       index: TwoTierIndex,                       // Vector indices (fast + quality)\n       lexical: Option<Box<dyn LexicalIndex>>,     // Tantivy or FTS5 (feature-gated)\n       reranker: Option<Box<dyn Reranker>>,         // Cross-encoder (optional)\n       embedder_stack: EmbedderStack,\n       config: TwoTierConfig,\n       canonicalizer: Box<dyn Canonicalizer>,\n       query_classifier: QueryClassifier,\n       metrics: Arc<TwoTierMetrics>,\n   }\n   The lexical index is SEPARATE from TwoTierIndex because:\n   - It has a different lifecycle (Tantivy writer needs explicit commit)\n   - It may be FTS5 instead of Tantivy (different implementation)\n   - When lexical feature is disabled, it's simply None\n\n4. ASUPERSYNC CONFORMANCE: Per the project mandate (MEMORY.md), the TwoTierSearcher.search() method should be async and take a Cx parameter:\n   pub async fn search(&self, cx: &Cx, query: &str, k: usize) -> impl Stream<Item = SearchPhase>\n   The progressive iterator becomes an async stream (asupersync streams, not futures::Stream).\n   Phase 0 (fast search) and Phase 1 (quality refinement) run as scoped tasks within a region.\n","created_at":"2026-02-13T21:10:24Z"},{"id":237,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"REVISION (review pass 7 - missing canonicalization dependency):\n\nADDED bd-3un.42 (canonicalization) as a blocking dependency. The architecture diagram shows:\n  Query → Canonicalize → Classify → Fast Embed / Lexical Search → ...\n\nThe TwoTierSearcher is the pipeline orchestrator. It already depends on bd-3un.43 (query classification) but was MISSING the canonicalization step that should precede it. Without canonicalization:\n- Unicode variants produce different embeddings for the same text\n- Markdown formatting noise degrades embedding quality\n- Excessive whitespace affects query classification token counts\n\nThe TwoTierSearcher.search() method should call canonicalize_query() BEFORE classify() and embed():\n  let canon_query = self.canonicalizer.canonicalize_query(query);\n  let query_class = QueryClass::classify(&canon_query);\n  let budget = CandidateBudget::derive(k, &self.config, &canon_query);\n  let fast_vec = self.fast_embedder.embed(cx, &canon_query).await?;\n","created_at":"2026-02-13T21:49:02Z"},{"id":266,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"REVIEW FIX — async stream vs sync iterator, Phase 0 error handling, and canonical API:\n\n1. SYNC ITERATOR vs ASYNC STREAM CONTRADICTION: The body describes a sync Iterator<Item = SearchPhase>. The asupersync revision says \"NO sync iterators for the progressive protocol — use async stream or callback.\" These contradict.\n\n   RESOLUTION: The progressive search API uses an async callback pattern (not Iterator, not Stream):\n\n   impl TwoTierSearcher {\n       pub async fn search(\n           &self,\n           cx: &Cx,\n           query: &str,\n           config: &TwoTierConfig,\n           on_phase: impl FnMut(SearchPhase) + Send,\n       ) -> Result<TwoTierMetrics, SearchError> { ... }\n   }\n\n   The callback receives SearchPhase::Initial and SearchPhase::Refined (or RefinementFailed) as they become available. This is simpler than async streams and compatible with structured concurrency.\n\n   For simple \"just give me final results\" usage:\n   pub async fn search_blocking(\n       &self,\n       cx: &Cx,\n       query: &str,\n       config: &TwoTierConfig,\n   ) -> Result<(Vec<ScoredResult>, TwoTierMetrics), SearchError> { ... }\n\n2. PHASE 0 ERROR HANDLING: What happens if the FAST embedding fails? The body doesn't address this. \n\n   RESOLUTION: If fast embedding fails:\n   - If lexical search is available: return lexical-only results as Initial phase (degraded but functional)\n   - If lexical search is also unavailable: return SearchError::NoSearchBackend\n   - Log WARN with the fast embedding error for diagnostics\n   Never silently return empty results.\n\n3. TIMEOUT SEMANTICS: The revision says \"use asupersync::time::timeout()\" for quality phase timeout. Clarify:\n   - The quality phase timeout applies to: quality embedding + quality vector search + optional rerank\n   - If timeout fires: yield RefinementFailed { initial_results, reason: SkipReason::Timeout }\n   - The initial results from Phase 1 are ALWAYS available (they were already yielded)\n\n4. METRICS COLLECTION: TwoTierMetrics should be populated regardless of which phases complete:\n   pub struct TwoTierMetrics {\n       pub fast_embed_ms: f64,\n       pub fast_search_ms: f64,\n       pub lexical_search_ms: Option<f64>,\n       pub rrf_fusion_ms: f64,\n       pub quality_embed_ms: Option<f64>,\n       pub quality_search_ms: Option<f64>,\n       pub rerank_ms: Option<f64>,\n       pub total_ms: f64,\n       pub kendall_tau: Option<f64>,    // Only if both phases complete\n       pub skip_reason: Option<SkipReason>,\n       pub promoted: usize,\n       pub demoted: usize,\n       pub stable: usize,\n   }\n\n5. TEST REQUIREMENTS:\n   - Happy path: fast+quality both succeed, Initial and Refined phases yielded\n   - Fast-only mode: config.fast_only=true, only Initial phase yielded\n   - Quality timeout: quality phase exceeds timeout, RefinementFailed with initial_results\n   - Fast embed failure + lexical available: degraded Initial with lexical-only results\n   - Fast embed failure + no lexical: SearchError::NoSearchBackend\n   - Metrics populated: all timing fields are non-zero (or None for skipped phases)\n   - Kendall tau: verify tau computation between fast and refined rankings\n   - Cancel during quality: cx cancelled mid-quality, returns Outcome::Cancelled with initial_results intact\n   - Empty query: returns empty results immediately (no search phases)","created_at":"2026-02-13T21:56:16Z"}]}
{"id":"bd-3un.25","title":"Implement FlashRank cross-encoder reranker","description":"Implement the FlashRank nano cross-encoder reranker. Cross-encoders produce a single relevance score for a (query, document) pair by attending to both simultaneously, which is more accurate than bi-encoder cosine similarity but much slower (hence used as a second-pass reranker on top-k candidates only).\n\npub struct FlashRankReranker {\n    session: Mutex<ort::Session>,     // ONNX Runtime session\n    tokenizer: tokenizers::Tokenizer,\n    max_length: usize,                // 512 tokens\n    name: String,\n    model_dir: PathBuf,\n}\n\nimpl Reranker for FlashRankReranker {\n    fn rerank(&self, query: &str, documents: &[&str]) -> SearchResult<Vec<f32>> {\n        let session = self.session.lock()?;\n        let batch_size = 32;\n        let mut all_scores = Vec::new();\n        \n        for chunk in documents.chunks(batch_size) {\n            // 1. Tokenize (query, doc) pairs\n            // 2. Pad/truncate to max_length\n            // 3. Run ONNX inference\n            // 4. Extract logits → scores\n            all_scores.extend(chunk_scores);\n        }\n        Ok(all_scores)\n    }\n}\n\nModel: flashrank-nano (~4MB, MiniLM-distilled cross-encoder)\n- Very small model, fast inference\n- Good quality for reranking top-100 candidates\n\nAlternative model support:\n- ms-marco-MiniLM-L-6-v2 (baseline cross-encoder)\n- mxbai-rerank-xsmall-v1\n\nDependencies:\n- ort = '2.0.0-rc.9' (ONNX Runtime)\n- tokenizers = '0.21'\n\nFeature gating: Behind 'rerank' feature flag\nFile: frankensearch-rerank/src/flashrank.rs\n\nReference:\n- xf: src/flashrank_reranker.rs, src/mxbai_reranker.rs\n- cass: src/search/fastembed_reranker.rs (ms-marco model)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:52:46.655290201Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:14.335437342Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["flashrank","phase8","rerank"],"dependencies":[{"issue_id":"bd-3un.25","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:46.655290201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.25","depends_on_id":"bd-3un.4","type":"blocks","created_at":"2026-02-13T17:55:35.396759509Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.25","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:35.478072033Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":16,"issue_id":"bd-3un.25","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. SIGMOID ACTIVATION MISSING: The FlashRank ONNX model outputs raw logits, NOT probabilities. The reranker MUST apply sigmoid activation to produce meaningful scores:\n\n   fn sigmoid(x: f32) -> f32 { 1.0 / (1.0 + (-x).exp()) }\n   \n   After getting raw logits from ONNX session output, apply:\n   scores.iter().map(|&s| sigmoid(s)).collect()\n\n   Without sigmoid, the raw logits can be negative or arbitrarily large, making them useless for ranking. Reference: xf flashrank_reranker.rs applies sigmoid.\n\n2. OUTPUT TENSOR NAME FALLBACK: ONNX models use different output tensor names. Try in order: \"logits\", \"output\", \"sentence_embedding\". If none match, use the first output tensor by index. Reference: xf mxbai_reranker.rs has this fallback chain.\n\n3. ONNX SESSION CONFIG: Set GraphOptimizationLevel::Level3 and intra_threads = rayon thread count for best performance. Reference: xf flashrank_reranker.rs.\n","created_at":"2026-02-13T20:24:11Z"},{"id":75,"issue_id":"bd-3un.25","author":"Dicklesworthstone","text":"CRITICAL IMPLEMENTATION NOTE: Sigmoid Activation\n\nThe raw ONNX output from cross-encoder models is a LOGIT, not a probability.\nYou MUST apply sigmoid activation: score = 1.0 / (1.0 + (-logit).exp())\n\nWithout sigmoid:\n  - Logits range from roughly -10 to +10\n  - Comparing raw logits between different query-doc pairs is meaningless\n  - The reranker appears to work but produces garbage rankings\n\nWith sigmoid:\n  - Scores range from 0.0 to 1.0\n  - Scores are interpretable as P(relevant | query, document)\n  - Rankings become meaningful and comparable\n\nThis was a real bug in the source codebase and must not be repeated.\n\nAlso: ONNX output tensor name varies by model:\n  - \"logits\" (most common)\n  - \"output\"\n  - \"sentence_embedding\"\n  - Fallback: first output tensor by index\nUse a name fallback chain, not a hardcoded string.\n","created_at":"2026-02-13T20:46:28Z"},{"id":151,"issue_id":"bd-3un.25","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — LIGHT REVISION (Mutex for ONNX session):\n\nSame pattern as bd-3un.8 (FastEmbed). Replace std::sync::Mutex with asupersync::sync::Mutex for cancel-aware lock acquisition on the ort::Session.\n\nBEFORE: session: std::sync::Mutex<ort::Session>\nAFTER:  session: asupersync::sync::Mutex<ort::Session>\n\nBenefits: cancel-aware locking, no poison on panic (Outcome::Panicked instead), contention tracking via ContendedMutex option.","created_at":"2026-02-13T21:06:13Z"},{"id":280,"issue_id":"bd-3un.25","author":"Dicklesworthstone","text":"REVIEW FIX — Mutex concurrency, max_length, and tests:\n\n1. CONCURRENCY: The ONNX session is behind asupersync::sync::Mutex. Under high concurrency, this serializes all reranking requests. For V1 this is acceptable (reranking is already the slow path). For V2, consider a Pool of ONNX sessions for concurrent reranking.\n\n2. max_length CONFIGURATION: The maximum input length for the cross-encoder (typically 512 tokens) should be configurable via TwoTierConfig, not hardcoded. Add: pub rerank_max_length: usize (default 512).\n\n3. TEST REQUIREMENTS:\n   - Rerank ordering: known query + known docs → expected reranked order\n   - Score range: all scores in [0, 1] after sigmoid\n   - Empty input: empty candidates → empty result, no panic\n   - Single candidate: returns same candidate with rerank score\n   - Long document: document exceeding max_length is truncated, not errored\n   - Concurrent access: multiple rerank calls via asupersync tasks don't deadlock\n   - Model loading: FlashRank loads model file correctly (test with fixture model)\n   - Invalid model path: returns SearchError, not panic","created_at":"2026-02-13T21:59:14Z"}]}
{"id":"bd-3un.26","title":"Implement rerank step (pipeline integration)","description":"Implement the RerankStep that integrates reranking into the search pipeline. This is a composable step that takes top-k candidates and re-scores them for better relevance ordering.\n\npub struct RerankStep {\n    reranker: Box<dyn Reranker>,\n    top_k_rerank: usize,           // Default: 100 (only rerank top N)\n    min_candidates: usize,         // Default: 5 (skip if too few)\n}\n\nimpl RerankStep {\n    pub fn new(reranker: Box<dyn Reranker>) -> Self;\n    pub fn with_top_k(self, k: usize) -> Self;\n    pub fn with_min_candidates(self, min: usize) -> Self;\n    \n    /// Rerank candidates in-place.\n    /// Sets rerank_score on each candidate, then re-sorts by rerank_score.\n    pub fn rerank(&self, query: &str, candidates: &mut [ScoredResult]) -> SearchResult<()> {\n        if candidates.len() < self.min_candidates {\n            return Ok(());  // Skip: too few to benefit from reranking\n        }\n        \n        let rerank_count = candidates.len().min(self.top_k_rerank);\n        let texts: Vec<&str> = candidates[..rerank_count]\n            .iter()\n            .map(|r| r.text.as_str())\n            .collect();\n        \n        let scores = self.reranker.rerank(query, &texts)?;\n        \n        // Verify score count\n        assert_eq!(scores.len(), rerank_count);\n        \n        // Apply rerank scores\n        for (i, score) in scores.iter().enumerate() {\n            candidates[i].rerank_score = Some(*score);\n        }\n        \n        // Re-sort: reranked candidates by rerank_score (desc),\n        // then non-reranked by original score (desc)\n        candidates[..rerank_count].sort_by(|a, b| \n            b.rerank_score.unwrap().partial_cmp(&a.rerank_score.unwrap()).unwrap()\n        );\n    }\n}\n\nDesign decisions:\n- Only top-k candidates are reranked (bounded cost)\n- Candidates below top-k keep their original scores\n- Minimum threshold prevents wasted effort on tiny result sets\n- Logging: model name, candidate count, score range, elapsed time\n\nFile: frankensearch-rerank/src/step.rs\n\nReference:\n- xf: src/rerank_step.rs (RerankStep, DEFAULT_TOP_K_RERANK=100, DEFAULT_MIN_CANDIDATES=5)\n- cass: same pattern in search pipeline","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:52:46.733177577Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:47.565154437Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase8","pipeline","rerank"],"dependencies":[{"issue_id":"bd-3un.26","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:46.733177577Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.26","depends_on_id":"bd-3un.25","type":"blocks","created_at":"2026-02-13T17:55:35.561429909Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":108,"issue_id":"bd-3un.26","author":"Dicklesworthstone","text":"REVISION: Rerank Step Pipeline Integration\n\nThe RerankStep is a composable pipeline stage. Design details:\n\n1. Pipeline Position:\n   Phase 1: quality_embed -> blend -> RerankStep -> final_fuse -> yield Refined\n   The reranker sees the blended results, not raw embeddings.\n   This means it re-scores the already-ranked candidates.\n\n2. Error Handling:\n   - Reranker model not loaded: SKIP (return input unchanged, log WARN)\n   - ONNX inference failure: SKIP (return input unchanged, log ERROR)\n   - Batch size exceeded: chunk into batches of 32, concatenate results\n   - Empty input: return empty (no-op)\n   NEVER let a reranker failure prevent search results from being returned.\n\n3. Score Integration:\n   - RerankStep sets rerank_score on each ScoredResult\n   - Then re-sorts by rerank_score descending\n   - The original fusion_score is preserved for comparison/debugging\n   - Kendall's tau between pre-rerank and post-rerank order: log at DEBUG\n\n4. Metrics (cross-reference bd-3un.39):\n   - rerank_duration_us: total reranking time\n   - rerank_candidates: number of candidates reranked\n   - rerank_model: model name used\n   - rerank_top_k_change: number of rank changes in top 10\n\n5. Configuration:\n   - top_k_rerank: 100 (only rerank top N candidates, rest kept as-is)\n   - min_candidates: 5 (skip reranking if fewer than N candidates)\n   - Configurable via TwoTierConfig.rerank_top_k and TwoTierConfig.rerank_min_candidates\n","created_at":"2026-02-13T20:57:43Z"},{"id":211,"issue_id":"bd-3un.26","author":"Dicklesworthstone","text":"REVISION (review pass 4 - asupersync conformance):\n\n1. RERANK METHOD IS ASYNC: Since Reranker.rerank() is now async (takes &Cx), the RerankStep.rerank() must also be async:\n\n   BEFORE:\n   pub fn rerank(&self, query: &str, candidates: &mut [ScoredResult]) -> SearchResult<()>\n\n   AFTER:\n   pub async fn rerank(&self, cx: &Cx, query: &str, candidates: &mut [ScoredResult]) -> asupersync::Outcome<(), SearchError>\n\n   The Cx enables cancellation during reranking (ONNX inference can take 10-50ms per batch of 32). If the parent search is cancelled, reranking stops and the pre-rerank results are used as-is.\n\n2. BATCH CHUNKING WITH CANCEL CHECKS: When reranking 100 candidates in chunks of 32:\n   for chunk in candidates.chunks_mut(batch_size) {\n       cx.checkpoint()?;  // Cancel check between chunks\n       let scores = self.reranker.rerank(cx, query, &texts).await?;\n       // apply scores...\n   }\n\n3. MUTEX IS asupersync::sync::Mutex: The reranker internally uses a Mutex for the ONNX session. Per mandate, this must be asupersync::sync::Mutex (not std::sync::Mutex), which supports cancel-aware acquisition:\n   let session = self.session.lock(cx).await?;  // Cancel-aware lock\n\n4. TEXT ACCESS: The current design accesses candidates[i].text.as_str() but ScoredResult may not carry the full text (it carries doc_id, score, metadata). The rerank step needs document text, which must be provided separately or looked up. Clarify: does the caller pass document texts alongside candidates, or does the rerank step look them up from the index?\n\n   Recommendation: The caller (TwoTierSearcher) passes texts alongside candidates:\n   pub async fn rerank(&self, cx: &Cx, query: &str, candidates: &mut [ScoredResult], texts: &[&str]) -> Outcome<(), SearchError>\n","created_at":"2026-02-13T21:13:07Z"},{"id":267,"issue_id":"bd-3un.26","author":"Dicklesworthstone","text":"REVIEW FIX — ScoredResult .text field, assert_eq panic, and error handling:\n\n1. ScoredResult HAS NO .text FIELD: The body assumes ScoredResult carries the full document text for reranking. Per bd-3un.5, ScoredResult has doc_id, score, source, metadata — NO text field. The reranker needs text, but ScoredResult doesn't have it.\n\n   RESOLUTION: The rerank step must retrieve text separately. Two options:\n   a) Accept a text lookup function: rerank(query, candidates, text_fn: impl Fn(&str) -> Option<String>)\n   b) Accept pre-fetched texts alongside candidates: rerank(query, candidates: &mut [ScoredResult], texts: &[&str])\n   \n   Option (a) is more flexible — the caller provides a closure that looks up text by doc_id (from Tantivy stored fields, or from the original document store). This avoids loading all texts upfront.\n\n   CANONICAL SIGNATURE:\n   pub async fn rerank_step(\n       cx: &Cx,\n       reranker: &dyn SendReranker,\n       query: &str,\n       candidates: &mut Vec<ScoredResult>,\n       text_fn: impl Fn(&str) -> Option<String> + Send + Sync,\n       min_candidates: usize,\n   ) -> Result<(), SearchError>\n\n2. assert_eq PANICS IN PRODUCTION: The body uses assert_eq!(scores.len(), rerank_count) which panics. For a library crate, this MUST be a proper error return.\n\n   RESOLUTION: Replace with:\n   if scores.len() != candidates.len() {\n       tracing::warn!(expected = candidates.len(), got = scores.len(), \"reranker score count mismatch — skipping rerank\");\n       return Ok(()); // Skip rerank, preserve original ranking\n   }\n\n3. ERROR HANDLING — SKIP ON FAILURE: The revision says \"NEVER let a reranker failure prevent search results.\" Implement this inside the step:\n   - If reranker returns Err: log WARN, return Ok(()) with candidates unchanged\n   - If text_fn returns None for a candidate: skip that candidate in reranking, keep its original score\n   - If fewer than min_candidates have text available: skip reranking entirely\n\n4. TEST REQUIREMENTS:\n   - Happy path: reranker reorders candidates correctly\n   - Missing text: text_fn returns None for some docs — those keep original scores\n   - Reranker failure: reranker returns Err — candidates unchanged, no panic\n   - Score count mismatch: reranker returns wrong number of scores — candidates unchanged\n   - min_candidates threshold: fewer candidates than threshold — skip reranking\n   - Empty candidates: no-op, returns Ok(())\n   - Score update: after rerank, ScoredResult.rerank_score is populated","created_at":"2026-02-13T21:56:47Z"}]}
{"id":"bd-3un.27","title":"Implement embedding job queue with backpressure","description":"Implement a background embedding job queue for incremental index building. When new documents arrive, they're queued for embedding and the index is updated in the background.\n\npub struct EmbeddingQueue {\n    config: EmbeddingJobConfig,\n    pending: Mutex<QueueState>,\n}\n\npub struct EmbeddingJobConfig {\n    pub batch_size: usize,               // Default: 32\n    pub flush_interval_ms: u64,          // Default: 5000ms\n    pub backpressure_threshold: usize,   // Default: 1000 pending\n    pub max_retries: u32,                // Default: 3\n    pub retry_base_delay_ms: u64,        // Default: 100ms (exponential backoff)\n}\n\npub struct EmbeddingRequest {\n    pub doc_id: String,\n    pub text: String,\n    pub metadata: Option<serde_json::Value>,\n    pub submitted_at: Instant,\n}\n\nimpl EmbeddingQueue {\n    pub fn enqueue(&self, request: EmbeddingRequest) -> Result<(), BackpressureError>;\n    pub fn drain_batch(&self, max: usize) -> Vec<EmbeddingRequest>;\n    pub fn pending_count(&self) -> usize;\n}\n\npub struct EmbeddingJobRunner {\n    queue: Arc<EmbeddingQueue>,\n    embedder: Arc<dyn Embedder>,\n    index_writer: Arc<Mutex<VectorIndexWriter>>,\n}\n\nimpl EmbeddingJobRunner {\n    pub fn process_batch(&self) -> SearchResult<usize>;\n}\n\nBackpressure: if pending >= threshold, drop new requests and log warning\nDeduplication: same doc_id replaces older request (keeps latest text)\nRetry: exponential backoff (100ms → 200ms → 400ms), up to 3 retries\n\nReference: agent-mail crates/mcp-agent-mail-search-core/src/embedding_jobs.rs lines 225-420","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:53:06.274252563Z","created_by":"ubuntu","updated_at":"2026-02-13T21:58:18.219705555Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["background","phase9","queue"],"dependencies":[{"issue_id":"bd-3un.27","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:06.274252563Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.27","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:35.643322319Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.27","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T20:23:21.207347625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.27","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:35.721098919Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":22,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. CANONICALIZATION INTEGRATION: The EmbeddingJobRunner MUST canonicalize text before embedding. From agent-mail embedding_jobs.rs process_batch_limit() line 568:\n\n   let canonical = canonicalizer.canonicalize(&request.text);\n   if canonical.is_empty() {\n       // Skip low-signal content (returns JobResult::Skipped)\n       continue;\n   }\n   let embedding = embedder.embed(&canonical)?;\n\nThe queue should accept a Canonicalizer instance at construction time.\n\n2. CONTENT HASH FOR DEDUP: Compute SHA-256 of canonicalized text BEFORE embedding. Store alongside the embedding for change detection:\n   - If doc_id exists in queue with same content_hash, skip re-embedding\n   - If doc_id exists with different content_hash, replace (re-embed)\n   This prevents wasted embedding computation when text hasn't changed.\n\n3. HASH-ONLY SKIP: From agent-mail embedding_jobs.rs line 664: Skip upserting hash-only embeddings to the vector index. Hash embeddings are computed on-the-fly during search and don't need to be stored.\n\n4. JOB METRICS: Add atomic counters (from agent-mail JobMetrics):\n   total_succeeded, total_retryable, total_failed, total_skipped\n   total_batches, total_embed_time_us, total_docs_embedded\n   All AtomicU64 with Ordering::Relaxed for lock-free reads.\n","created_at":"2026-02-13T20:25:47Z"},{"id":32,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (Embedding Job Queue)\n\n## Mathematical Upgrade: From Fixed Threshold to Queueing Theory + Adaptive Rate Control\n\nCurrent design uses a fixed backpressure_threshold of 1000 pending items. This is ad-hoc and either too aggressive (drops work unnecessarily) or too permissive (allows memory pressure).\n\n### 1. Little's Law for Steady-State Analysis\n\nLittle's Law: L = λW (queue length = arrival rate × service time)\n\nAt steady state, if embeddings arrive at λ docs/sec and each takes W seconds to process:\n  expected_queue_depth = λ × W\n\nFor MiniLM at 128ms per doc (batch of 32 → ~4ms amortized):\n  If λ = 100 docs/sec, expected queue = 100 × 0.004 = 0.4 (no backpressure needed)\n  If λ = 10,000 docs/sec (bulk indexing), expected queue = 10,000 × 0.004 = 40\n\nThe OPTIMAL threshold is a function of arrival rate, not a fixed constant.\n\n### 2. AIMD Adaptive Backpressure (Additive Increase, Multiplicative Decrease)\n\nBorrow from TCP congestion control:\n\n  pub struct AdaptiveBackpressure {\n      window: AtomicUsize,      // Current admission window\n      min_window: usize,        // Floor (default: 32)\n      max_window: usize,        // Ceiling (default: 10_000)\n      increase_step: usize,     // Additive increase (default: 16)\n      decrease_factor: f32,     // Multiplicative decrease (default: 0.5)\n  }\n\n  // On successful batch completion: window += increase_step\n  // On memory pressure detected: window *= decrease_factor\n\nThis automatically adapts to the system's capacity. During bulk indexing, the window opens up. When memory is tight, it contracts. Provably converges to the optimal operating point under stationary conditions.\n\n### 3. Token Bucket for Smooth Rate Limiting\n\nInstead of hard rejection when queue is full, use a token bucket for smooth rate control:\n\n  pub struct TokenBucket {\n      tokens: AtomicF64,\n      max_tokens: f64,         // Burst capacity\n      refill_rate: f64,        // Tokens per second\n      last_refill: Instant,\n  }\n\n  impl TokenBucket {\n      pub fn try_acquire(&self) -> bool {\n          self.refill();\n          if self.tokens.load() >= 1.0 {\n              self.tokens.fetch_sub(1.0);\n              true\n          } else {\n              false\n          }\n      }\n  }\n\nThe refill_rate should be set to the measured embedding throughput (self-tuning: measure actual batch processing rate and update).\n\n### 4. Expected Loss Decision for Drop vs Queue\n\nWhen the queue is near capacity, use expected loss minimization to decide whether to drop or queue:\n\n  L(queue, system_ok) = latency_cost(queue_depth + 1)  // queuing adds latency\n  L(drop, system_ok)  = document_value                  // losing the document\n  L(queue, system_overloaded) = crash_cost              // OOM / degraded service\n  L(drop, system_overloaded) = 0                        // safe, no impact\n\n  action* = argmin_a Σ_s L(a,s) × P(s|queue_depth, memory_usage)\n\nThis gives a principled drop policy that accounts for document importance (if available via metadata priority field).\n","created_at":"2026-02-13T20:29:55Z"},{"id":147,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MAJOR REVISION (replaces Mutex + crossbeam channels):\n\nThe embedding job queue transforms from a Mutex-wrapped VecDeque with crossbeam channels into asupersync's two-phase mpsc channel with native backpressure and cancel-correct semantics.\n\nBEFORE (crossbeam + Mutex):\n  - pending: Mutex<VecDeque<EmbeddingJob>>\n  - crossbeam::channel::bounded(capacity) for dispatch\n  - Manual backpressure: if pending >= threshold, drop and log\n  - AtomicU64 for queue depth metrics\n  - AIMD rate control implemented manually\n\nAFTER (asupersync):\n  - asupersync::channel::mpsc::channel(capacity) — bounded, two-phase\n  - Backpressure via channel back-pressure (bounded capacity blocks sender)\n  - asupersync::combinator::rate_limit for AIMD-style throttling\n  - asupersync::combinator::bulkhead for concurrency isolation\n  - Cancel-safe: reserve/commit prevents data loss on cancellation\n\nREVISED ARCHITECTURE:\n\npub struct EmbeddingJobQueue {\n    sender: asupersync::channel::mpsc::Sender<EmbeddingJob>,\n    receiver: asupersync::channel::mpsc::Receiver<EmbeddingJob>,\n    rate_limiter: asupersync::combinator::RateLimiter,\n    metrics: QueueMetrics,\n}\n\nimpl EmbeddingJobQueue {\n    pub fn new(capacity: usize) -> Self {\n        let (sender, receiver) = asupersync::channel::mpsc::channel(capacity);\n        Self {\n            sender,\n            receiver,\n            rate_limiter: RateLimiter::new(RateLimitPolicy::token_bucket(100, Duration::from_secs(1))),\n            metrics: QueueMetrics::default(),\n        }\n    }\n\n    /// Enqueue a job (cancel-safe, two-phase).\n    /// Returns Err if queue is full (backpressure) or cancelled.\n    pub async fn enqueue(&self, cx: &Cx, job: EmbeddingJob) -> asupersync::Result<()> {\n        // Rate limiting (AIMD-style via token bucket)\n        self.rate_limiter.acquire(cx).await?;\n\n        // Two-phase send: reserve slot, then commit\n        let permit = self.sender.reserve(cx).await?;  // Blocks if at capacity\n        permit.send(job);  // Linear, infallible — no data loss possible\n        self.metrics.enqueued.fetch_add(1, Ordering::Relaxed);\n        Ok(())\n    }\n\n    /// Drain up to batch_size jobs (non-blocking).\n    pub fn drain_batch(&self, cx: &Cx, batch_size: usize) -> Vec<EmbeddingJob> {\n        let mut batch = Vec::with_capacity(batch_size);\n        for _ in 0..batch_size {\n            match self.receiver.try_recv() {\n                Ok(job) => batch.push(job),\n                Err(_) => break,\n            }\n        }\n        batch\n    }\n}\n\nBACKPRESSURE MODEL (replaces manual AIMD):\n  - Primary: bounded channel capacity (blocks sender when full)\n  - Secondary: RateLimiter with token bucket (smooths burst traffic)\n  - Tertiary: Budget enforcement on the region (deadline-based drop)\n  - On exhaustion: sender.reserve() returns Cancelled or ChannelFull error\n\nEXPECTED-LOSS DROP POLICY (from alien-artifact comment):\n  - When channel is full and budget is exhausted:\n    Actions: {enqueue, drop_oldest, drop_newest, expand_buffer}\n    Loss: drop_oldest loses stale work (loss 2); drop_newest loses fresh work (loss 5); expand_buffer risks OOM (loss 10)\n    Decision: drop_oldest (FIFO semantics preserved)\n  - This maps naturally to asupersync's bounded channel: oldest items are consumed first\n\nOBLIGATION TRACKING:\n  - Every SendPermit is a linear obligation\n  - Must be sent() or abort()ed — cannot be silently dropped\n  - Lab runtime's ObligationLeakOracle catches leaks automatically\n\nDEPENDENCY CHANGES:\n  - REMOVE: crossbeam-channel\n  - REMOVE: std::sync::Mutex for queue state\n  - ADD: asupersync::channel::mpsc\n  - ADD: asupersync::combinator::rate_limit (replaces manual AIMD)\n  - KEEP: AtomicU64 for metrics counters (fine, these are simple atomics)","created_at":"2026-02-13T21:06:08Z"},{"id":190,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"REVISION (review pass 4 - asupersync conformance):\n\n1. CHANNEL TYPE: Replace Mutex<QueueState> with asupersync::channel::mpsc for the embedding queue. The two-phase reserve/commit pattern fits perfectly:\n   - Producer reserves a slot (backpressure via bounded channel capacity)\n   - Producer commits the EmbeddingRequest (zero-copy if possible)\n   - Consumer claims batches via drain_batch()\n\n   BEFORE (prohibited):\n   pending: Mutex<QueueState>\n\n   AFTER (correct):\n   pending_tx: asupersync::channel::mpsc::Sender<EmbeddingRequest>,\n   pending_rx: asupersync::channel::mpsc::Receiver<EmbeddingRequest>,\n\n2. BACKPRESSURE: Instead of checking pending.len() >= threshold, use a bounded mpsc channel with capacity = backpressure_threshold. When the channel is full, enqueue() returns BackpressureError. This is more efficient than Mutex-guarded Vec because:\n   - No lock contention between producers and consumers\n   - Backpressure is enforced by the channel capacity itself\n   - Drain semantics are built into the receiver\n\n3. DEDUP IN CHANNEL: The \"same doc_id replaces older request\" dedup is harder with a channel (FIFO, no random access). Two options:\n   a) Accept duplicates in the channel, dedup at claim time (simpler, slightly more memory)\n   b) Use a HashMap<String, usize> alongside the channel for dedup lookup (more complex)\n   Option (a) is recommended: dedup at claim time with a seen HashSet. The rare duplicate costs one extra claim, which is negligible vs embedding time.\n\n4. EMBED METHODS ARE ASYNC: Since Embedder.embed() is now async (asupersync migration), the EmbeddingJobRunner.process_batch() must also be async:\n   pub async fn process_batch(&self, cx: &Cx) -> asupersync::Outcome<usize, SearchError>\n","created_at":"2026-02-13T21:10:26Z"},{"id":274,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"REVIEW FIX — BackpressureError type, dedup strategy, and test requirements:\n\n1. BackpressureError UNDEFINED: The body references BackpressureError but this type isn't defined in bd-3un.2. \n\n   RESOLUTION: Add as a SearchError variant:\n   SearchError::QueueFull { pending: usize, capacity: usize }\n   This communicates both the current state and the limit, enabling callers to make informed retry decisions.\n\n2. DEDUP STRATEGY RECONCILIATION: Body says \"same doc_id replaces older request.\" Asupersync revision says \"accept duplicates, dedup at claim time.\" These are different.\n\n   RESOLUTION: Use the body's approach (replace older request) since it saves embedding compute. Implementation:\n   - Maintain a HashSet<String> of pending doc_ids alongside the channel\n   - On submit: if doc_id already in pending set, the new text replaces the old (remove old from queue, add new)\n   - On claim: remove doc_id from pending set\n   - This requires a VecDeque<EmbedJob> instead of a raw channel for O(1) dedup lookup\n\n3. TWO-PHASE CHANNEL: Per asupersync, use reserve()/send() for the job channel to prevent data loss on cancellation:\n   let slot = tx.reserve(cx).await?;  // Blocks if queue full (backpressure)\n   slot.send(embed_job);              // Infallible after reservation\n\n4. TEST REQUIREMENTS:\n   - Submit and process: submit job, worker claims it, embedding produced\n   - Backpressure: fill queue to capacity, next submit returns QueueFull\n   - Deduplication: submit same doc_id twice, only last version is processed\n   - Content hash skip: submit doc with unchanged content hash, embedding skipped\n   - Cancel safety: cancel during embedding, no data loss (two-phase channel)\n   - Batch processing: submit 10 docs, worker processes in batches of configured size\n   - Graceful drain: on shutdown, all pending jobs are processed before exit","created_at":"2026-02-13T21:58:18Z"}]}
{"id":"bd-3un.28","title":"Implement index refresh worker (background thread)","description":"Implement a background worker thread that periodically processes the embedding queue and refreshes the vector index. This runs on a dedicated OS thread (not async).\n\npub struct IndexRefreshWorker {\n    config: RefreshWorkerConfig,\n    runner: Arc<EmbeddingJobRunner>,\n    shutdown: Arc<AtomicBool>,\n}\n\npub struct RefreshWorkerConfig {\n    pub refresh_interval_ms: u64,    // Default: 1000ms\n    pub max_docs_per_cycle: usize,   // Default: 1000\n}\n\nimpl IndexRefreshWorker {\n    pub fn new(config: RefreshWorkerConfig, runner: Arc<EmbeddingJobRunner>) -> Self;\n    \n    /// Run blocking loop on dedicated thread. Returns when shutdown signal received.\n    pub fn run(&self) {\n        loop {\n            if self.shutdown.load(Ordering::Acquire) { return; }\n            self.run_cycle();  // Process up to max_docs_per_cycle\n            std::thread::sleep(Duration::from_millis(self.config.refresh_interval_ms));\n        }\n    }\n    \n    /// Signal graceful shutdown.\n    pub fn shutdown(&self) {\n        self.shutdown.store(true, Ordering::Release);\n    }\n}\n\nUsage pattern:\nlet worker = Arc::new(IndexRefreshWorker::new(config, runner));\nlet w = worker.clone();\nstd::thread::spawn(move || w.run());  // Dedicated thread\n\n// Later:\nworker.shutdown();\n\nKey design: all search operations are synchronous (no tokio needed). The worker is the only background component, and it uses std::thread::sleep for simplicity. This avoids pulling in an async runtime for consumers that don't need one.\n\nReference: agent-mail crates/mcp-agent-mail-search-core/src/embedding_jobs.rs lines 732-847","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:53:06.349608297Z","created_by":"ubuntu","updated_at":"2026-02-13T21:56:56.125081651Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["background","phase9","worker"],"dependencies":[{"issue_id":"bd-3un.28","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:06.349608297Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.28","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T17:55:35.803372793Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.28","depends_on_id":"bd-3un.27","type":"blocks","created_at":"2026-02-13T17:55:35.885948744Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":47,"issue_id":"bd-3un.28","author":"Dicklesworthstone","text":"REVISION: Index Refresh Worker Details\n\n1. Graceful Shutdown Semantics:\n   - AtomicBool::store(true, Ordering::Release) signals shutdown\n   - Worker checks flag at top of each cycle, not mid-batch\n   - On shutdown signal: finish current batch, flush pending writes, then exit\n   - join() on worker thread with timeout (5s), then log WARN if exceeded\n   - Drop impl calls shutdown + join to prevent leaked threads\n\n2. Error Recovery:\n   - Failed embedding: log WARN, increment retry counter, re-queue with exponential backoff\n   - Failed index write: log ERROR, skip this cycle, retry next cycle\n   - Repeated failures (3+ consecutive): pause worker for 30s, log ERROR with stack\n   - Never panic in worker thread (catch_unwind wrapper with error logging)\n\n3. Metrics (cross-reference bd-3un.39 tracing):\n   - Counter: documents_embedded_total, documents_failed_total\n   - Gauge: queue_depth, worker_state (idle/processing/error/shutdown)\n   - Histogram: batch_duration_ms, docs_per_second\n   - Log at INFO per cycle: \"index_refresh_cycle docs={n} duration_ms={ms} queue_remaining={q}\"\n\n4. Integration Points:\n   - Receives jobs from bd-3un.27 (embedding job queue) via crossbeam channel\n   - Triggers reload on bd-3un.23 (TwoTierIndex) after successful write\n   - Reports staleness metrics to bd-3un.41 (staleness detection)\n   - Worker is the ONLY component that writes to vector indices (single-writer guarantee)\n\n5. Thread Configuration:\n   - Dedicated OS thread (std::thread::spawn), NOT tokio/async\n   - Thread name: \"frankensearch-refresh\" for debuggability\n   - Thread priority: normal (not elevated, to avoid starving search threads)\n   - Stack size: default (8MB) is sufficient\n","created_at":"2026-02-13T20:44:51Z"},{"id":146,"issue_id":"bd-3un.28","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MAJOR REVISION (replaces std::thread + crossbeam + AtomicBool):\n\nThis bead undergoes the largest architectural change in the asupersync migration. The background refresh worker transforms from a manual OS thread with ad-hoc shutdown signaling into an asupersync structured concurrency region with cancel-correct lifecycle management.\n\nBEFORE (std::thread):\n  - std::thread::spawn(move || w.run())\n  - AtomicBool for shutdown signaling\n  - crossbeam channel for job dispatch\n  - Manual join() with timeout\n  - catch_unwind wrapper for panic safety\n  - std::thread::sleep for polling interval\n\nAFTER (asupersync):\n  - cx.region(|scope| async { ... }) for structured ownership\n  - Cx cancellation protocol (request -> drain -> finalize) for clean shutdown\n  - asupersync::channel::mpsc for two-phase job dispatch (reserve/commit = no data loss)\n  - scope.spawn() returns TaskHandle; region close waits automatically\n  - Outcome::Panicked captured structurally (no catch_unwind needed)\n  - cx.sleep(duration) for cancel-aware polling interval\n\nREVISED ARCHITECTURE:\n\npub struct RefreshWorker {\n    receiver: asupersync::channel::mpsc::Receiver<EmbeddingJob>,\n    index_writer: Arc<asupersync::sync::Mutex<VectorIndexWriter>>,\n    poll_interval: Duration,\n    batch_size: usize,\n}\n\nimpl RefreshWorker {\n    /// Run the worker within an asupersync region.\n    /// The region guarantees: no orphan tasks, clean shutdown on cancel.\n    pub async fn run(&self, cx: &Cx) -> asupersync::Outcome<(), SearchError> {\n        loop {\n            // Cancel-aware sleep (returns Cancelled if shutdown requested)\n            cx.sleep(self.poll_interval).await;\n            cx.checkpoint()?;  // Early exit point on cancellation\n\n            // Drain available jobs (non-blocking)\n            let batch = self.drain_batch(cx).await;\n            if batch.is_empty() { continue; }\n\n            // Process batch with budget enforcement\n            let budget = Budget {\n                deadline: Some(cx.now() + Duration::from_secs(30)),\n                poll_quota: 10_000,\n                ..Default::default()\n            };\n\n            // Acquire Mutex through asupersync (cancel-aware, no deadlock on shutdown)\n            let mut writer = self.index_writer.lock(cx).await?;\n            for job in &batch {\n                cx.checkpoint()?;  // Per-job cancellation check\n                writer.add_vector(job.doc_id(), job.embedding())?;\n            }\n            writer.commit()?;\n        }\n    }\n}\n\nLIFECYCLE MANAGEMENT:\n\n// In TwoTierIndex or main entry point:\npub async fn start_worker(cx: &Cx, config: WorkerConfig) -> asupersync::Outcome<(), SearchError> {\n    let (sender, receiver) = asupersync::channel::mpsc::channel(config.queue_capacity);\n\n    cx.region(|scope| async {\n        // Worker task is owned by the region\n        scope.spawn(|cx| async {\n            let worker = RefreshWorker::new(receiver, config);\n            worker.run(&cx).await\n        });\n\n        // Region stays alive until parent cancels\n        // When parent cancels: worker receives CancelKind::ParentCancelled\n        // Worker's cx.checkpoint() returns Cancelled\n        // Worker exits loop cleanly\n        // Region waits for worker to finish (structured concurrency guarantee)\n    }).await\n}\n\nSHUTDOWN PROTOCOL:\n1. Parent calls cancel on the region\n2. Worker's cx.sleep() returns immediately with Cancelled\n3. Worker's cx.checkpoint() returns Err(Cancelled)\n4. Worker exits loop, drops resources\n5. Region close confirms worker is done (no orphans)\n6. Multi-phase: Request -> Drain (finish current batch) -> Finalize (close writer)\n\nJOB DISPATCH (two-phase, cancel-safe):\n  let permit = sender.reserve(&cx).await?;  // Phase 1: reserve slot (cancel-safe)\n  permit.send(job);                          // Phase 2: commit (linear, infallible)\n  // If cancelled between phases: permit dropped, obligation resolved cleanly\n\nTESTING WITH LAB RUNTIME:\n  let lab = LabRuntime::new(LabConfig::new(42));\n  lab.run(|cx| async {\n      // Virtual time: cx.sleep() advances instantly in lab mode\n      // Deterministic scheduling: reproducible test runs\n      // Oracle checks: QuiescenceOracle, ObligationLeakOracle, TaskLeakOracle\n  });\n\nKEY BENEFITS:\n1. No catch_unwind needed — Outcome::Panicked is structural\n2. No AtomicBool shutdown flag — cancellation protocol handles it\n3. No manual join() timeout — region close is the guarantee\n4. No crossbeam dependency — asupersync channels are two-phase (cancel-safe)\n5. Deterministic testing via LabRuntime (virtual time, no real sleeps)\n6. Budget enforcement on processing (deadline, poll quota)\n\nDEPENDENCY CHANGES:\n- REMOVE: crossbeam-channel dependency\n- ADD: asupersync (workspace dependency)\n- REMOVE: std::thread::spawn usage\n- REMOVE: AtomicBool shutdown flag pattern","created_at":"2026-02-13T21:06:07Z"},{"id":189,"issue_id":"bd-3un.28","author":"Dicklesworthstone","text":"REVISION (review pass 4 - asupersync conformance):\n\n1. CRITICAL: std::thread::spawn IS PROHIBITED. Per the project mandate (MEMORY.md): \"Background worker: asupersync region + scope.spawn (NOT std::thread::spawn)\". The RefreshWorker must use asupersync's structured concurrency:\n\n   BEFORE (prohibited):\n   std::thread::spawn(move || worker.run());\n\n   AFTER (correct):\n   asupersync::scope!(cx, |scope| {\n       scope.spawn(|cx| async move {\n           worker.run(&cx).await;\n       });\n   });\n\n   The worker.run() method becomes async and takes &Cx for cancellation:\n   pub async fn run(&self, cx: &Cx) -> asupersync::Outcome<WorkerReport, SearchError> {\n       loop {\n           cx.checkpoint()?;  // Cancel check\n           self.run_cycle(cx).await?;\n           cx.sleep(Duration::from_millis(self.config.refresh_interval_ms)).await;\n       }\n   }\n\n2. SHUTDOWN MECHANISM CHANGE: Replace AtomicBool shutdown flag with Cx cancellation:\n   - Parent scope cancels the Cx when shutdown is desired\n   - cx.checkpoint() in the loop returns Outcome::Cancelled\n   - Worker cleans up (flush pending writes) in a bracket/finally block\n   - No more manual AtomicBool + join() + timeout patterns\n\n3. SLEEP IS VIRTUAL IN TESTS: cx.sleep() uses virtual time in LabRuntime, so tests complete instantly. This enables deterministic testing of the refresh interval logic without real delays.\n\n4. ERROR RECOVERY: Replace catch_unwind with asupersync's Outcome::Panicked propagation. Panics in the worker scope propagate as structured errors, not silent thread deaths.\n","created_at":"2026-02-13T21:10:25Z"},{"id":251,"issue_id":"bd-3un.28","author":"Dicklesworthstone","text":"CORRECTNESS FIX: Description says std::thread but this is PROHIBITED\n\nThe description says:\n  \"This runs on a dedicated OS thread (not async)\"\n  \"spawn on a dedicated std::thread\"\n\nThis is STALE. Per AGENTS.md and the asupersync migration:\n  - std::thread::spawn IS PROHIBITED\n  - Must use asupersync structured concurrency: cx.region() + scope.spawn()\n  - Shutdown uses Cx cancellation, NOT AtomicBool\n  - Sleep uses cx.sleep() (virtual time in LabRuntime), NOT std::thread::sleep\n\nThe refresh worker is an asupersync background task, not an OS thread.\nImplementers: follow the asupersync migration comments (#2, #3), not the\noriginal description's threading model.\n\nKey change: the \"blocking loop\" becomes an async loop with cx.checkpoint()\ncancel points. When the parent context is cancelled, the worker exits\ngracefully after draining the current batch.\n","created_at":"2026-02-13T21:54:12Z"},{"id":268,"issue_id":"bd-3un.28","author":"Dicklesworthstone","text":"REVIEW FIX — Complete body rewrite needed (std::thread prohibited):\n\n1. BODY ENTIRELY CONTRADICTS REVISIONS: The body says \"dedicated OS thread\" and uses std::thread::spawn. The asupersync mandate prohibits std::thread::spawn. The body's entire design premise is invalidated and needs complete replacement.\n\n   CANONICAL DESIGN (replaces body):\n   The index refresh worker runs as an asupersync region task, NOT an OS thread.\n\n   pub struct IndexRefreshWorker {\n       rx: asupersync::sync::Receiver<RefreshCommand>,\n       index: Arc<asupersync::sync::RwLock<TwoTierIndex>>,\n   }\n\n   impl IndexRefreshWorker {\n       pub async fn run(self, cx: &Cx) -> Outcome<(), SearchError> {\n           // Single-writer guarantee: this is the ONLY task that writes to the index\n           loop {\n               match self.rx.recv(cx).await {\n                   Outcome::Ok(RefreshCommand::EmbedBatch(docs)) => {\n                       // Embed documents, update vector index\n                       let mut idx = self.index.write(cx).await;\n                       // ... embedding and index update logic\n                   }\n                   Outcome::Ok(RefreshCommand::Rebuild) => {\n                       // Full index rebuild\n                   }\n                   Outcome::Ok(RefreshCommand::Shutdown) => break,\n                   Outcome::Cancelled => break, // Graceful cancel via Cx\n                   Outcome::Panicked(p) => {\n                       tracing::error!(\"refresh worker panic: {:?}\", p);\n                       // Log and continue — don't let one bad batch kill the worker\n                   }\n                   Outcome::Err(e) => {\n                       tracing::warn!(\"refresh worker recv error: {}\", e);\n                   }\n               }\n           }\n           Outcome::Ok(())\n       }\n   }\n\n2. SINGLE-WRITER GUARANTEE: The worker is the ONLY component that writes to vector indices. All reads happen through RwLock::read(). This eliminates data races without fine-grained locking.\n\n3. CANCELLATION: Instead of AtomicBool for shutdown, use Cx cancellation. When the parent scope drops, the worker's Cx is cancelled, causing recv() to return Outcome::Cancelled.\n\n4. ERROR RECOVERY: Instead of catch_unwind, use Outcome::Panicked from asupersync. Failed batches are logged and skipped — the worker continues processing subsequent commands.\n\n5. BATCH COALESCING: If multiple EmbedBatch commands queue up while the worker is busy, coalesce them:\n   let mut batch = first_batch;\n   while let Outcome::Ok(RefreshCommand::EmbedBatch(more)) = self.rx.try_recv() {\n       batch.extend(more);\n   }\n\n6. TEST REQUIREMENTS:\n   - Graceful shutdown: send Shutdown command, worker exits cleanly\n   - Cancel via Cx: drop parent scope, worker exits via Outcome::Cancelled\n   - Single-writer guarantee: concurrent write attempts blocked by RwLock\n   - Error recovery: bad embedding (returns Err) doesn't kill worker, next batch succeeds\n   - Batch coalescing: 3 rapid EmbedBatch commands coalesced into 1 batch\n   - Index consistency: read during write returns stale-but-valid data (RwLock semantics)\n   - LabRuntime deterministic test: verify shutdown ordering is deterministic","created_at":"2026-02-13T21:56:56Z"}]}
{"id":"bd-3un.29","title":"Design and implement Cargo feature flags","description":"Design the feature flag system that allows consumers to pick exactly the components they need. Feature flags control which dependencies are compiled in, keeping the crate lightweight by default.\n\nFeature hierarchy:\n\n[features]\ndefault = ['hash']  # Minimal: hash embedder only, always works\n\n# Individual components\nhash = []                                    # FNV-1a hash embedder (zero deps)\nmodel2vec = ['dep:safetensors', 'dep:tokenizers', 'dep:dirs']  # Potion-128M fast embedder\nfastembed = ['dep:fastembed']                # MiniLM-L6 quality embedder (brings ONNX)\nlexical = ['dep:tantivy']                   # Tantivy full-text search\nrerank = ['dep:ort', 'dep:tokenizers']      # Cross-encoder rerankers\nann = ['dep:hnsw_rs']                       # HNSW approximate nearest neighbors\ndownload = ['dep:reqwest']                  # Model download from HuggingFace\n\n# Bundles\nsemantic = ['hash', 'model2vec', 'fastembed']  # All embedding models\nhybrid = ['semantic', 'lexical']               # Semantic + lexical + RRF fusion\nfull = ['hybrid', 'rerank', 'ann', 'download'] # Everything\n\n# Performance\nsimd = ['dep:wide']  # SIMD dot product (recommended, default in most bundles)\n\nDesign principles:\n1. 'hash' is always available (zero deps, always works)\n2. Each ML model is independently selectable\n3. 'full' gives you everything but isn't the default\n4. Consumer picks their budget: just 'hash' for testing, 'semantic' for embeddings, 'hybrid' for production\n\nConditional compilation in code:\n#[cfg(feature = 'model2vec')]\npub mod model2vec_embedder;\n\n#[cfg(feature = 'fastembed')]\npub mod fastembed_embedder;\n\nWorkspace-level feature forwarding:\nEach sub-crate exposes its features, and the facade crate re-exports them with forwarding.\n\nReference: agent-mail crates/mcp-agent-mail-search-core/Cargo.toml (semantic, hybrid features)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:53:36.360407614Z","created_by":"ubuntu","updated_at":"2026-02-13T21:58:26.004491604Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","features","phase10"],"dependencies":[{"issue_id":"bd-3un.29","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:36.360407614Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.29","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T17:55:40.689834540Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":14,"issue_id":"bd-3un.29","author":"Dicklesworthstone","text":"REVISION NOTES (review pass):\n\nSIMD CHANGE: The 'simd' feature flag should be REMOVED. The 'wide' crate should be an unconditional dependency of frankensearch-index. Rationale:\n1. wide provides portable SIMD (x86 SSE2/AVX2, ARM NEON) with automatic scalar fallback\n2. There is NO scalar dot product implementation defined anywhere in the beads\n3. Without wide, vector search would have no dot product function at all\n4. wide is zero-overhead on non-SIMD platforms (degrades to scalar loops)\n5. It's a small, well-maintained crate with no transitive dependencies\n\nSimilarly, 'half' (f16) should be unconditional in frankensearch-index since the FSVI format uses f16 by default.\n\nUPDATED feature list (remove 'simd', keep everything else):\n  default = ['hash']\n  hash = []\n  model2vec = ['dep:safetensors', 'dep:tokenizers', 'dep:dirs']\n  fastembed = ['dep:fastembed']\n  lexical = ['dep:tantivy']\n  rerank = ['dep:ort', 'dep:tokenizers']\n  ann = ['dep:hnsw_rs']\n  download = ['dep:reqwest']\n  semantic = ['hash', 'model2vec', 'fastembed']\n  hybrid = ['semantic', 'lexical']\n  full = ['hybrid', 'rerank', 'ann', 'download']\n\nIn frankensearch-index/Cargo.toml:\n  [dependencies]\n  wide = \"0.7\"    # Always available, portable SIMD\n  half = \"2.4\"    # Always available, f16 support\n  memmap2 = \"0.9\" # Always available, memory-mapped I/O\n","created_at":"2026-02-13T20:13:42Z"},{"id":154,"issue_id":"bd-3un.29","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — FEATURE FLAG UPDATE:\n\nBEFORE:\n  download = ['dep:reqwest']\n\nAFTER:\n  download = ['asupersync/tls']\n\nThe 'download' feature no longer pulls in reqwest (and its tokio transitive dep). Instead it enables asupersync's TLS feature for HTTPS model downloads via asupersync's native HTTP/1.1 client.\n\nAlso add a new workspace-level feature:\n  asupersync-lab = ['asupersync/test-internals']  # For LabRuntime in tests\n\nFull revised feature list:\n  default = ['hash']\n  hash = []\n  model2vec = ['dep:safetensors', 'dep:tokenizers']\n  fastembed = ['dep:fastembed']\n  lexical = ['dep:tantivy']\n  rerank = ['dep:ort', 'dep:tokenizers']\n  ann = ['dep:hnsw_rs']\n  download = ['asupersync/tls']                     # CHANGED: was dep:reqwest\n  semantic = ['hash', 'model2vec', 'fastembed']\n  hybrid = ['semantic', 'lexical']\n  full = ['hybrid', 'rerank', 'ann', 'download']\n\nNOTE: asupersync itself is an UNCONDITIONAL dependency (like wide and half). All crates use it for sync primitives, Cx context, and structured concurrency. Only the TLS feature is gated behind 'download'.","created_at":"2026-02-13T21:06:21Z"},{"id":239,"issue_id":"bd-3un.29","author":"Dicklesworthstone","text":"CORRECTNESS FIX: Stale reqwest reference in description\n\nThe description's feature flag listing still shows:\n  download = ['dep:reqwest']   # Model auto-download from HuggingFace\n\nThis is STALE. Per AGENTS.md and the asupersync migration:\n  download = ['asupersync/tls']  # Model download via asupersync HTTP (NO reqwest/tokio)\n\nreqwest is FORBIDDEN because it transitively depends on tokio.\nThe download system uses asupersync's native HTTP/1.1 client instead.\n\nImplementers: do NOT add reqwest to Cargo.toml. Use asupersync/tls.\n","created_at":"2026-02-13T21:50:14Z"},{"id":275,"issue_id":"bd-3un.29","author":"Dicklesworthstone","text":"REVIEW FIX — Stale body references and feature combination testing:\n\n1. STALE BODY REFERENCES: The body still references dep:dirs (removed per asupersync revision) and dep:reqwest (replaced by asupersync/tls). Update the body to match the canonical feature flags in AGENTS.md:\n   - model2vec = ['dep:safetensors', 'dep:tokenizers']  (NO dep:dirs)\n   - download = ['asupersync/tls']  (NO dep:reqwest)\n   - simd feature REMOVED (wide is unconditional)\n\n2. FEATURE COMBINATION TESTING: Add a CI step requirement:\n   cargo hack --feature-powerset check\n   This verifies that every combination of features compiles. Critical because feature-gated code paths can have hidden compilation errors.\n\n3. FEATURE FLAG REFERENCE (canonical, from AGENTS.md):\n   default = ['hash']\n   hash = []\n   model2vec = ['dep:safetensors', 'dep:tokenizers']\n   fastembed = ['dep:fastembed']\n   lexical = ['dep:tantivy']\n   rerank = ['dep:ort', 'dep:tokenizers']\n   ann = ['dep:hnsw_rs']\n   download = ['asupersync/tls']\n   semantic = ['hash', 'model2vec', 'fastembed']\n   hybrid = ['semantic', 'lexical']\n   full = ['hybrid', 'rerank', 'ann', 'download']\n\n4. TEST REQUIREMENTS:\n   - Default features (hash only) compiles: cargo check --no-default-features --features hash\n   - Full features compiles: cargo check --all-features\n   - Each individual feature compiles independently\n   - Feature powerset: cargo hack --feature-powerset check (CI step)","created_at":"2026-02-13T21:58:26Z"}]}
{"id":"bd-3un.3","title":"Define Embedder trait and core embedding types","description":"Define the core Embedder trait in frankensearch-core. This is the central abstraction that all embedding models implement. Must be object-safe (dyn Embedder) for runtime polymorphism.\n\nTrait design (synthesized from all 3 codebases):\n\npub trait Embedder: Send + Sync {\n    /// Generate embedding for a single text.\n    fn embed(&self, text: &str) -> SearchResult<Vec<f32>>;\n    \n    /// Batch embedding for multiple texts (default: sequential).\n    fn embed_batch(&self, texts: &[&str]) -> SearchResult<Vec<Vec<f32>>> {\n        texts.iter().map(|t| self.embed(t)).collect()\n    }\n    \n    /// Embedding dimension (e.g., 256 for potion, 384 for MiniLM).\n    fn dimension(&self) -> usize;\n    \n    /// Unique identifier string (e.g., 'minilm-384', 'fnv1a-384').\n    fn id(&self) -> &str;\n    \n    /// Human-readable model name.\n    fn model_name(&self) -> &str;\n    \n    /// Whether this produces semantically meaningful embeddings.\n    /// Hash embedders return false; ML models return true.\n    fn is_semantic(&self) -> bool;\n    \n    /// Performance category for tiering decisions.\n    fn category(&self) -> ModelCategory;\n    \n    /// Whether this model supports Matryoshka Representation Learning (dim truncation).\n    fn supports_mrl(&self) -> bool { false }\n    \n    /// Truncate embedding to target dimension (only valid if supports_mrl()).\n    fn truncate_embedding(&self, embedding: &[f32], target_dim: usize) -> SearchResult<Vec<f32>> {\n        if target_dim >= embedding.len() { return Ok(embedding.to_vec()); }\n        Ok(l2_normalize(&embedding[..target_dim]))\n    }\n}\n\nSupporting types:\n\npub enum ModelCategory {\n    /// Hash-based (FNV-1a): ~0.07ms, no semantic meaning\n    HashEmbedder,\n    /// Static token embeddings (Model2Vec/potion): ~0.5-1ms, decent semantics\n    StaticEmbedder,\n    /// Transformer inference (MiniLM, BGE): ~100-500ms, best quality\n    TransformerEmbedder,\n}\n\npub enum ModelTier {\n    /// Ultra-fast for immediate results (hash or static embedder)\n    Fast,\n    /// High-quality for refinement (transformer embedder)\n    Quality,\n}\n\npub struct ModelInfo {\n    pub id: String,\n    pub name: String,\n    pub dimension: usize,\n    pub category: ModelCategory,\n    pub tier: ModelTier,\n    pub is_semantic: bool,\n    pub supports_mrl: bool,\n    pub huggingface_id: Option<String>,\n    pub size_bytes: Option<u64>,\n    pub license: Option<String>,\n}\n\nAlso provide helper functions:\n- pub fn l2_normalize(vec: &[f32]) -> Vec<f32>\n- pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32\n\nReference implementations:\n- cass: src/search/embedder.rs lines 60-151\n- xf: src/embedder.rs (adds category(), supports_mrl())\n- agent-mail: crates/mcp-agent-mail-search-core/src/embedder.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:30.625424732Z","created_by":"ubuntu","updated_at":"2026-02-13T22:00:04.179847695Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1","trait"],"dependencies":[{"issue_id":"bd-3un.3","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:30.625424732Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.3","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:00.598580912Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":23,"issue_id":"bd-3un.3","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. MODEL SEARCH PATHS: Embedder implementations should check multiple standard paths for model files, in priority order:\n   a. FRANKENSEARCH_MODEL_DIR env var (explicit override)\n   b. ~/.cache/frankensearch/models/{model_name}/\n   c. ~/.local/share/frankensearch/models/{model_name}/\n   d. ~/.cache/huggingface/hub/models--{org}--{model}/snapshots/{latest}/\n   Reference: xf flashrank_reranker.rs model search paths.\n\n2. is_ready() METHOD: Add an is_ready() method (default: true) that checks if the model is loaded and functional. From agent-mail embedder.rs:\n   fn is_ready(&self) -> bool { true }\n   The hash embedder is always ready. ML models check if ONNX session is loaded.\n\n3. EMBEDDER ERROR TYPES: The EmbedderResult should use SearchError variants, not a separate EmbedderError. Keep the error hierarchy unified. However, from xf embedder.rs, define clear variant mappings:\n   - Unavailable -> SearchError::EmbedderUnavailable\n   - EmbeddingFailed -> SearchError::EmbeddingFailed\n   - InvalidInput -> SearchError::InvalidConfig\n\n4. BATCH EMBEDDING DEFAULT: The default embed_batch() calls embed() sequentially. For ML models, override with actual batch processing for 2-3x throughput during indexing.\n","created_at":"2026-02-13T20:26:03Z"},{"id":162,"issue_id":"bd-3un.3","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — CORE TRAIT REVISION (Embedder + Reranker):\n\nThe Embedder and Reranker traits gain a Cx parameter for cancel-aware operations:\n\nBEFORE:\n  pub trait Embedder: Send + Sync {\n      fn embed(&self, text: &str) -> SearchResult<Vec<f32>>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n  }\n\nAFTER:\n  pub trait Embedder: Send + Sync {\n      async fn embed(&self, cx: &Cx, text: &str) -> asupersync::Result<Vec<f32>>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n  }\n\n  pub trait Reranker: Send + Sync {\n      async fn rerank(&self, cx: &Cx, query: &str, docs: &[&str]) -> asupersync::Result<Vec<f32>>;\n  }\n\nKEY IMPLICATIONS:\n1. embed() is now async — enables cancel-aware Mutex acquisition for ONNX sessions\n2. Cx parameter enables: cancellation checks, budget enforcement, tracing\n3. Hash embedder: embed() is synchronous internally but async for trait uniformity\n   (just wraps the sync computation — zero overhead for fast embedders)\n4. Model2Vec: same — synchronous internally, async trait wrapper\n5. FastEmbed: genuinely benefits — cancel-aware Mutex lock on ONNX session\n6. FlashRank reranker: same — cancel-aware Mutex for ONNX session\n\nRETURN TYPE: asupersync::Result<T> instead of our SearchResult<T>. This enables:\n  - Outcome::Cancelled propagation (embedder cancelled = search cancelled)\n  - Outcome::Panicked propagation (ONNX crash = graceful RefinementFailed)\n  - Integration with asupersync's error recovery actions\n\nOBJECT SAFETY: async trait methods require dyn-compatible async traits.\n  Option A: Use #[async_trait] attribute (allocating but simple)\n  Option B: Use RPITIT (Return Position Impl Trait In Trait) — Rust 2024 nightly supports this\n  Option C: Use asupersync's own async trait pattern\n  DECISION: Use RPITIT (Rust 2024 edition supports it natively, no macro needed)","created_at":"2026-02-13T21:06:36Z"},{"id":223,"issue_id":"bd-3un.3","author":"Dicklesworthstone","text":"REVIEW FIX — Async trait dyn-compatibility resolution:\n\nPROBLEM: The Embedder and Reranker traits must be:\n  1. async (for cancel-aware Mutex acquisition in ONNX embedders)\n  2. dyn-compatible (for runtime polymorphism: Box<dyn Embedder>, Arc<dyn Embedder>)\n\nThese two requirements conflict because `async fn` in traits produces opaque return types that are NOT dyn-compatible, even in Rust 2024 nightly with RPITIT.\n\nRESOLUTION: Use the `trait_variant` crate (rust-lang official, part of async-wg output) to generate both a static-dispatch and dyn-compatible version:\n\n  use trait_variant::make;\n\n  #[make(SendEmbedder: Send)]\n  pub trait Embedder: Sync {\n      async fn embed(&self, cx: &Cx, text: &str) -> Result<Vec<f32>, SearchError>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n      fn id(&self) -> &str;\n      fn is_ready(&self) -> bool;\n  }\n\nThis generates:\n  - `Embedder` — the base async trait (not dyn-compatible)\n  - `SendEmbedder` — auto-generated dyn-compatible variant with Send bounds\n\nUsage:\n  - Concrete types implement `Embedder`\n  - Generic code uses `impl Embedder` or `T: Embedder`\n  - Dynamic dispatch uses `Box<dyn SendEmbedder>` or `Arc<dyn SendEmbedder>`\n\nALTERNATIVE if trait_variant is not desired: Manual desugaring with BoxFuture:\n\n  pub trait Embedder: Send + Sync {\n      fn embed<'a>(&'a self, cx: &'a Cx, text: &'a str) -> Pin<Box<dyn Future<Output = Result<Vec<f32>, SearchError>> + Send + 'a>>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n      fn id(&self) -> &str;\n      fn is_ready(&self) -> bool;\n  }\n\nThis is dyn-compatible and requires no proc-macro, but is verbose. Implementors can use a helper macro to reduce boilerplate.\n\nDECISION: Use `trait_variant` (Option A). It's the Rust async-wg's official solution, minimal dependency, and generates clean code. Add `trait_variant = \"0.1\"` to workspace dependencies in bd-3un.1.\n\nSAME APPROACH FOR RERANKER:\n\n  #[make(SendReranker: Send)]\n  pub trait Reranker: Sync {\n      async fn rerank(&self, cx: &Cx, query: &str, docs: &[&str]) -> Result<Vec<f32>, SearchError>;\n      fn model_name(&self) -> &str;\n      fn max_length(&self) -> usize;\n      fn is_available(&self) -> bool;\n  }\n\nNON-ASYNC METHODS STAY SYNC: dimension(), is_semantic(), category(), id(), is_ready(), model_name(), max_length(), is_available() all remain synchronous. Only the heavy-compute methods (embed, rerank) are async.\n\nUTILITY FUNCTIONS STAY SYNC: l2_normalize(), cosine_similarity(), truncate_embedding() are standalone functions, not trait methods. They stay synchronous.\n\nTEST REQUIREMENTS for bd-3un.3:\n  - Implement a MockEmbedder for testing (hash-based, deterministic)\n  - Verify dyn SendEmbedder works: Box<dyn SendEmbedder> can call embed()\n  - Verify Arc<dyn SendEmbedder> is Send + Sync\n  - l2_normalize produces unit vectors (norm within f32 epsilon of 1.0)\n  - cosine_similarity(v, v) ≈ 1.0\n  - cosine_similarity of orthogonal vectors ≈ 0.0\n  - truncate_embedding with MRL: verify dimension reduction is correct\n  - Edge cases: empty text, very long text (>8192 chars), unicode text\n\nTEST REQUIREMENTS for bd-3un.4:\n  - Implement a MockReranker for testing\n  - Verify dyn SendReranker works: Box<dyn SendReranker> can call rerank()\n  - rerank() output length matches input docs length\n  - is_available() == false → rerank() returns Err or is skipped by pipeline\n  - Edge cases: empty docs list, single doc, query longer than max_length","created_at":"2026-02-13T21:46:26Z"},{"id":292,"issue_id":"bd-3un.3","author":"Dicklesworthstone","text":"REVIEW FIX (cross-cutting) — LexicalSearch trait belongs in frankensearch-core:\n\nARCHITECTURAL DECISION: The LexicalSearch trait (abstract interface for lexical search backends) belongs in frankensearch-core alongside Embedder and Reranker. This follows the same pattern: core defines traits, implementation crates provide concrete types.\n\npub trait LexicalSearch: Send + Sync {\n    async fn search(&self, cx: &Cx, query: &str, limit: usize) -> Result<Vec<ScoredResult>, SearchError>;\n    async fn index_document(&self, cx: &Cx, doc: &IndexableDocument) -> Result<(), SearchError>;\n    async fn commit(&self, cx: &Cx) -> Result<(), SearchError>;\n    fn doc_count(&self) -> usize;\n}\n\nThis trait is implemented by:\n- TantivyIndex (frankensearch-lexical, behind 'lexical' feature)\n- Potentially other backends in the future (SQLite FTS5, etc.)\n\nThe facade (bd-3un.30) re-exports both the trait (from core) and the implementation (from lexical).","created_at":"2026-02-13T22:00:04Z"}]}
{"id":"bd-3un.30","title":"Design public API surface and facade re-exports","description":"Design the public API surface of the frankensearch facade crate. This is what consumers actually import and use. It should provide a clean, ergonomic API that hides internal crate boundaries.\n\nThe frankensearch crate (facade) re-exports from sub-crates:\n\n// frankensearch/src/lib.rs\n\n// Core types (always available)\npub use frankensearch_core::{\n    SearchError, SearchResult,\n    Embedder, ModelCategory, ModelTier, ModelInfo,\n    Reranker,\n    ScoredResult, VectorHit, FusedHit, SourceContribution,\n    SearchMode, SearchPhase,\n    l2_normalize, cosine_similarity,\n};\n\n// Embedders\npub use frankensearch_embed::{HashEmbedder};\n#[cfg(feature = 'model2vec')]\npub use frankensearch_embed::{Model2VecEmbedder};\n#[cfg(feature = 'fastembed')]\npub use frankensearch_embed::{FastEmbedEmbedder};\npub use frankensearch_embed::{EmbedderStack, TwoTierAvailability};\n\n// Vector index\npub use frankensearch_index::{VectorIndex, VectorIndexWriter};\n#[cfg(feature = 'simd')]\npub use frankensearch_index::{dot_product_f16_f32};\n\n// Lexical search\n#[cfg(feature = 'lexical')]\npub use frankensearch_lexical::{LexicalIndex, LexicalQuery, LexicalHit, IndexableDocument};\n\n// Fusion\npub use frankensearch_fusion::{\n    TwoTierConfig, TwoTierIndex, TwoTierSearcher,\n    RrfConfig, rrf_fuse,\n    blend_two_tier, min_max_normalize,\n};\n\n// Reranking\n#[cfg(feature = 'rerank')]\npub use frankensearch_rerank::{FlashRankReranker, RerankStep};\n\n// Model management\npub use frankensearch_embed::{EmbedderRegistry, RegisteredEmbedder, ModelManifest};\n\nErgonomic convenience:\n// One-liner to get started:\nlet search = frankensearch::TwoTierSearcher::auto(data_dir)?;\nfor phase in search.search('my query', 10) {\n    // handle results\n}\n\nThe 'auto' constructor should detect available models and build the appropriate stack automatically.\n\nFile: frankensearch/src/lib.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:53:36.438488543Z","created_by":"ubuntu","updated_at":"2026-02-13T21:57:13.142818242Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","facade","phase10"],"dependencies":[{"issue_id":"bd-3un.30","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:36.438488543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.12","type":"blocks","created_at":"2026-02-13T17:55:41.126092025Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T17:55:40.871811355Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T17:55:40.785987719Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.26","type":"blocks","created_at":"2026-02-13T17:55:40.956981386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.29","type":"blocks","created_at":"2026-02-13T17:55:41.042653297Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":78,"issue_id":"bd-3un.30","author":"Dicklesworthstone","text":"REVISION: Facade API Design Principles\n\nThe facade is the critical bottleneck (betweenness 100%). Its design determines UX quality.\n\n1. One-Liner Entry Point:\n   let searcher = frankensearch::TwoTierSearcher::auto(data_dir)?;\n   This must work with zero configuration. auto() calls:\n   - EmbedderStack::auto_detect(data_dir) for model discovery\n   - TwoTierConfig::default() for sane defaults\n   - TwoTierIndex::open(data_dir) for index loading\n   - Returns error ONLY if data_dir doesn't exist or has no index files\n\n2. Error Propagation:\n   Facade functions return SearchResult<T> (alias for Result<T, SearchError>).\n   NEVER panic. NEVER unwrap. All failures are expressed as SearchError variants.\n   The facade adds NO new error types -- it re-uses SearchError from core.\n\n3. Conditional Compilation:\n   Each sub-module is gated:\n   #[cfg(feature = \"semantic\")] pub mod embed;\n   #[cfg(feature = \"lexical\")] pub mod lexical;\n   #[cfg(feature = \"rerank\")] pub mod rerank;\n   The facade crate with default features (hash only) should compile in <5s.\n\n4. Re-export Strategy:\n   - Re-export types users need: TwoTierSearcher, SearchPhase, ScoredResult, TwoTierConfig\n   - Re-export traits users implement: Embedder, Reranker\n   - Do NOT re-export internal types: VectorIndex internals, SIMD functions, queue internals\n   - pub use frankensearch_core::{SearchError, SearchResult, ScoredResult, SearchPhase};\n   - pub use frankensearch_fusion::{TwoTierSearcher, TwoTierConfig};\n\n5. Version Compatibility:\n   The facade's public API is the stability surface. Internal crate APIs can change freely.\n   Document which types are part of the public API vs internal in rustdoc.\n","created_at":"2026-02-13T20:47:17Z"},{"id":164,"issue_id":"bd-3un.30","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — FACADE RE-EXPORTS:\n\nThe facade crate (frankensearch/) should re-export key asupersync types that consumers need:\n\npub use asupersync::{Cx, Outcome, Scope};\npub use asupersync::error::{Error as AsyncError, Result as AsyncResult};\n\nThis means consumers of frankensearch get the Cx context type without depending on asupersync directly. The facade provides a unified API:\n\n  use frankensearch::{TwoTierSearcher, TwoTierConfig, Cx, Outcome};\n\n  let results = searcher.search(&cx, \"my query\", 10).await;\n\nThe Cx is passed DOWN from the consumer's runtime. frankensearch does NOT create its own runtime — it's a library that runs within the consumer's asupersync context.","created_at":"2026-02-13T21:06:42Z"},{"id":240,"issue_id":"bd-3un.30","author":"Dicklesworthstone","text":"USER EXPERIENCE GAP: Missing indexing convenience API\n\nThe facade provides a one-liner for SEARCHING:\n  let searcher = TwoTierSearcher::auto(data_dir)?;\n\nBut there is NO corresponding convenience API for INDEXING. Users currently\nmust manually:\n  1. Create EmbedderStack via auto_detect()\n  2. Create VectorIndexWriter for fast tier\n  3. Optionally create VectorIndexWriter for quality tier\n  4. Optionally create Tantivy LexicalIndex\n  5. For each document: canonicalize, embed (fast + quality), write to both indices\n  6. Commit Tantivy, finish VectorIndexWriter, fsync\n\nPROPOSED: Add an IndexBuilder to the facade:\n\n  let builder = frankensearch::IndexBuilder::new(data_dir)\n      .with_config(config)    // optional, uses defaults\n      .build()?;              // creates EmbedderStack + index files\n\n  // Add documents (handles embedding, dedup, and index writes internally)\n  builder.add_document(\"doc-1\", \"Hello world\", None)?;\n  builder.add_documents(docs.iter())?;\n\n  // Finalize (commits Tantivy, finishes FSVI, fsync)\n  let stats = builder.finish()?;\n  // stats: IndexBuildStats { doc_count, fast_index_size, quality_index_size, duration }\n\nUnder the hood, IndexBuilder:\n  - Uses EmbedderStack::auto_detect() for embedding\n  - Handles fast + quality tier embedding in batch (bd-3un.27 queue)\n  - Creates Tantivy index if feature=\"lexical\" enabled\n  - Creates FSVI indices with f16 quantization\n  - Applies text canonicalization (bd-3un.42)\n  - Deduplicates by content hash (bd-3w1.4 if feature=\"storage\")\n  - Reports progress via tracing spans\n\nThis makes frankensearch a true \"drop-in library\" — both search and indexing\nare one-liner operations. Without this, the library is only half-convenient.\n\nThe IndexBuilder should live in the facade crate alongside TwoTierSearcher.\n","created_at":"2026-02-13T21:50:15Z"},{"id":269,"issue_id":"bd-3un.30","author":"Dicklesworthstone","text":"REVIEW FIX — simd feature removed, LexicalSearch trait location, and missing re-exports:\n\n1. REMOVED simd FEATURE: The body contains #[cfg(feature = 'simd')] guards on dot_product_f16_f32. Per bd-3un.29's revision, the simd feature was REMOVED — wide is an unconditional dependency. Remove ALL #[cfg(feature = 'simd')] guards. dot_product_f16_f32 is always available.\n\n2. LexicalSearch TRAIT LOCATION: The body re-exports LexicalIndex from frankensearch_lexical. Per the architectural resolution:\n   - LexicalSearch TRAIT lives in frankensearch-core (abstract interface)\n   - TantivyIndex STRUCT lives in frankensearch-lexical (concrete implementation)\n   - The facade re-exports BOTH:\n     pub use frankensearch_core::LexicalSearch;          // trait\n     pub use frankensearch_lexical::TantivyIndex;        // impl (behind 'lexical' feature)\n\n3. MISSING RE-EXPORTS — Add:\n   pub use frankensearch_fusion::TwoTierMetrics;         // Users need metrics for monitoring\n   pub use frankensearch_core::QueryClass;               // Users may want to pre-classify queries  \n   pub use frankensearch_core::CandidateBudget;          // Useful for advanced users\n   pub use asupersync::Cx;                               // Required by all async APIs\n   pub use asupersync::Outcome;                          // Return type of async operations\n\n4. TwoTierSearcher::auto() BEHAVIOR: The body defines auto(data_dir) as a one-liner but doesn't specify behavior when no models are found. \n   \n   RESOLUTION: auto() ALWAYS succeeds. If no semantic models are found:\n   - EmbedderStack uses hash embedder as fast tier, quality = None\n   - has_quality() returns false\n   - Search works but returns hash-quality results (sufficient for testing/development)\n   Log INFO: \"No semantic models found at {data_dir}, using hash embedder (development mode)\"\n\n5. TEST REQUIREMENTS:\n   - Facade compiles under each feature combination (cargo hack --feature-powerset)\n   - All re-exported types are accessible via frankensearch::TypeName\n   - auto() with empty directory succeeds with hash-only embedder\n   - auto() with model files detects and loads them","created_at":"2026-02-13T21:57:13Z"}]}
{"id":"bd-3un.31","title":"Write unit tests for all core components","description":"Write comprehensive unit tests for all frankensearch components. Tests should cover happy paths, edge cases, and error conditions.\n\nTest categories by component:\n\n1. Hash Embedder Tests:\n   - Known input→output regression tests (deterministic)\n   - Dimension configuration (128, 256, 384)\n   - Empty input handling\n   - Unicode text handling\n   - L2 normalization verification (unit length)\n\n2. Score Normalization Tests:\n   - min_max_normalize: [0.5, 1.0, 0.0] → [0.5, 1.0, 0.0]\n   - All equal scores → [1.0, 1.0, 1.0]\n   - Single score → [1.0]\n   - Empty input → []\n   - Negative scores handling\n\n3. RRF Fusion Tests:\n   - Lexical-only (no semantic results)\n   - Semantic-only (no lexical results)\n   - Overlap scoring (docs in both get higher scores)\n   - Offset and limit handling\n   - Empty inputs\n\n4. Two-Tier Blending Tests:\n   - blend_factor=0.0 → fast scores only\n   - blend_factor=1.0 → quality scores only\n   - blend_factor=0.7 → weighted combination\n   - Docs only in fast set\n   - Docs only in quality set\n   - Rank correlation metrics\n\n5. Vector Index Tests:\n   - Write and read back (round-trip)\n   - f16 quantization accuracy\n   - Header CRC32 verification\n   - Corrupted file detection\n   - Dimension mismatch detection\n\n6. SIMD Dot Product Tests:\n   - Known vectors → expected dot product\n   - Orthogonal vectors → 0.0\n   - Identical vectors → 1.0 (if normalized)\n   - Various dimensions (including non-8-aligned for remainder handling)\n   - Scalar fallback equivalence\n\n7. Embedder Stack Tests:\n   - Auto-detection with all models available\n   - Fallback to hash-only\n   - Fast-only availability\n   - Quality-only availability\n\nTarget: > 80% line coverage for core modules.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:54:09.804163360Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:19.665323307Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase11","testing","unit"],"dependencies":[{"issue_id":"bd-3un.31","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:09.804163360Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T17:55:49.431519547Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.19","type":"blocks","created_at":"2026-02-13T17:55:49.270170941Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T17:55:49.350892548Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T17:55:49.191034775Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:13:16.243990500Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":12,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"REVISION NOTES (review pass):\n\n1. INLINE TESTS: Per the epic's TESTING POLICY, each component should have its own #[cfg(test)] inline tests. This bead (bd-3un.31) covers CROSS-COMPONENT and ORCHESTRATION-LEVEL unit tests that verify interactions between components.\n\n2. LOGGING IN TESTS: All tests must configure tracing-test subscriber to capture log events. Key assertions should verify that expected log events are emitted:\n   - Verify \"search_completed\" event is logged with correct fields\n   - Verify \"quality_model_unavailable\" WARN is logged when quality model missing\n   - Verify per-phase timing spans are emitted\n\n3. TEST FIXTURES: Use the ground truth corpus from bd-3un.38 (tests/fixtures/) for any tests needing realistic data. Do NOT generate random data inline -- use the shared fixtures.\n\n4. COVERAGE REPORT: Include a #[cfg(test)] function that lists all tested component interactions and their coverage status, so we can track what's tested.\n","created_at":"2026-02-13T20:13:12Z"},{"id":51,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"REVISION: Unit Tests Edge Case Enumeration\n\nBeyond the 7 test categories already specified, each category MUST include these edge cases:\n\n1. Scoring Edge Cases (applies to normalization, RRF, blending):\n   - All scores identical: verify no NaN/Inf, all outputs equal\n   - All scores zero: verify no division-by-zero\n   - Single result: verify single-element collections work\n   - Empty result set: verify empty input -> empty output (no panic)\n   - NaN score: verify NaN-safe handling via total_cmp() (NaN sorts last)\n   - Negative scores: BM25 can produce negative scores in some configurations\n   - Very large scores: f32::MAX, verify no overflow in summation\n   - Score of exactly 0.0 and exactly 1.0: boundary conditions\n\n2. RRF-Specific Tests:\n   - Same document in both lexical and semantic: verify score addition\n   - Document in only one source: verify single contribution score\n   - K=0 edge case: formula becomes 1/(rank+1), still valid\n   - K=60 with rank=0: verify 1/61 = 0.016393...\n   - 4-level tie-breaking: construct cases that exercise each tiebreaker\n   - Deterministic ordering: same inputs always produce same output order\n\n3. Two-Tier Blending Tests:\n   - blend_factor=0.0: output should equal fast-tier scores exactly\n   - blend_factor=1.0: output should equal quality-tier scores exactly\n   - blend_factor=0.5: output should be arithmetic mean\n   - Missing quality score: verify 0.0 default is applied correctly\n   - kendall_tau with identical rankings: should return 1.0\n   - kendall_tau with reversed rankings: should return -1.0\n\n4. Vector Index Tests:\n   - Zero-dimension vector: should be rejected at index creation\n   - Dimension mismatch: query dim != index dim -> clear error\n   - Empty index (0 records): search returns empty, no crash\n   - Index with 1 record: top-k=10 returns 1 result\n   - top-k > record_count: returns all records, no padding\n   - CRC32 verification: corrupt 1 byte in header -> detect on open\n\n5. Hash Embedder Tests:\n   - Empty string: should produce zero vector (or handled gracefully)\n   - Single token: verify deterministic output\n   - Same input twice: verify identical embeddings (determinism)\n   - Very long input (100K chars): verify truncation + consistent output\n   - Unicode input: verify no panic on multi-byte UTF-8\n\n6. Embedder Stack Tests:\n   - No models available: should return HashOnly availability\n   - Only fast model: should return FastOnly\n   - Only quality model: should return QualityOnly\n   - Both models: should return Full\n   - DimReduceEmbedder: verify truncation preserves direction (cosine > 0.99)\n\n7. Coverage Tracking:\n   - Each test file includes a coverage_check() function\n   - Lists all public functions in the module under test\n   - Asserts each function has at least one test (compile-time reminder)\n","created_at":"2026-02-13T20:44:55Z"},{"id":86,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"GRAVEYARD COMPONENT TEST COVERAGE (added in refinement pass 2):\n\nThe following graveyard beads add components that need unit test coverage in this bead:\n\n1. bd-l7v (S3-FIFO Cache):\n   - CachePolicy trait implementations (S3Fifo, Unbounded, NoCache)\n   - Cache hit/miss counting\n   - Small->Main promotion on frequency threshold\n   - Ghost queue re-admission\n   - Memory budget enforcement and eviction\n   - Concurrent get/insert thread safety\n   - Cache transparency: identical rankings with/without cache\n\n2. bd-22k (Score Calibration):\n   - Identity calibrator preserves scores exactly\n   - TemperatureScaling with T=1.0 equals sigmoid\n   - Platt scaling monotonicity\n   - Isotonic regression monotonicity guarantee\n   - ECE computation correctness\n   - Batch vs sequential calibration equivalence\n   - JSON serialization/deserialization round-trip\n\n3. bd-1cr (Robust Statistics):\n   - TDigest quantile accuracy (p50 within 1% on 10K normal samples)\n   - TDigest merge correctness\n   - MedianMAD on known dataset\n   - HuberEstimator outlier resistance\n   - HyperLogLog cardinality estimation accuracy\n   - Concurrent update safety\n\n4. bd-2ps (Sequential Testing / PhaseGate):\n   - E-value bounded under null hypothesis\n   - SkipQuality triggers when fast is sufficient\n   - Timeout resets correctly\n   - Integration with SearchPhase::RefinementFailed\n\n5. bd-6sj (OPE) — offline tool, tests go in its own module:\n   - IPS under uniform policy = simple average\n   - DR variance <= IPS variance\n   - ESS computation\n   - Clipping reduces variance","created_at":"2026-02-13T20:52:14Z"},{"id":166,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — TESTING STRATEGY (applies to bd-3un.31, bd-3un.32, bd-3un.38, bd-3un.40):\n\nAll test beads gain access to asupersync's LabRuntime for deterministic testing. This is a MAJOR improvement over non-deterministic std::thread-based testing.\n\nKEY ADDITIONS:\n\n1. LabRuntime for deterministic tests (bd-3un.31, bd-3un.32):\n   - Virtual time: cx.sleep() advances instantly, no real waits\n   - Deterministic scheduling: same seed = same execution order\n   - Reproducible: failing tests can be replayed with exact same schedule\n   - Oracles: automatic verification of correctness properties\n\n2. Oracles to add to every test (bd-3un.31, bd-3un.32):\n   - QuiescenceOracle: verify no orphan tasks after test\n   - ObligationLeakOracle: verify no leaked channel permits\n   - TaskLeakOracle: verify no stray tasks\n   - DeterminismOracle: verify same seed produces same result\n\n3. DPOR schedule exploration (bd-3un.32 integration tests):\n   - For concurrent tests (e.g., concurrent ingest+search):\n     ScheduleExplorer explores all meaningful interleavings\n   - Catches race conditions that non-deterministic tests might miss\n   - Coverage metrics show how many distinct schedules were explored\n\n4. Cancellation injection testing (bd-3un.40 e2e tests):\n   - CancellationInjector: inject cancellation at specific points\n   - Verify: cancel during Phase 0 → clean exit, no leaked resources\n   - Verify: cancel during Phase 1 → Initial results still valid\n   - Verify: cancel during index rebuild → no corrupt files\n\n5. Test fixture corpus (bd-3un.38):\n   - No changes needed — the corpus is data, not async code\n   - But tests using the corpus should use LabRuntime for determinism\n\nEXAMPLE TEST PATTERN:\n\n  #[test]\n  fn progressive_search_deterministic() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          let searcher = setup_test_searcher(&cx).await;\n          let phases: Vec<_> = searcher.search(&cx, \"rust traits\", 10).collect().await;\n\n          assert_eq!(phases.len(), 2);\n          assert!(matches!(phases[0], SearchPhase::Initial { .. }));\n          assert!(matches!(phases[1], SearchPhase::Refined { .. }));\n      });\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }\n\n  #[test]\n  fn cancel_during_quality_embedding() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          let searcher = setup_test_searcher(&cx).await;\n\n          // Race search against a timeout to simulate cancellation\n          match asupersync::combinator::timeout(\n              |cx| searcher.search(&cx, \"rust traits\", 10).collect(),\n              cx.now() + Duration::from_millis(20),  // Cancel during quality embedding\n          ).await {\n              Outcome::Ok(_) => panic!(\"should have timed out\"),\n              Outcome::Cancelled(_) => { /* expected: quality embedding cancelled */ },\n              _ => panic!(\"unexpected outcome\"),\n          }\n      });\n      // Verify no leaked resources even after cancellation\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }","created_at":"2026-02-13T21:06:46Z"},{"id":281,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"REVIEW FIX — Missing test entries and coverage tooling:\n\n1. COVERAGE TOOLING: Specify cargo-llvm-cov for measuring line coverage:\n   cargo +nightly llvm-cov --workspace --html\n   Target: >80% line coverage across the workspace.\n\n2. MISSING TEST ENTRIES — Add test coverage for these components currently absent from bd-3un.31:\n   - Canonicalization (bd-3un.42): NFC normalization, code block collapsing, low-signal filter, truncation, empty output\n   - Reranking (bd-3un.25/26): text lookup, score mismatch handling, skip-on-failure\n   - Staleness detection (bd-3un.41): sentinel file read/write, concurrent reload\n   - Query classification (bd-3un.43): each QueryClass variant, boundary cases, unicode\n   - Score normalization (bd-3un.19): MinMax, z-score edge cases, NaN handling\n   - Two-tier blending (bd-3un.21): alpha values, single-source penalty\n   - Config validation (bd-3un.22): mutual exclusion, invalid values\n\n3. LabRuntime DETERMINISTIC TESTS: All concurrent tests should use LabRuntime with DPOR schedule exploration to catch race conditions deterministically.","created_at":"2026-02-13T21:59:19Z"}]}
{"id":"bd-3un.32","title":"Write integration tests (end-to-end search pipeline)","description":"Write integration tests that exercise the full search pipeline end-to-end using the hash embedder (no ML model downloads needed).\n\nTest scenarios:\n\n1. Basic Two-Tier Flow (with hash embedder as both tiers):\n   - Index 100 test documents\n   - Search with a query\n   - Verify SearchPhase::Initial is yielded first\n   - Verify SearchPhase::Refined is yielded second\n   - Verify result count <= k\n\n2. Hybrid Search (lexical + semantic):\n   - Index documents in both Tantivy and vector index\n   - Search with hybrid mode\n   - Verify RRF fusion produces results from both sources\n   - Verify documents appearing in both rank higher\n\n3. Reranking Integration:\n   - Search produces initial results\n   - Apply rerank step\n   - Verify rerank_score is set\n   - Verify re-ordering occurred\n\n4. Progressive Iterator Contract:\n   - fast_only mode: only Initial phase, no Refined\n   - quality_only mode: no Initial, only one result set\n   - Normal mode: Initial then Refined (exactly 2 phases)\n   - Quality failure: Initial then RefinementFailed\n\n5. Configuration Tests:\n   - Env var overrides work\n   - Builder pattern produces correct config\n   - Invalid blend_factor rejected\n   - Invalid dimensions rejected\n\n6. Persistence Tests:\n   - Create index, close, reopen, search\n   - Multiple embedder IDs in separate index files\n   - Concurrent reads (no data races)\n\nTest data: Use a fixed corpus of 100 synthetic documents covering diverse topics, stored in tests/fixtures/.\n\nAll integration tests should work with 'default' features (hash embedder only, no ML downloads).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:54:09.885746030Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:27.179067760Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["integration","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.32","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:09.885746030Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.32","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T17:55:49.509909806Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.32","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:13:29.179707716Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":13,"issue_id":"bd-3un.32","author":"Dicklesworthstone","text":"REVISION NOTES (review pass):\n\n1. LOGGING REQUIREMENTS: Every integration test must:\n   - Initialize tracing-test subscriber for log capture\n   - Log timing for each pipeline stage (embed, search, fuse, rerank)\n   - Assert that expected tracing events were emitted with correct fields\n   - On failure: dump full tracing output to stderr for debugging\n\n2. TEST FIXTURES: Use the shared corpus from bd-3un.38 (tests/fixtures/) -- do NOT create ad-hoc test data inline. The ground truth relevance data enables NDCG@10 regression checks.\n\n3. DETAILED FAILURE OUTPUT: When a test fails, it should print:\n   - Query text\n   - Expected top-10 doc_ids (from ground truth)\n   - Actual top-10 doc_ids with scores\n   - Phase (Initial vs Refined)\n   - Latency breakdown\n   - All tracing spans for the failed query\n\n4. FEATURE MATRIX: Tests should run across feature combinations:\n   - default (hash only): basic pipeline works\n   - semantic: embedder stack works\n   - lexical: Tantivy integration works\n   - hybrid: fusion works\n   - full: everything works together\n\nUse #[cfg(feature = \"...\")] to conditionally compile feature-specific tests.\n","created_at":"2026-02-13T20:13:25Z"},{"id":87,"issue_id":"bd-3un.32","author":"Dicklesworthstone","text":"GRAVEYARD COMPONENT INTEGRATION TEST COVERAGE (added in refinement pass 2):\n\nEnd-to-end scenarios that exercise graveyard components:\n\n1. S3-FIFO Cache Integration (bd-l7v):\n   - Run 100 searches with cache enabled, verify hit rate > 0 after warm-up\n   - Run same query sequence with/without cache, verify identical result sets (isomorphism)\n   - Run under memory pressure (small cache budget), verify eviction works without panic\n\n2. Score Calibration Integration (bd-22k):\n   - Train calibrators on test fixture corpus (bd-3un.38)\n   - Run full pipeline with Identity vs Isotonic calibration, compare NDCG@10\n   - Verify calibrated scores are in [0,1] range\n   - Verify ranking order preserved (monotonicity)\n\n3. Robust Statistics Integration (bd-1cr):\n   - Run 100 searches, verify TwoTierMetrics reports valid p50/p90/p99\n   - Inject one very slow query, verify median is stable (not pulled by outlier)\n\n4. PhaseGate Integration (bd-2ps):\n   - Run sequence where fast tier matches quality tier, verify SkipQuality eventually triggers\n   - Run sequence where quality tier significantly improves results, verify AlwaysRefine holds\n   - Verify PhaseGate + AdaptiveFusionParams compose without interference","created_at":"2026-02-13T20:52:29Z"},{"id":167,"issue_id":"bd-3un.32","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — TESTING STRATEGY (applies to bd-3un.31, bd-3un.32, bd-3un.38, bd-3un.40):\n\nAll test beads gain access to asupersync's LabRuntime for deterministic testing. This is a MAJOR improvement over non-deterministic std::thread-based testing.\n\nKEY ADDITIONS:\n\n1. LabRuntime for deterministic tests (bd-3un.31, bd-3un.32):\n   - Virtual time: cx.sleep() advances instantly, no real waits\n   - Deterministic scheduling: same seed = same execution order\n   - Reproducible: failing tests can be replayed with exact same schedule\n   - Oracles: automatic verification of correctness properties\n\n2. Oracles to add to every test (bd-3un.31, bd-3un.32):\n   - QuiescenceOracle: verify no orphan tasks after test\n   - ObligationLeakOracle: verify no leaked channel permits\n   - TaskLeakOracle: verify no stray tasks\n   - DeterminismOracle: verify same seed produces same result\n\n3. DPOR schedule exploration (bd-3un.32 integration tests):\n   - For concurrent tests (e.g., concurrent ingest+search):\n     ScheduleExplorer explores all meaningful interleavings\n   - Catches race conditions that non-deterministic tests might miss\n   - Coverage metrics show how many distinct schedules were explored\n\n4. Cancellation injection testing (bd-3un.40 e2e tests):\n   - CancellationInjector: inject cancellation at specific points\n   - Verify: cancel during Phase 0 → clean exit, no leaked resources\n   - Verify: cancel during Phase 1 → Initial results still valid\n   - Verify: cancel during index rebuild → no corrupt files\n\n5. Test fixture corpus (bd-3un.38):\n   - No changes needed — the corpus is data, not async code\n   - But tests using the corpus should use LabRuntime for determinism\n\nEXAMPLE TEST PATTERN:\n\n  #[test]\n  fn progressive_search_deterministic() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          let searcher = setup_test_searcher(&cx).await;\n          let phases: Vec<_> = searcher.search(&cx, \"rust traits\", 10).collect().await;\n\n          assert_eq!(phases.len(), 2);\n          assert!(matches!(phases[0], SearchPhase::Initial { .. }));\n          assert!(matches!(phases[1], SearchPhase::Refined { .. }));\n      });\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }\n\n  #[test]\n  fn cancel_during_quality_embedding() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          let searcher = setup_test_searcher(&cx).await;\n\n          // Race search against a timeout to simulate cancellation\n          match asupersync::combinator::timeout(\n              |cx| searcher.search(&cx, \"rust traits\", 10).collect(),\n              cx.now() + Duration::from_millis(20),  // Cancel during quality embedding\n          ).await {\n              Outcome::Ok(_) => panic!(\"should have timed out\"),\n              Outcome::Cancelled(_) => { /* expected: quality embedding cancelled */ },\n              _ => panic!(\"unexpected outcome\"),\n          }\n      });\n      // Verify no leaked resources even after cancellation\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }","created_at":"2026-02-13T21:06:47Z"},{"id":282,"issue_id":"bd-3un.32","author":"Dicklesworthstone","text":"REVIEW FIX — CI strategy, persistence round-trip, and config loading:\n\n1. CI STRATEGY for feature matrix: Use GitHub Actions matrix strategy:\n   strategy:\n     matrix:\n       features: [default, semantic, lexical, hybrid, full]\n   This runs integration tests across all feature combinations in parallel.\n\n2. PERSISTENCE ROUND-TRIP TEST: Add test:\n   - Create TwoTierIndex with known documents\n   - Save to disk (FSVI format)\n   - Drop all in-memory state\n   - Reload from disk\n   - Search and verify identical results to pre-save search\n   This catches serialization bugs and ensures index durability.\n\n3. CONFIG FILE LOADING TEST: Add test:\n   - Write frankensearch.toml with custom config values\n   - Load TwoTierConfig::from_file()\n   - Verify all values match the TOML file\n   - Verify defaults for omitted values","created_at":"2026-02-13T21:59:27Z"}]}
{"id":"bd-3un.33","title":"Write benchmarks for performance-critical paths","description":"Write criterion benchmarks for the performance-critical paths. These establish regression baselines and verify we meet performance budgets.\n\nBenchmark groups:\n\n1. SIMD Dot Product:\n   - f16×f32 dot product: 128, 256, 384, 768 dimensions\n   - f32×f32 dot product: same dimensions\n   - Scalar vs SIMD comparison\n   - Target: < 1μs for 384-dim\n\n2. Hash Embedder:\n   - Short text (10 words): target < 0.1ms\n   - Medium text (100 words): target < 0.5ms\n   - Long text (1000 words): target < 2ms\n\n3. Vector Search (brute-force):\n   - 1K vectors × 384-dim: target < 1ms\n   - 10K vectors × 384-dim: target < 15ms\n   - 100K vectors × 384-dim: target < 150ms\n\n4. RRF Fusion:\n   - 100 lexical + 100 semantic results: target < 0.5ms\n   - 1000 + 1000 results: target < 5ms\n\n5. Score Normalization:\n   - 100 scores: target < 0.01ms\n   - 10K scores: target < 0.1ms\n\n6. Vector Index I/O:\n   - Write 10K records: target < 500ms\n   - Open/mmap existing index: target < 10ms\n\nPerformance budgets (from cass/xf):\n- Full pipeline (hash embed + search + fusion): < 50ms for 10K docs\n- These match the perf.rs budgets from the source projects\n\nDependencies: criterion = '0.5' (dev-dependency)\nFile: benches/search_bench.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:09.967451570Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:31.652164793Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["benchmarks","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.33","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:09.967451570Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.33","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T17:55:49.592472722Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":39,"issue_id":"bd-3un.33","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (Benchmarks)\n\n## Profile-Driven Benchmark Design\n\n### Mandatory Baseline Protocol\n\nBEFORE any optimization, capture golden baselines per the extreme-optimization methodology:\n\n  hyperfine --warmup 3 --runs 10 'cargo run --example bench_quick'\n\n### Benchmark Groups with Opportunity Matrices\n\nEach benchmark should compute AND REPORT an opportunity matrix:\n\n  | Function            | p50 (us) | p99 (us) | Budget (us) | Headroom | Priority |\n  |---------------------|----------|----------|-------------|----------|----------|\n  | dot_product_f16_f32 | 1.2      | 1.8      | 2.0         | 10%      | Low      |\n  | search_top_k_10K    | 12000    | 15000    | 15000       | 0%       | HIGH     |\n  | rrf_fuse_200        | 45       | 80       | 500         | 84%      | Low      |\n\nAuto-classify: if headroom < 20%, priority = HIGH (at risk of budget violation).\n\n### Regression Detection with Statistical Rigor\n\nInstead of simple \"is it faster?\", use proper statistical testing:\n\n1. Welch's t-test for paired comparisons (accounts for unequal variances)\n2. Effect size (Cohen's d) to distinguish meaningful vs statistically-significant-but-tiny\n3. Only report regressions when:\n   - p-value < 0.01 AND\n   - Cohen's d > 0.5 (medium effect) AND\n   - Absolute change > 5% of budget\n\nThis prevents false alarm fatigue from noisy CI benchmarks.\n\n### Cache State Awareness\n\nBenchmarks MUST test both cold and warm cache states:\n\n  // Cold cache: drop OS page cache before each run\n  sync && echo 3 > /proc/sys/vm/drop_caches  // (requires root)\n\n  // Warm cache: run 3x warmup before measuring\n  for _ in 0..3 { search_top_k(index, query, 10); }\n\nReport both. Cold cache numbers are what users experience on first query; warm cache is steady-state.\n\n### Flamegraph Integration\n\nAuto-generate flamegraphs when a benchmark exceeds budget:\n\n  if measured_p99 > budget * 0.9 {\n      // Trigger: cargo flamegraph --bench search_bench -- --bench search_top_k_10K\n      // Save to benches/flamegraphs/{date}_{benchmark}.svg\n  }\n\n### Isomorphism Proofs in Benchmarks\n\nEvery benchmark that tests an optimization should VERIFY golden outputs:\n\n  #[bench]\n  fn bench_search_with_prefetch(b: &mut Bencher) {\n      // Setup: compute golden results WITHOUT optimization\n      let golden = search_top_k_baseline(index, query, 10);\n\n      b.iter(|| {\n          let results = search_top_k_optimized(index, query, 10);\n          // VERIFY: same results as golden\n          assert_eq!(results.len(), golden.len());\n          for (r, g) in results.iter().zip(golden.iter()) {\n              assert_eq!(r.doc_id, g.doc_id);\n              assert!((r.score - g.score).abs() < 1e-6);\n          }\n      });\n  }\n\nThis ensures optimizations NEVER silently change behavior.\n","created_at":"2026-02-13T20:32:42Z"},{"id":283,"issue_id":"bd-3un.33","author":"Dicklesworthstone","text":"REVIEW FIX — Privilege requirements and additional benchmarks:\n\n1. PRIVILEGE NOTES: cargo flamegraph requires perf_event_open (root or perf_event_paranoid=1). Cold-cache benchmark (echo 3 > /proc/sys/vm/drop_caches) requires root. Note these are for local development only, not CI.\n\n2. ADDITIONAL BENCHMARKS:\n   - Full TwoTierSearcher pipeline: end-to-end search latency at various corpus sizes\n   - Tantivy indexing throughput: documents/second for 1K, 10K, 100K docs\n   - RRF fusion: measure fusion overhead as function of result count\n   - Rerank latency: cross-encoder inference time per document","created_at":"2026-02-13T21:59:31Z"}]}
{"id":"bd-3un.34","title":"Write API documentation and usage examples","description":"Write comprehensive API documentation (rustdoc) and usage examples for frankensearch.\n\nDocumentation requirements:\n1. Crate-level docs (lib.rs) with:\n   - Overview of the 2-tier hybrid search architecture\n   - Quick start example\n   - Feature flag guide\n   - Performance characteristics\n\n2. Module-level docs for each public module explaining purpose and usage\n\n3. All public types and functions must have rustdoc comments with:\n   - Description of what it does\n   - Parameter explanations\n   - Return value semantics\n   - Error conditions\n   - Usage example (at least for key APIs)\n\n4. Examples directory (examples/):\n   - basic_search.rs: Hash embedder + vector search (no ML deps)\n   - hybrid_search.rs: Tantivy + semantic fusion\n   - two_tier_search.rs: Progressive search with fast + quality\n   - custom_embedder.rs: Implementing a custom Embedder trait\n   - index_documents.rs: Building an index from scratch\n\n5. Architecture overview in docs:\n   - Explain the 2-tier concept and why it exists\n   - Link to the X post / bakeoff results\n   - Explain RRF fusion algorithm\n   - Show the data flow diagram\n   - Performance comparison table (hash vs potion vs MiniLM)\n\nEach example should be self-contained and compilable with appropriate feature flags.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:20.698803786Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:35.884691788Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","examples","phase12"],"dependencies":[{"issue_id":"bd-3un.34","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:20.698803786Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.34","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T17:55:49.673691571Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":253,"issue_id":"bd-3un.34","author":"Dicklesworthstone","text":"REVISION: Documentation and Examples Plan\n\n1. Examples MUST include the IndexBuilder API (per bd-3un.30):\n\n   examples/index_and_search.rs -- The \"hello world\" of frankensearch:\n     use frankensearch::{IndexBuilder, TwoTierSearcher, Cx};\n\n     async fn main(cx: &Cx) {\n         // Build index\n         let builder = IndexBuilder::new(\"./my_index\").build(cx).await?;\n         builder.add_document(\"doc-1\", \"Rust ownership and borrowing\", None).await?;\n         builder.add_document(\"doc-2\", \"Python garbage collection\", None).await?;\n         let stats = builder.finish(cx).await?;\n         println!(\"Indexed {} documents\", stats.doc_count);\n\n         // Search\n         let searcher = TwoTierSearcher::auto(\"./my_index\", cx).await?;\n         let mut stream = searcher.search(cx, \"memory management\", 10);\n         while let Some(phase) = stream.next().await {\n             match phase {\n                 SearchPhase::Initial { results, .. } => println!(\"Fast: {} results\", results.len()),\n                 SearchPhase::Refined { results, .. } => println!(\"Quality: {} results\", results.len()),\n                 SearchPhase::RefinementFailed { initial, .. } => println!(\"Using fast results\"),\n             }\n         }\n     }\n\n2. Feature-Gated Examples:\n   - examples/basic_search.rs (default features, hash embedder)\n   - examples/hybrid_search.rs (features = ['hybrid'])\n   - examples/custom_embedder.rs (implementing the Embedder trait)\n   - examples/with_reranking.rs (features = ['full'])\n\n3. Rustdoc Requirements:\n   - Every public type: one-sentence summary + usage example\n   - Every error variant: when it occurs + recovery guidance\n   - TwoTierConfig: document every field with default value and valid range\n   - Feature flags: document in crate-level docs with a feature matrix table\n\n4. Architecture Docs:\n   - Data flow diagram (ASCII art in lib.rs doc comment)\n   - Performance comparison table (hash vs potion vs MiniLM)\n   - Feature flag decision tree (\"which features do I need?\")\n","created_at":"2026-02-13T21:54:12Z"},{"id":284,"issue_id":"bd-3un.34","author":"Dicklesworthstone","text":"REVIEW FIX — Doctest requirements and usage examples:\n\n1. DOCTEST MANDATE: All rustdoc examples MUST be valid doctests (cargo test --doc). This ensures examples stay in sync with the API.\n\n2. ADDITIONAL EXAMPLES:\n   - Error handling pattern: what to do with SearchResult (match on Ok/Err, log errors, degrade gracefully)\n   - Canonicalizer usage: implementing a custom Canonicalizer for domain-specific preprocessing\n   - Config from file: loading TwoTierConfig from frankensearch.toml\n   - Async context: how to pass &Cx through the search pipeline","created_at":"2026-02-13T21:59:35Z"}]}
{"id":"bd-3un.35","title":"Migrate xf to use frankensearch crate","description":"Replace xf's bespoke search implementation with frankensearch. xf was one of the two original projects (along with cass) where the 2-tier hybrid search was developed.\n\nFiles to replace in /data/projects/xf:\n- src/embedder.rs → use frankensearch::Embedder trait\n- src/hash_embedder.rs → use frankensearch::HashEmbedder\n- src/fastembed_embedder.rs → use frankensearch::FastEmbedEmbedder\n- src/model2vec_embedder.rs → use frankensearch::Model2VecEmbedder\n- src/static_mrl_embedder.rs → use frankensearch MRL support\n- src/vector.rs → use frankensearch::VectorIndex\n- src/hybrid.rs → use frankensearch::{rrf_fuse, blend_two_tier}\n- src/search.rs → use frankensearch::LexicalIndex\n- src/reranker.rs → use frankensearch::Reranker trait\n- src/flashrank_reranker.rs → use frankensearch::FlashRankReranker\n- src/mxbai_reranker.rs → adapt to frankensearch\n- src/rerank_step.rs → use frankensearch::RerankStep\n- src/model_registry.rs → use frankensearch::EmbedderRegistry\n- src/config.rs (TwoTierConfig) → use frankensearch::TwoTierConfig\n\nMigration strategy:\n1. Add frankensearch as workspace dependency\n2. Replace trait definitions with re-exports\n3. Replace implementations one at a time, verifying tests pass\n4. Remove replaced source files\n5. Update Cargo.toml to remove now-unused direct deps\n6. Run full test suite + bakeoff validation\n\nBinary format migration:\n- xf uses XFVI magic → frankensearch uses FSVI magic\n- Need a one-time migration tool or support reading legacy format\n- OR: rebuild indices (simpler, preferred)\n\nVerify: all xf search tests pass, bakeoff results unchanged.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:54.296387127Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:40.262654588Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["migration","phase13","xf"],"dependencies":[{"issue_id":"bd-3un.35","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:54.296387127Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.35","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T17:55:49.754624203Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.35","depends_on_id":"bd-3un.40","type":"blocks","created_at":"2026-02-13T20:13:47.786058255Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":8,"issue_id":"bd-3un.35","author":"Dicklesworthstone","text":"XF MIGRATION NOTES: xf has the most mature two-tier implementation since it was the first project where this was developed. The xf migration is the easiest because its search code is relatively self-contained in individual files.\n\nBinary index migration: xf currently stores vector.fast.idx and vector.quality.idx with XFVI magic bytes. After migration, these become FSVI format. Since index rebuilding is fast (seconds for typical X archive sizes), the simplest approach is to delete old indices and rebuild with 'xf index --rebuild'.\n\nImportant xf-specific code to NOT migrate (keep in xf):\n- Tweet/Like/DM/Grok type handling (domain-specific)\n- X archive parsing (totally unrelated to search)\n- TUI rendering (uses frankensearch search results but renders its own way)\n- Config file format (xf has its own config.toml structure)","created_at":"2026-02-13T17:57:22Z"},{"id":285,"issue_id":"bd-3un.35","author":"Dicklesworthstone","text":"REVIEW FIX — Async migration impact and rebuild command:\n\n1. ASYNC MIGRATION IMPACT: xf currently uses synchronous search calls. After switching to frankensearch (with asupersync), xf must adopt async or use search_blocking(). Since xf has a TUI (ratatui), the recommended approach:\n   - xf's main event loop runs in an asupersync region\n   - Search calls are async within that region\n   - TUI rendering remains synchronous (ratatui is sync)\n\n2. REBUILD COMMAND: Add `xf index --rebuild` CLI command that:\n   - Reads existing tweets/likes/DMs from the SQLite database\n   - Embeds all documents with the new embedder stack\n   - Writes new FSVI index files\n   - Validates by running the ground truth queries\n\n3. VALIDATION CRITERION: \"All 20 ground truth queries produce identical top-10 results\" or, if ranking changes due to improved embeddings, \"NDCG@10 >= current baseline for all ground truth queries.\"","created_at":"2026-02-13T21:59:40Z"}]}
{"id":"bd-3un.36","title":"Migrate cass to use frankensearch crate","description":"Replace cass (coding_agent_session_search) search implementation with frankensearch.\n\nFiles to replace in /data/projects/coding_agent_session_search:\n- src/search/embedder.rs → frankensearch::Embedder\n- src/search/hash_embedder.rs → frankensearch::HashEmbedder\n- src/search/fastembed_embedder.rs → frankensearch::FastEmbedEmbedder\n- src/search/vector_index.rs → frankensearch::VectorIndex\n- src/search/ann_index.rs → frankensearch HNSW (if 'ann' feature)\n- src/search/two_tier_search.rs → frankensearch::TwoTierSearcher\n- src/search/tantivy.rs → frankensearch::LexicalIndex\n- src/search/reranker.rs → frankensearch::Reranker\n- src/search/fastembed_reranker.rs → frankensearch reranker impl\n- src/search/embedder_registry.rs → frankensearch::EmbedderRegistry\n- src/search/reranker_registry.rs → frankensearch reranker registry\n- src/search/model_download.rs → frankensearch model download\n- src/search/daemon_client.rs → keep (cass-specific daemon protocol)\n- src/search/query.rs → adapt to use frankensearch types\n\nNote: cass has a daemon_client.rs that forwards embedding requests to a background daemon process. This is cass-specific and should NOT be in frankensearch. Instead, cass should implement frankensearch::Embedder with a DaemonEmbedder that wraps the daemon protocol.\n\nBinary format: cass uses CVVI magic with 70-byte domain-specific rows. These domain fields (MessageID, AgentID, WorkspaceID, Role, ChunkIdx) don't belong in frankensearch. The migration should store these in a separate metadata index, with frankensearch only owning the vector data.\n\nMigration strategy:\n1. Add frankensearch dependency with features = ['full']\n2. Create adapter types for cass-specific needs (DaemonEmbedder, MetadataIndex)\n3. Replace search modules one at a time\n4. Verify cass_bakeoff_validation.sh passes (NDCG@10 >= 0.25)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:54.384496776Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:45.229830962Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cass","migration","phase13"],"dependencies":[{"issue_id":"bd-3un.36","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:54.384496776Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.36","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T17:55:49.835226386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.36","depends_on_id":"bd-3un.40","type":"blocks","created_at":"2026-02-13T20:13:47.869309673Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":9,"issue_id":"bd-3un.36","author":"Dicklesworthstone","text":"CASS MIGRATION NOTES: cass is the most complex migration because it has:\n1. A daemon_client.rs that forwards embedding requests to a hot background process\n2. Domain-specific 70-byte CVVI rows with MessageID, AgentID, WorkspaceID, Role, ChunkIdx\n3. Tight coupling between search and the session data model\n\nStrategy: cass should implement frankensearch::Embedder with a DaemonEmbedder wrapper that uses the existing daemon protocol. The daemon itself can use frankensearch internally for the actual embedding computation.\n\nFor the CVVI row metadata: create a cass-specific MetadataIndex that maps doc_id → (MessageID, AgentID, etc.). The frankensearch VectorIndex only stores (doc_id, embedding). This separation is cleaner and lets frankensearch stay domain-agnostic.","created_at":"2026-02-13T17:57:22Z"},{"id":286,"issue_id":"bd-3un.36","author":"Dicklesworthstone","text":"REVIEW FIX — Adapter types, NDCG threshold, and async impact:\n\n1. UNTRACKED ADAPTER TYPES: MetadataIndex and DaemonEmbedder are new components needed for the cass migration but not tracked in any bead. These should be documented as part of this bead's deliverables:\n   - MetadataIndex: wraps frankensearch index with cass-specific 70-byte CVVI row metadata\n   - DaemonEmbedder: implements frankensearch::Embedder trait, delegates to cass's embedding daemon\n\n2. NDCG THRESHOLD: NDCG@10 >= 0.25 is very low. Change to:\n   - NDCG@10 must not regress from current baseline (measure before migration, compare after)\n   - Minimum absolute threshold: NDCG@10 >= 0.5\n\n3. ASYNC IMPACT: cass needs to adopt async (asupersync) or use search_blocking() for synchronous contexts. Document the migration strategy.","created_at":"2026-02-13T21:59:45Z"}]}
{"id":"bd-3un.37","title":"Migrate mcp_agent_mail_rust to use frankensearch crate","description":"Replace mcp_agent_mail_rust search implementation with frankensearch. Agent-mail was the most recent adopter and its search-core crate already has the cleanest separation.\n\nFiles to replace in /data/projects/mcp_agent_mail_rust:\n- crates/mcp-agent-mail-search-core/src/two_tier.rs → frankensearch::TwoTierSearcher\n- crates/mcp-agent-mail-search-core/src/embedder.rs → frankensearch::Embedder\n- crates/mcp-agent-mail-search-core/src/model2vec.rs → frankensearch::Model2VecEmbedder\n- crates/mcp-agent-mail-search-core/src/fastembed.rs → frankensearch::FastEmbedEmbedder\n- crates/mcp-agent-mail-search-core/src/auto_init.rs → frankensearch::EmbedderStack\n- crates/mcp-agent-mail-search-core/src/vector_index.rs → frankensearch::VectorIndex\n- crates/mcp-agent-mail-search-core/src/fusion.rs → frankensearch::{rrf_fuse, RrfConfig}\n- crates/mcp-agent-mail-search-core/src/embedding_jobs.rs → frankensearch background queue\n\nKeep agent-mail-specific:\n- crates/mcp-agent-mail-search-core/src/hybrid_candidates.rs (domain-specific budget logic)\n- crates/mcp-agent-mail-search-core/src/lexical_parser.rs (agent-mail query syntax)\n- crates/mcp-agent-mail-db/src/search_planner.rs (domain-specific query planning)\n- crates/mcp-agent-mail-db/src/embeddings.rs (SQLite persistence)\n\nThe mcp-agent-mail-search-core crate should become a thin wrapper around frankensearch with agent-mail-specific query parsing and persistence.\n\nMigration strategy:\n1. Add frankensearch = { path = '../../frankensearch', features = ['hybrid'] }\n2. Replace types and implementations incrementally\n3. Run search V3 quality gates (SPEC-search-v3-quality-gates.md)\n4. Verify TUI search still works with progressive display","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:54.462089901Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:53.908530724Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-mail","migration","phase13"],"dependencies":[{"issue_id":"bd-3un.37","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:54.462089901Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.37","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T17:55:49.917870995Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.37","depends_on_id":"bd-3un.40","type":"blocks","created_at":"2026-02-13T20:13:47.954171906Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":10,"issue_id":"bd-3un.37","author":"Dicklesworthstone","text":"AGENT-MAIL MIGRATION NOTES: agent-mail is the most recent adopter and already has the cleanest separation (its search-core crate). The migration should:\n1. Replace mcp-agent-mail-search-core internals with frankensearch\n2. Keep mcp-agent-mail-db as the persistence layer (SQLite embeddings table)\n3. Keep domain-specific query planning in search_planner.rs\n\nagent-mail's feature-gated approach (semantic, hybrid features) maps directly to frankensearch's feature flags. The workspace Cargo.toml can forward features:\n  frankensearch = { version = '0.1', features = ['hybrid'] }\n\nThe TUI progressive display already consumes SearchPhase — it should work identically with frankensearch's SearchPhase enum.","created_at":"2026-02-13T17:57:22Z"},{"id":289,"issue_id":"bd-3un.37","author":"Dicklesworthstone","text":"REVIEW FIX — Quality gate reference, embedding_jobs.rs scope, and async:\n\n1. QUALITY GATE: The migration references \"search V3 quality gates (SPEC-search-v3-quality-gates.md)\" without inlining the criteria. Key quality gates for the migration:\n   - Recall@10 >= 0.85 on the agent-mail ground truth corpus\n   - P95 search latency < 200ms for Phase 1 (Initial)\n   - P95 search latency < 500ms for Phase 2 (Refined)\n   - Zero search errors on the validation suite\n   - Progressive display: TUI shows Initial results within 50ms of search start\n\n2. embedding_jobs.rs MIGRATION SCOPE: Not all of embedding_jobs.rs migrates to frankensearch. Keep in agent-mail:\n   - Domain-specific batch limits (per mailbox)\n   - Priority ordering (new mail > old mail)\n   - Mailbox-specific embedding schedules\n   Migrate to frankensearch (via bd-3un.27 job queue):\n   - Backpressure logic\n   - Embedding dispatch\n   - Content hash dedup\n\n3. ASYNC COMPATIBILITY: agent-mail already uses an async runtime. The migration to asupersync is straightforward since the TUI progressive display already consumes SearchPhase. Replace the current async runtime integration with asupersync::region().","created_at":"2026-02-13T21:59:53Z"}]}
{"id":"bd-3un.38","title":"Create test fixture corpus and ground truth relevance data","description":"Create a synthetic test fixture corpus for use by all test beads. This corpus must be realistic enough to validate search quality without requiring ML model downloads.\n\nCorpus spec (stored in tests/fixtures/):\n- 100 synthetic documents across 5 topic clusters:\n  1. Programming/Rust (20 docs): ownership, borrowing, async, traits, error handling\n  2. Machine Learning (20 docs): embeddings, transformers, tokenization, ONNX, fine-tuning\n  3. System Admin (20 docs): Docker, Kubernetes, networking, monitoring, deployment\n  4. Cooking/Recipes (20 docs): completely unrelated domain for negative testing\n  5. Mixed/Overlap (20 docs): documents spanning 2+ topics for fusion testing\n\nEach document has:\n- doc_id: deterministic format \"test-{cluster}-{n:03}\" (e.g., \"test-rust-007\")\n- content: 50-200 words of realistic text (NOT lorem ipsum)\n- title: short descriptive title\n- created_at: staggered timestamps across a 30-day range\n- doc_type: cluster name\n- metadata: JSON with word_count, reading_level, language\n\nGround truth relevance file (tests/fixtures/relevance.json):\n- For 20 predefined test queries, the expected top-10 relevant doc_ids\n- This enables NDCG@10 computation for regression testing\n- Queries span exact match, semantic similarity, cross-topic, and negative cases\n\nExample ground truth entries:\n  \"rust ownership\" -> [test-rust-001, test-rust-003, test-rust-012, ...]\n  \"how to deploy containers\" -> [test-sysadmin-005, test-sysadmin-011, ...]\n  \"chocolate chip cookies\" -> [test-cooking-002, test-cooking-015, ...] (NO programming docs)\n\nAlso include:\n- tests/fixtures/edge_cases.json: Empty strings, unicode, very long text, single word, special chars\n- tests/fixtures/README.md: Documenting the corpus structure and how to regenerate\n\nThe corpus should be committed to the repo (small enough) so tests are reproducible without any setup.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:11:37.574321889Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:51.634568039Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fixtures","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.38","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:11:37.574321889Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.38","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T20:11:41.570461740Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":49,"issue_id":"bd-3un.38","author":"Dicklesworthstone","text":"REVISION: Test Fixture Corpus Enhancement\n\nThe current spec covers 100 docs across 5 topic clusters with 20 ground truth queries. This is a good foundation but needs the following additions for production-quality testing:\n\n1. Adversarial Inputs (add 20 docs to corpus, total 120):\n   - Empty string document (doc_id: \"adversarial_empty\")\n   - Single-character document: \"x\"\n   - Very long document: 50,000 chars of repeated text (tests truncation)\n   - Unicode stress test: CJK characters, RTL Arabic, emoji sequences, combining diacriticals\n   - Mixed-script: \"Hello\" in 10 languages within one document\n   - Code-heavy: 95% code block with minimal natural language\n   - Low-signal: document of only stopwords (\"the a an is are was were\")\n   - HTML injection attempt: \"<script>alert('xss')</script>\"\n   - SQL-like content: \"DROP TABLE documents; SELECT * FROM users\"\n   - Null bytes and control characters in content\n   - Document with identical content but different doc_id (tests dedup)\n   - Document with very long doc_id (1000 chars)\n\n2. Multilingual Samples (potion-128M is \"multilingual\"):\n   - 5 docs in German, 5 in French (within the 20 additional docs)\n   - Ground truth queries in non-English: test that potion handles multilingual correctly\n   - Cross-lingual query: English query should find relevant German doc\n\n3. Canonicalization Edge Cases:\n   - Document with nested markdown: headers, lists, code blocks, tables\n   - Document with only a code block (no natural language)\n   - Document with excessive whitespace and newlines\n   - Document with markdown frontmatter (YAML header)\n\n4. Pre-computed Hash Embeddings:\n   - Store golden hash embeddings for 10 key documents\n   - Regression test: hash_embedder(doc) == stored_golden_embedding\n   - This catches accidental changes to the hash function\n\n5. Ground Truth Enhancement:\n   - Add 5 adversarial queries to the existing 20:\n     * Empty query: \"\" (should return empty results)\n     * Single char: \"x\" (edge case)\n     * Very long query: 500 words (tests truncation)\n     * Query matching no documents (should return empty, not error)\n     * Boolean query: \"rust AND NOT cooking\"\n   - Total: 25 ground truth queries with expected top-10 doc_ids\n\n6. Fixture Format:\n   - fixtures/corpus.json: all documents\n   - fixtures/queries.json: all ground truth queries with relevance judgments\n   - fixtures/golden_embeddings.json: pre-computed hash embeddings\n   - fixtures/edge_cases.json: adversarial inputs with expected behaviors\n   - fixtures/README.md: document the fixture design decisions\n","created_at":"2026-02-13T20:44:53Z"},{"id":288,"issue_id":"bd-3un.38","author":"Dicklesworthstone","text":"REVIEW FIX — Document count, hash-based ground truth, and file format:\n\n1. DOCUMENT COUNT: Body says 100 documents, revision adds 20 adversarial → total 120. Update body to 120 total: 100 clustered + 20 adversarial.\n\n2. HASH-BASED GROUND TRUTH CAVEAT: Ground truth relevance judgments are meaningful only for real embedding models (potion, MiniLM). For hash-based tests, the corpus validates pipeline correctness (phases, structure, error handling) rather than ranking quality. Add this clarification.\n\n3. FILE FORMAT DOCUMENTATION: Add explicit JSON schema definitions:\n   - corpus.json: [{ id: string, title: string, body: string, cluster: string, metadata: object }]\n   - queries.json: [{ query: string, relevant_ids: [string], query_class: string }]\n   - golden_embeddings.json: { embedder: string, vectors: { doc_id: [f32] } }","created_at":"2026-02-13T21:59:51Z"}]}
{"id":"bd-3un.39","title":"Add structured tracing/logging throughout pipeline","description":"Add structured tracing and logging throughout the entire frankensearch pipeline using the tracing crate. This is critical for debugging, performance monitoring, and the detailed logging the e2e test scripts need.\n\nTracing configuration:\n\n1. Crate-level subscriber setup (optional, consumer can bring their own):\n   pub fn init_default_tracing(level: tracing::Level) {\n       tracing_subscriber::fmt()\n           .with_env_filter(EnvFilter::from_default_env()\n               .add_directive(format!(\"frankensearch={}\", level).parse().unwrap()))\n           .with_timer(tracing_subscriber::fmt::time::uptime())\n           .with_target(true)\n           .with_thread_ids(true)\n           .init();\n   }\n\n2. Instrument key operations with spans:\n\n   Embedding:\n   #[tracing::instrument(skip(text), fields(model = %self.id(), dim = self.dimension(), text_len = text.len()))]\n   fn embed(&self, text: &str) -> SearchResult<Vec<f32>>\n\n   Vector search:\n   #[tracing::instrument(skip(index, query), fields(index_size = index.record_count(), dim = index.dimension(), k))]\n   fn search_top_k(...)\n\n   RRF fusion:\n   #[tracing::instrument(skip(lexical, semantic), fields(lexical_count = lexical.len(), semantic_count = semantic.len(), k = config.k))]\n   fn rrf_fuse(...)\n\n   Two-tier search (the big one):\n   #[tracing::instrument(skip(self), fields(query_len = query.len(), k, mode))]\n   fn search(&self, query: &str, k: usize) -> TwoTierSearchIter\n\n3. Key events to log:\n\n   INFO level:\n   - \"index_opened\" { path, record_count, dimension, embedder_id, format_version }\n   - \"model_loaded\" { model_name, dimension, load_time_ms, category }\n   - \"search_completed\" { phase, result_count, latency_ms, query_len }\n   - \"index_rebuilt\" { old_count, new_count, rebuild_time_ms }\n\n   DEBUG level:\n   - \"query_embedded\" { model, dimension, latency_us }\n   - \"vector_search_done\" { candidates_scanned, results_returned, latency_ms }\n   - \"rrf_fused\" { lexical_candidates, semantic_candidates, fused_count, overlap_count }\n   - \"scores_blended\" { fast_count, quality_count, blend_factor, rank_correlation }\n   - \"rerank_done\" { model, candidates_in, latency_ms, score_range }\n\n   WARN level:\n   - \"quality_model_unavailable\" { reason, fallback }\n   - \"refinement_timeout\" { budget_ms, elapsed_ms }\n   - \"backpressure_triggered\" { queue_depth, threshold }\n\n   TRACE level (very hot path, disabled by default):\n   - Individual dot product scores\n   - Per-record filter decisions\n   - Token-level embedding details\n\n4. Performance timing via tracing spans:\n   Every span automatically gets duration. In tests, use tracing-test crate to capture and assert on logged events.\n\nDependencies:\n- tracing = \"0.1\"\n- tracing-subscriber = \"0.3\" (with fmt, env-filter features)\n- tracing-test = \"0.2\" (dev-dependency, for test assertions on logs)\n\nFile: frankensearch-core/src/tracing_config.rs (optional helpers)\nIntegration: spans and events go in each component's source file\n\nThis bead is about DEFINING the logging schema and ensuring every component instruments properly. The actual log lines go in the component code (per the epic's LOGGING POLICY comment).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:12:04.368694010Z","created_by":"ubuntu","updated_at":"2026-02-13T22:00:00.133030854Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["logging","phase10","quality","tracing"],"dependencies":[{"issue_id":"bd-3un.39","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:12:04.368694010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.39","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T20:12:08.162501866Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":79,"issue_id":"bd-3un.39","author":"Dicklesworthstone","text":"REVISION: Structured Tracing Instrumentation Plan\n\nSystematic tracing is essential for debugging the 2-tier pipeline. Key requirements:\n\n1. Span Hierarchy (parent-child relationship):\n   frankensearch::search (root span per query)\n   +-- phase0::fast_embed (fast-tier embedding)\n   +-- phase0::fast_search (brute-force vector search)\n   +-- phase0::lexical_search (Tantivy BM25, if hybrid)\n   +-- phase0::rrf_fuse (RRF fusion)\n   +-- phase1::quality_embed (quality-tier embedding)\n   +-- phase1::blend (fast+quality score blending)\n   +-- phase1::rerank (cross-encoder reranking, if enabled)\n   +-- phase1::final_fuse (re-fusion with lexical)\n\n2. Mandatory Fields per Span:\n   - query_len: usize (chars, not bytes)\n   - query_class: &str (\"empty\", \"identifier\", \"short_keyword\", \"natural_language\")\n   - phase: &str (\"initial\", \"refined\", \"refinement_failed\")\n   - duration_us: u64 (microseconds)\n\n3. Key Events (structured fields, not string interpolation):\n   INFO events:\n   - index_opened { path, doc_count, fast_dim, quality_dim, format_version }\n   - model_loaded { model_id, dimension, load_time_ms, memory_bytes }\n   - search_completed { query_len, phase, result_count, total_ms, fast_ms, quality_ms }\n   - index_rebuilt { doc_count, duration_ms, fast_size_bytes, quality_size_bytes }\n\n   DEBUG events:\n   - query_embedded { model_id, dimension, duration_us, query_class }\n   - vector_search_done { candidates, top_k, duration_us }\n   - rrf_fused { lexical_count, semantic_count, fused_count, duration_us }\n   - scores_blended { blend_factor, kendall_tau, promoted, demoted, stable }\n\n   WARN events:\n   - quality_model_unavailable { reason, fallback }\n   - refinement_timeout { timeout_ms, phase }\n   - index_stale { reason, last_rebuild, doc_count_mismatch }\n\n   ERROR events:\n   - embedding_failed { model_id, error, query_len }\n   - index_corrupted { path, expected_crc, actual_crc }\n\n4. Subscriber Setup Helper:\n   pub fn init_tracing(level: tracing::Level) -> tracing::subscriber::DefaultGuard\n   Sets up fmt subscriber with:\n   - JSON output when FRANKENSEARCH_LOG_FORMAT=json (for log aggregation)\n   - Pretty output otherwise (human-readable with colors)\n   - Timer with microsecond precision\n   - Thread name in each event\n   - Span events on close (with duration)\n\n5. Performance Budget:\n   Tracing overhead MUST be < 1% of total search time.\n   Use tracing's compile-time filtering (max_level_info for release builds).\n   DEBUG and TRACE events compiled out in release mode.\n","created_at":"2026-02-13T20:47:17Z"},{"id":165,"issue_id":"bd-3un.39","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — TRACING INTEGRATION:\n\nasupersync has its own observability module (observability/) with structured logging, metrics, and task inspection. The frankensearch tracing layer should integrate with both:\n1. The standard `tracing` crate (for consumers who use tracing-subscriber)\n2. asupersync's observability module (for consumers who use asupersync's diagnostics)\n\nasupersync also has a `tracing-integration` feature flag for bridging between the two.\n\nADDITION: Use cx.trace(event) for asupersync-native event recording alongside tracing::instrument:\n\n  #[tracing::instrument(skip(cx, self))]\n  pub async fn search(&self, cx: &Cx, query: &str, k: usize) {\n      cx.trace(TraceEvent::new(\"search_start\").with_field(\"query_len\", query.len()));\n      // ...\n      cx.trace(TraceEvent::new(\"search_complete\").with_field(\"result_count\", results.len()));\n  }\n\nThis gives dual output: tracing spans for standard subscribers + asupersync trace events for lab runtime replay and diagnostics.","created_at":"2026-02-13T21:06:44Z"},{"id":291,"issue_id":"bd-3un.39","author":"Dicklesworthstone","text":"REVIEW FIX — Dual tracing overhead and compile-time level control:\n\n1. DUAL TRACING OVERHEAD: Both tracing crate and asupersync cx.trace() emit events for instrumented functions. Benchmark the overhead. If dual tracing exceeds 1% of search time:\n   - Make asupersync tracing opt-in: #[cfg(feature = \"asupersync-trace\")]\n   - The tracing crate remains the primary observability layer\n\n2. COMPILE-TIME LEVEL CONTROL: Use tracing's compile-time features to strip expensive trace levels in release builds:\n   [dependencies]\n   tracing = { version = \"0.1\", features = [\"max_level_info\"] }  # Release profile\n   This makes TRACE and DEBUG events zero-cost in release.\n\n3. GUARD ON DEBUG-ONLY EVENTS: Use #[cfg(debug_assertions)] for TRACE-level events that are only useful during development.","created_at":"2026-02-13T22:00:00Z"}]}
{"id":"bd-3un.4","title":"Define Reranker trait and reranking types","description":"Define the Reranker trait in frankensearch-core. Cross-encoder rerankers are used as a second pass after initial retrieval to improve relevance ordering. Must be object-safe.\n\npub trait Reranker: Send + Sync {\n    /// Score query-document pairs. Returns relevance scores (higher = more relevant).\n    /// The returned Vec must have the same length as documents.\n    fn rerank(&self, query: &str, documents: &[&str]) -> SearchResult<Vec<f32>>;\n    \n    /// Model name identifier.\n    fn model_name(&self) -> &str;\n    \n    /// Maximum input sequence length in tokens.\n    fn max_length(&self) -> usize;\n    \n    /// Whether this reranker is currently available (model loaded).\n    fn is_available(&self) -> bool;\n}\n\nThis is distinct from bi-encoders (Embedder trait) because cross-encoders attend to query+document simultaneously, producing a relevance score rather than a vector.\n\nReference implementations:\n- cass: src/search/reranker.rs (217 lines) - trait with RerankerError variants\n- xf: src/reranker.rs - same trait, slightly different error handling\n- agent-mail: not yet integrated (planned)\n\nThe trait goes in frankensearch-core, implementations in frankensearch-rerank.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:39.952093270Z","created_by":"ubuntu","updated_at":"2026-02-13T21:46:28.203476555Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1","trait"],"dependencies":[{"issue_id":"bd-3un.4","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:39.952093270Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.4","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:00.678125073Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":107,"issue_id":"bd-3un.4","author":"Dicklesworthstone","text":"REVISION: Reranker Trait Design Details\n\n1. Object Safety:\n   The Reranker trait MUST be object-safe (usable as dyn Reranker) because:\n   - The facade stores Box<dyn Reranker> for runtime model selection\n   - Users may implement custom rerankers (e.g., domain-specific scoring)\n   This means: no generics on methods, no Self in return types, &self receivers only\n\n2. Cross-Encoder vs Bi-Encoder:\n   The Reranker trait is DISTINCT from the Embedder trait because:\n   - Embedders (bi-encoders): encode query and document INDEPENDENTLY, compare via cosine\n   - Rerankers (cross-encoders): attend to query AND document TOGETHER in one forward pass\n   - Cross-encoders are ~10x slower but ~5-10% more accurate for relevance scoring\n   - Always used as a second-pass refinement on top-k candidates, never for full-corpus search\n\n3. Thread Safety:\n   Reranker: Send + Sync is required (same as Embedder)\n   In practice, ONNX sessions need Mutex wrapping (same pattern as FastEmbed in bd-3un.8)\n\n4. Batch Reranking:\n   The trait defines rerank(&self, query: &str, documents: &[&str]) -> Vec<f32>\n   This takes ALL candidates in one call for efficient batch inference.\n   Single-document reranking is just a batch of 1.\n\n5. Graceful Unavailability:\n   is_available() returns false when the model isn't loaded yet.\n   The pipeline (bd-3un.26) checks is_available() before calling rerank().\n   If unavailable, the pipeline skips reranking (results still valid from blending).\n","created_at":"2026-02-13T20:57:43Z"},{"id":163,"issue_id":"bd-3un.4","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — CORE TRAIT REVISION (Embedder + Reranker):\n\nThe Embedder and Reranker traits gain a Cx parameter for cancel-aware operations:\n\nBEFORE:\n  pub trait Embedder: Send + Sync {\n      fn embed(&self, text: &str) -> SearchResult<Vec<f32>>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n  }\n\nAFTER:\n  pub trait Embedder: Send + Sync {\n      async fn embed(&self, cx: &Cx, text: &str) -> asupersync::Result<Vec<f32>>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n  }\n\n  pub trait Reranker: Send + Sync {\n      async fn rerank(&self, cx: &Cx, query: &str, docs: &[&str]) -> asupersync::Result<Vec<f32>>;\n  }\n\nKEY IMPLICATIONS:\n1. embed() is now async — enables cancel-aware Mutex acquisition for ONNX sessions\n2. Cx parameter enables: cancellation checks, budget enforcement, tracing\n3. Hash embedder: embed() is synchronous internally but async for trait uniformity\n   (just wraps the sync computation — zero overhead for fast embedders)\n4. Model2Vec: same — synchronous internally, async trait wrapper\n5. FastEmbed: genuinely benefits — cancel-aware Mutex lock on ONNX session\n6. FlashRank reranker: same — cancel-aware Mutex for ONNX session\n\nRETURN TYPE: asupersync::Result<T> instead of our SearchResult<T>. This enables:\n  - Outcome::Cancelled propagation (embedder cancelled = search cancelled)\n  - Outcome::Panicked propagation (ONNX crash = graceful RefinementFailed)\n  - Integration with asupersync's error recovery actions\n\nOBJECT SAFETY: async trait methods require dyn-compatible async traits.\n  Option A: Use #[async_trait] attribute (allocating but simple)\n  Option B: Use RPITIT (Return Position Impl Trait In Trait) — Rust 2024 nightly supports this\n  Option C: Use asupersync's own async trait pattern\n  DECISION: Use RPITIT (Rust 2024 edition supports it natively, no macro needed)","created_at":"2026-02-13T21:06:37Z"},{"id":224,"issue_id":"bd-3un.4","author":"Dicklesworthstone","text":"REVIEW FIX — Async trait dyn-compatibility resolution:\n\nPROBLEM: The Embedder and Reranker traits must be:\n  1. async (for cancel-aware Mutex acquisition in ONNX embedders)\n  2. dyn-compatible (for runtime polymorphism: Box<dyn Embedder>, Arc<dyn Embedder>)\n\nThese two requirements conflict because `async fn` in traits produces opaque return types that are NOT dyn-compatible, even in Rust 2024 nightly with RPITIT.\n\nRESOLUTION: Use the `trait_variant` crate (rust-lang official, part of async-wg output) to generate both a static-dispatch and dyn-compatible version:\n\n  use trait_variant::make;\n\n  #[make(SendEmbedder: Send)]\n  pub trait Embedder: Sync {\n      async fn embed(&self, cx: &Cx, text: &str) -> Result<Vec<f32>, SearchError>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n      fn id(&self) -> &str;\n      fn is_ready(&self) -> bool;\n  }\n\nThis generates:\n  - `Embedder` — the base async trait (not dyn-compatible)\n  - `SendEmbedder` — auto-generated dyn-compatible variant with Send bounds\n\nUsage:\n  - Concrete types implement `Embedder`\n  - Generic code uses `impl Embedder` or `T: Embedder`\n  - Dynamic dispatch uses `Box<dyn SendEmbedder>` or `Arc<dyn SendEmbedder>`\n\nALTERNATIVE if trait_variant is not desired: Manual desugaring with BoxFuture:\n\n  pub trait Embedder: Send + Sync {\n      fn embed<'a>(&'a self, cx: &'a Cx, text: &'a str) -> Pin<Box<dyn Future<Output = Result<Vec<f32>, SearchError>> + Send + 'a>>;\n      fn dimension(&self) -> usize;\n      fn is_semantic(&self) -> bool;\n      fn category(&self) -> EmbedderCategory;\n      fn id(&self) -> &str;\n      fn is_ready(&self) -> bool;\n  }\n\nThis is dyn-compatible and requires no proc-macro, but is verbose. Implementors can use a helper macro to reduce boilerplate.\n\nDECISION: Use `trait_variant` (Option A). It's the Rust async-wg's official solution, minimal dependency, and generates clean code. Add `trait_variant = \"0.1\"` to workspace dependencies in bd-3un.1.\n\nSAME APPROACH FOR RERANKER:\n\n  #[make(SendReranker: Send)]\n  pub trait Reranker: Sync {\n      async fn rerank(&self, cx: &Cx, query: &str, docs: &[&str]) -> Result<Vec<f32>, SearchError>;\n      fn model_name(&self) -> &str;\n      fn max_length(&self) -> usize;\n      fn is_available(&self) -> bool;\n  }\n\nNON-ASYNC METHODS STAY SYNC: dimension(), is_semantic(), category(), id(), is_ready(), model_name(), max_length(), is_available() all remain synchronous. Only the heavy-compute methods (embed, rerank) are async.\n\nUTILITY FUNCTIONS STAY SYNC: l2_normalize(), cosine_similarity(), truncate_embedding() are standalone functions, not trait methods. They stay synchronous.\n\nTEST REQUIREMENTS for bd-3un.3:\n  - Implement a MockEmbedder for testing (hash-based, deterministic)\n  - Verify dyn SendEmbedder works: Box<dyn SendEmbedder> can call embed()\n  - Verify Arc<dyn SendEmbedder> is Send + Sync\n  - l2_normalize produces unit vectors (norm within f32 epsilon of 1.0)\n  - cosine_similarity(v, v) ≈ 1.0\n  - cosine_similarity of orthogonal vectors ≈ 0.0\n  - truncate_embedding with MRL: verify dimension reduction is correct\n  - Edge cases: empty text, very long text (>8192 chars), unicode text\n\nTEST REQUIREMENTS for bd-3un.4:\n  - Implement a MockReranker for testing\n  - Verify dyn SendReranker works: Box<dyn SendReranker> can call rerank()\n  - rerank() output length matches input docs length\n  - is_available() == false → rerank() returns Err or is skipped by pipeline\n  - Edge cases: empty docs list, single doc, query longer than max_length","created_at":"2026-02-13T21:46:28Z"}]}
{"id":"bd-3un.40","title":"Write e2e validation scripts with detailed logging","description":"Write standalone e2e validation scripts that exercise the full frankensearch pipeline with detailed, colorized logging output. These are NOT cargo tests -- they are runnable scripts/binaries that a developer can execute to validate everything works correctly and see exactly what is happening at each stage.\n\nScripts to create:\n\n1. tests/e2e/validate_full_pipeline.rs (binary target in Cargo.toml)\n   - Requires: hash feature only (no ML model downloads)\n   - Steps:\n     a. Create temp directory\n     b. Load test fixture corpus (from tests/fixtures/)\n     c. Initialize hash embedder\n     d. Build vector index (FSVI format) with progress bar\n     e. Build Tantivy lexical index with progress bar\n     f. Execute 20 predefined queries from ground truth\n     g. For each query:\n        - Log: query text, expected results\n        - Phase 1 (Initial): log latency, top-5 results with scores\n        - Phase 2 (Refined): log latency, top-5 results with scores, rank changes\n        - Compute NDCG@10 against ground truth\n        - Log: pass/fail with detailed explanation if failed\n     h. Summary: queries tested, pass rate, avg latency per phase, overall NDCG\n   - Exit code: 0 if all pass, 1 if any fail\n\n2. tests/e2e/validate_index_io.rs (binary target)\n   - Tests vector index round-trip integrity\n   - Steps:\n     a. Create vectors of known values\n     b. Write FSVI index\n     c. Log: file size, records written, format details\n     d. Reopen index via mmap\n     e. Read back every vector, compare with originals\n     f. Log: max error per vector (f16 quantization loss)\n     g. Verify header CRC32\n     h. Test corruption detection (flip a byte, verify error)\n   - Exit code: 0 if all checks pass\n\n3. tests/e2e/validate_fusion.rs (binary target)\n   - Tests RRF fusion and score blending correctness\n   - Steps:\n     a. Create synthetic lexical and semantic result sets with known rankings\n     b. Run RRF fusion, verify output order matches expected\n     c. Run score blending with various blend factors\n     d. Log: input rankings, fusion scores, output rankings, expected vs actual\n     e. Test edge cases: empty inputs, single source, all-overlap, zero-overlap\n\n4. tests/e2e/bench_quick.rs (binary target)\n   - Quick performance smoke test (< 30 seconds)\n   - Steps:\n     a. Generate 10K random vectors (384-dim)\n     b. Write to FSVI index\n     c. Run 100 searches, measure latency distribution\n     d. Log: p50, p90, p99 latency, throughput (queries/sec)\n     e. Assert p99 < 50ms for 10K vectors\n     f. Run SIMD dot product microbenchmark\n     g. Log: throughput in GFLOP/s\n\nLogging format for all scripts:\n  [TIMESTAMP] [LEVEL] [STEP] message\n  [2026-01-15T10:30:00Z] [INFO] [INDEX] Built vector index: 100 records, 384 dims, 76.8 KB\n  [2026-01-15T10:30:00Z] [DEBUG] [SEARCH] Query \"rust ownership\": Phase::Initial 12.3ms, 10 results\n  [2026-01-15T10:30:00Z] [PASS] [QUERY] \"rust ownership\" NDCG@10=0.89 (threshold: 0.25)\n\nUse colored output (via colored crate or owo-colors):\n- GREEN: pass, success\n- RED: fail, error\n- YELLOW: warning, degraded\n- CYAN: info, timing\n- DIM: debug details\n\nEach script should be runnable with:\n  cargo run --example validate_full_pipeline\n  cargo run --example validate_index_io\n  cargo run --example validate_fusion\n  cargo run --example bench_quick\n\nAnd also accessible via a master script:\n  ./tests/e2e/run_all.sh (runs all 4, reports summary)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:12:32.915337598Z","created_by":"ubuntu","updated_at":"2026-02-13T22:00:10.912749498Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","logging","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.40","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:12:32.915337598Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.40","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T20:12:37.666997010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.40","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:12:37.750726993Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.40","depends_on_id":"bd-3un.39","type":"blocks","created_at":"2026-02-13T20:12:37.835096814Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":50,"issue_id":"bd-3un.40","author":"Dicklesworthstone","text":"REVISION: E2E Validation Scripts Enhancement\n\nThe current 4 scripts are solid. Add these critical test scenarios:\n\n1. Degradation / Fallback Test (add to validate_full_pipeline.rs):\n   - Test A: Quality model unavailable -> system falls back to fast-only mode\n   - Test B: Fast model unavailable -> system falls back to hash-only mode\n   - Test C: Both ML models unavailable -> system uses hash embedder\n   - Test D: Lexical index corrupted -> semantic-only search works\n   - Test E: Vector index corrupted -> lexical-only search works\n   - Each test: verify graceful degradation (no panic, correct SearchPhase)\n   - Colorized output: YELLOW for expected degradation, RED for unexpected failure\n\n2. Concurrent Access Test (new script: validate_concurrency.rs):\n   - Spawn 4 reader threads + 1 writer thread\n   - Readers: continuous search queries against existing index\n   - Writer: rebuilds index with new documents\n   - Assert: readers never see corrupted results (no partial writes)\n   - Assert: readers may see stale results (eventual consistency OK)\n   - Assert: no panics, no deadlocks (timeout after 10s)\n   - This validates the OnceLock + atomic rename concurrency model\n\n3. Memory Baseline Test (add to bench_quick.rs):\n   - Measure RSS before and after full pipeline run\n   - Track peak RSS during search (via /proc/self/status on Linux)\n   - Budget: RSS delta < 50MB for 10K doc index\n   - Log: \"memory_baseline rss_before={mb} rss_peak={mb} rss_after={mb} delta={mb}\"\n   - YELLOW if delta > 30MB, RED if delta > 50MB\n\n4. Progressive Iterator Contract Test (add to validate_full_pipeline.rs):\n   - Verify iterator yields exactly 2 phases when quality model available\n   - Verify iterator yields exactly 1 phase (Initial only) when quality unavailable\n   - Verify SearchPhase::Refined results are >= quality of SearchPhase::Initial\n   - Verify SearchPhase::RefinementFailed carries the Initial results unchanged\n   - Time each phase: Initial < 20ms, Refined < 300ms (with hash embedders in test)\n\n5. Logging Verification:\n   - Each script captures tracing output to a StringWriter\n   - At end: verify key tracing events were emitted (model_loaded, search_completed, etc.)\n   - Missing events: YELLOW warning (tracing coverage regression)\n\n6. Master Script Enhancement (run_all.sh):\n   - Run all 5 scripts (original 4 + concurrency test)\n   - Summary table at end with pass/fail/warn counts per script\n   - Exit code: 0 if all pass, 1 if any fail, 2 if any warn\n   - --verbose flag for full output, default is summary only\n   - --quick flag to skip concurrency and memory tests (for CI)\n","created_at":"2026-02-13T20:44:54Z"},{"id":168,"issue_id":"bd-3un.40","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — TESTING STRATEGY (applies to bd-3un.31, bd-3un.32, bd-3un.38, bd-3un.40):\n\nAll test beads gain access to asupersync's LabRuntime for deterministic testing. This is a MAJOR improvement over non-deterministic std::thread-based testing.\n\nKEY ADDITIONS:\n\n1. LabRuntime for deterministic tests (bd-3un.31, bd-3un.32):\n   - Virtual time: cx.sleep() advances instantly, no real waits\n   - Deterministic scheduling: same seed = same execution order\n   - Reproducible: failing tests can be replayed with exact same schedule\n   - Oracles: automatic verification of correctness properties\n\n2. Oracles to add to every test (bd-3un.31, bd-3un.32):\n   - QuiescenceOracle: verify no orphan tasks after test\n   - ObligationLeakOracle: verify no leaked channel permits\n   - TaskLeakOracle: verify no stray tasks\n   - DeterminismOracle: verify same seed produces same result\n\n3. DPOR schedule exploration (bd-3un.32 integration tests):\n   - For concurrent tests (e.g., concurrent ingest+search):\n     ScheduleExplorer explores all meaningful interleavings\n   - Catches race conditions that non-deterministic tests might miss\n   - Coverage metrics show how many distinct schedules were explored\n\n4. Cancellation injection testing (bd-3un.40 e2e tests):\n   - CancellationInjector: inject cancellation at specific points\n   - Verify: cancel during Phase 0 → clean exit, no leaked resources\n   - Verify: cancel during Phase 1 → Initial results still valid\n   - Verify: cancel during index rebuild → no corrupt files\n\n5. Test fixture corpus (bd-3un.38):\n   - No changes needed — the corpus is data, not async code\n   - But tests using the corpus should use LabRuntime for determinism\n\nEXAMPLE TEST PATTERN:\n\n  #[test]\n  fn progressive_search_deterministic() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          let searcher = setup_test_searcher(&cx).await;\n          let phases: Vec<_> = searcher.search(&cx, \"rust traits\", 10).collect().await;\n\n          assert_eq!(phases.len(), 2);\n          assert!(matches!(phases[0], SearchPhase::Initial { .. }));\n          assert!(matches!(phases[1], SearchPhase::Refined { .. }));\n      });\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }\n\n  #[test]\n  fn cancel_during_quality_embedding() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          let searcher = setup_test_searcher(&cx).await;\n\n          // Race search against a timeout to simulate cancellation\n          match asupersync::combinator::timeout(\n              |cx| searcher.search(&cx, \"rust traits\", 10).collect(),\n              cx.now() + Duration::from_millis(20),  // Cancel during quality embedding\n          ).await {\n              Outcome::Ok(_) => panic!(\"should have timed out\"),\n              Outcome::Cancelled(_) => { /* expected: quality embedding cancelled */ },\n              _ => panic!(\"unexpected outcome\"),\n          }\n      });\n      // Verify no leaked resources even after cancellation\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }","created_at":"2026-02-13T21:06:48Z"},{"id":293,"issue_id":"bd-3un.40","author":"Dicklesworthstone","text":"REVIEW FIX — Example targets, exit codes, async concurrent test, and download test:\n\n1. TARGET TYPE: Use [[example]] targets in Cargo.toml, not [[bin]] targets. Examples are the correct Cargo convention for validation scripts:\n   [[example]]\n   name = \"validate_full_pipeline\"\n   required-features = [\"full\"]\n\n2. EXIT CODES: Use standard exit codes:\n   - 0 = success (including warnings logged to stderr)\n   - 1 = failure\n   Non-standard exit code 2 for warnings causes CI confusion.\n\n3. CONCURRENT ACCESS TEST: Post-asupersync, use asupersync tasks (not OS threads):\n   asupersync::region(|cx| async {\n       let (readers, writer) = asupersync::join!(\n           cx,\n           spawn_readers(cx, &index, 4),\n           spawn_writer(cx, &index),\n       );\n   });\n\n4. DOWNLOAD FAILURE TEST: Add test for the download feature path:\n   - Mock a failed download (network error mid-stream)\n   - Verify graceful error handling (SearchError::DownloadFailed or fallback to hash embedder)","created_at":"2026-02-13T22:00:10Z"}]}
{"id":"bd-3un.41","title":"Implement index staleness detection and cache management","description":"Implement index staleness detection and cache management for the TwoTierIndex. When source data changes but the index hasn't been rebuilt, the system should detect this and optionally trigger a rebuild.\n\nStaleness detection (from xf VectorIndexCache pattern):\n\npub struct IndexStaleness {\n    pub is_stale: bool,\n    pub index_modified: SystemTime,\n    pub newest_source: Option<SystemTime>,\n    pub index_record_count: usize,\n    pub estimated_source_count: Option<usize>,\n    pub reason: Option<String>,\n}\n\nDetection strategies:\n1. Timestamp comparison: compare index file mtime with source data directory mtime\n2. Count mismatch: compare record_count in index header with count of source documents (if provided by caller)\n3. Sentinel file: write a .index_built_at sentinel with build timestamp + source hash\n4. Manual invalidation: caller can explicitly mark index as stale\n\npub struct IndexCache {\n    fast: OnceLock<Option<VectorIndex>>,\n    quality: OnceLock<Option<VectorIndex>>,\n    lexical: OnceLock<Option<LexicalIndex>>,\n    data_dir: PathBuf,\n}\n\nimpl IndexCache {\n    pub fn new(data_dir: PathBuf) -> Self;\n    \n    /// Get or lazily load the fast-tier index\n    pub fn fast_index(&self) -> Option<&VectorIndex>;\n    \n    /// Get or lazily load the quality-tier index\n    pub fn quality_index(&self) -> Option<&VectorIndex>;\n    \n    /// Get or lazily load the lexical index\n    pub fn lexical_index(&self) -> Option<&LexicalIndex>;\n    \n    /// Check if any index is stale\n    pub fn check_staleness(&self) -> IndexStaleness;\n    \n    /// Invalidate cache (next access will reload from disk)\n    /// Note: OnceLock can't be reset, so this creates a new cache\n    pub fn invalidate(self) -> Self;\n    \n    /// Get the two-tier index (combines fast + quality)\n    pub fn two_tier_index(&self, config: &TwoTierConfig) -> Option<TwoTierIndex>;\n}\n\nSentinel file format (.frankensearch_index_meta):\n{\n    \"built_at\": \"2026-01-15T10:30:00Z\",\n    \"source_count\": 1234,\n    \"source_hash\": \"sha256:abc123...\",   // optional, hash of source file list\n    \"fast_embedder\": \"potion-multilingual-128M\",\n    \"quality_embedder\": \"all-MiniLM-L6-v2\",\n    \"fast_dimension\": 256,\n    \"quality_dimension\": 384,\n    \"format_version\": 1\n}\n\nAuto-rebuild policy (configurable):\n- AutoRebuild::Never: just report staleness\n- AutoRebuild::Prompt: log warning, let caller decide\n- AutoRebuild::Auto: rebuild in background when stale detected\n\nLogging:\n- INFO: \"index_cache_hit\" { fast: true, quality: true, lexical: true }\n- WARN: \"index_stale\" { reason, index_age_secs, source_count_mismatch }\n- INFO: \"index_rebuild_triggered\" { strategy, estimated_docs }\n\nReference:\n- xf: src/main.rs VectorIndexCache with OnceLock (lines 50-150)\n- cass: src/search/vector_index.rs (index staleness checks)\n\nFile: frankensearch-fusion/src/cache.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:12:58.215966879Z","created_by":"ubuntu","updated_at":"2026-02-13T21:57:24.686728554Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cache","phase7","staleness"],"dependencies":[{"issue_id":"bd-3un.41","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:12:58.215966879Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.41","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:57:24.686668391Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.41","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T20:13:02.312195181Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":30,"issue_id":"bd-3un.41","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Index Staleness Detection)\n\n## Mathematical Upgrade: From Timestamp-Based to Information-Theoretic Staleness\n\nCurrent design uses timestamp/count comparison for staleness. This is crude — an index can be \"fresh\" by timestamp but stale by content (if the distribution of new documents differs from indexed documents), or \"stale\" by timestamp but still perfectly representative.\n\n### 1. KL Divergence Staleness Score\n\nMaintain a term frequency distribution for the indexed corpus (computed at build time) and compare against a running term frequency distribution of incoming documents:\n\n  staleness_score = D_KL(P_new || P_indexed)\n  = Σᵢ P_new(termᵢ) × log(P_new(termᵢ) / P_indexed(termᵢ))\n\nWhen KL divergence exceeds a threshold (e.g., 0.5 nats), the index is \"stale\" in a formally meaningful sense — the distribution of content has shifted enough that the index no longer represents it well.\n\nImplementation: Maintain a Count-Min Sketch (O(k) space, O(1) update) for both indexed and new term distributions. Compute KL divergence periodically (every 100 new documents).\n\n### 2. CUSUM Change-Point Detection\n\nUse the CUSUM (Cumulative Sum Control Chart) algorithm to detect when search quality degrades:\n\n  S_n = max(0, S_{n-1} + (x_n - μ₀ - k))\n\nWhere x_n is the search quality metric (e.g., mean reciprocal rank from click data), μ₀ is the expected quality, and k is the allowable slack. When S_n exceeds threshold h, a change point is detected → trigger rebuild.\n\nCUSUM is formally optimal for detecting mean shifts (Lorden 1971) and requires O(1) state.\n\n### 3. Survival Analysis for Index Lifetime\n\nModel index lifetime as a Weibull distribution:\n\n  h(t) = (k/λ)(t/λ)^{k-1}  // hazard rate\n  S(t) = exp(-(t/λ)^k)       // survival function\n\nFit k and λ from historical rebuild intervals. This gives:\n- P(index still good at time t): S(t)\n- Expected time until rebuild needed: λ × Γ(1 + 1/k)\n- Optimal rebuild schedule: minimize expected cost of staleness + rebuild\n\n### 4. Practical Implementation\n\nCombine all three into a single staleness score:\n\n  pub struct StalenessDetector {\n      // Lightweight (< 1KB total state)\n      term_sketch_indexed: CountMinSketch,  // Frozen at build time\n      term_sketch_new: CountMinSketch,      // Updated with each new document\n      cusum_state: f32,                     // Running CUSUM statistic\n      docs_since_build: u64,\n      build_timestamp: SystemTime,\n  }\n\n  impl StalenessDetector {\n      pub fn staleness_score(&self) -> StalenessReport {\n          StalenessReport {\n              kl_divergence: self.compute_kl(),\n              cusum_alarm: self.cusum_state > CUSUM_THRESHOLD,\n              docs_since_build: self.docs_since_build,\n              estimated_quality_loss: self.estimated_quality_loss(),\n              rebuild_recommended: self.kl_divergence > 0.5 || self.cusum_alarm,\n          }\n      }\n  }\n\nThis is the alien-artifact version: formally principled staleness detection with provable properties, but operationally simple (O(1) per document, O(k) space).\n","created_at":"2026-02-13T20:29:52Z"},{"id":158,"issue_id":"bd-3un.41","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — LIGHT REVISION (OnceCell + background rebuild):\n\nSame OnceCell migration as bd-3un.23. Additionally, the \"AutoRebuild::Auto\" background rebuild mode should use an asupersync region:\n\nBEFORE:\n  - OnceLock for index cache\n  - \"background rebuild\" mechanism unspecified\n\nAFTER:\n  - asupersync::sync::OnceCell for cancel-aware lazy init\n  - Background rebuild via asupersync region:\n\n  pub async fn check_and_rebuild(&self, cx: &Cx) -> asupersync::Result<()> {\n      let staleness = self.detect_staleness()?;\n      if staleness.is_stale && self.config.auto_rebuild == AutoRebuild::Auto {\n          cx.region(|scope| async {\n              scope.spawn(|cx| async {\n                  // Rebuild index in background region\n                  // If parent is cancelled, this rebuild is cancelled too\n                  self.rebuild_index(cx).await\n              });\n          }).await?;\n      }\n      Ok(())\n  }","created_at":"2026-02-13T21:06:31Z"},{"id":212,"issue_id":"bd-3un.41","author":"Dicklesworthstone","text":"REVISION (review pass 4 - trait extraction and storage integration):\n\n1. STALENESS DETECTOR TRAIT: This bead should define a StalenessDetector trait that can be implemented by both:\n   a) File-based staleness (this bead, default when storage feature is off)\n   b) Storage-backed staleness (bd-3w1.12, when storage feature is on)\n\n   pub trait StalenessDetector: Send + Sync {\n       fn check(&self, index_name: &str) -> SearchResult<StalenessReport>;\n       fn quick_check(&self) -> SearchResult<bool>;\n   }\n\n   The IndexCache uses Box<dyn StalenessDetector> so it works with either implementation. When the 'storage' feature is enabled, the storage-backed version is used (more accurate, queries the document database). When disabled, the file-based version is used (timestamp comparison, sentinel files).\n\n2. INDEX CACHE vs TWOTIERINDEX: There's potential confusion between:\n   - TwoTierIndex (bd-3un.23): manages VECTOR indices only (fast + quality)\n   - IndexCache (this bead): manages ALL indices (vector + lexical) with lazy loading and staleness\n\n   Clarification: IndexCache WRAPS TwoTierIndex and adds:\n   a) Lexical index management (Tantivy or FTS5)\n   b) Lazy loading via OnceLock\n   c) Staleness detection and cache invalidation\n\n   The TwoTierSearcher (bd-3un.24) should use IndexCache, not TwoTierIndex directly:\n   pub struct TwoTierSearcher {\n       cache: IndexCache,  // wraps TwoTierIndex + lexical index\n       // ... other fields\n   }\n\n3. LEXICAL INDEX IN CACHE: The IndexCache holds OnceLock<Option<Box<dyn LexicalIndex>>>. This requires the LexicalIndex trait to be defined in frankensearch-core (the trait extraction from bd-3un.18 / bd-3un.1 revision). The cache doesn't depend on Tantivy OR FTS5 directly - it depends on the trait.\n\n4. ASUPERSYNC NOTE: OnceLock for lazy initialization is fine (it's a sync primitive for one-time init). The IndexCache methods that trigger I/O (loading indices from disk) should take &Cx for cancellation. Consider using asupersync::sync::OnceCell instead of std::sync::OnceLock for cancel-aware initialization.\n","created_at":"2026-02-13T21:13:08Z"},{"id":270,"issue_id":"bd-3un.41","author":"Dicklesworthstone","text":"REVIEW FIX — IndexCache vs TwoTierIndex dependency, ArcSwap for atomic replacement, and staleness trait:\n\n1. IndexCache vs TwoTierIndex DEPENDENCY INCONSISTENCY: The revision says \"TwoTierSearcher should use IndexCache, not TwoTierIndex directly.\" But bd-3un.24 depends on bd-3un.23 (TwoTierIndex), not on bd-3un.41 (IndexCache). \n\n   RESOLUTION: IndexCache WRAPS TwoTierIndex. The dependency chain is:\n   TwoTierSearcher → IndexCache → TwoTierIndex\n   bd-3un.24 should add a dependency on bd-3un.41 (this bead).\n   IndexCache provides staleness checking and atomic replacement around TwoTierIndex.\n\n2. ArcSwap FOR ATOMIC REPLACEMENT: The body's invalidate(self) -> Self pattern is architecturally fragile (requires replacing all references). Use arc-swap instead:\n\n   pub struct IndexCache {\n       inner: arc_swap::ArcSwap<TwoTierIndex>,\n       staleness: Box<dyn StalenessDetector>,\n   }\n\n   impl IndexCache {\n       pub fn current(&self) -> arc_swap::Guard<Arc<TwoTierIndex>> {\n           self.inner.load()\n       }\n       \n       pub async fn refresh(&self, cx: &Cx, new_index: TwoTierIndex) {\n           self.inner.store(Arc::new(new_index));\n           // Old index dropped when last reader finishes\n       }\n   }\n\n   This allows lock-free reads and atomic replacement without invalidating existing references.\n\n3. StalenessDetector TRAIT:\n   pub trait StalenessDetector: Send + Sync {\n       /// Check if the index at the given path is stale.\n       /// path = directory containing the FSVI files and sentinel.\n       fn is_stale(&self, index_path: &Path) -> Result<bool, SearchError>;\n   }\n\n   Default implementation: SentinelFileDetector checks .frankensearch_sentinel.json for:\n   - last_modified timestamp\n   - document_count\n   - embedder_revision hash\n\n4. DEPENDENCY: Add bd-3un.2 (SearchError) for error types. Also add arc-swap to workspace deps.\n\n5. TEST REQUIREMENTS:\n   - Fresh index: is_stale returns false\n   - Stale sentinel: modify sentinel timestamp, is_stale returns true\n   - Missing sentinel (first run): is_stale returns true (trigger rebuild)\n   - Atomic replacement: readers using old index unaffected during refresh\n   - Concurrent reads during refresh: no blocking, no corruption\n   - Sentinel round-trip: write sentinel, read sentinel, values match","created_at":"2026-02-13T21:57:21Z"}]}
{"id":"bd-3un.42","title":"Implement text canonicalization pipeline","description":"Implement a text canonicalization/preprocessing pipeline in frankensearch-core that normalizes text before embedding. This is CRITICAL for search quality -- without proper preprocessing, embeddings are noisy and search results degrade.\n\nAll three source codebases have this:\n- cass: canonicalize.rs (1,039 lines) with streaming implementation\n- agent-mail: CanonPolicy (Full, TitleOnly) with embed_document() helper\n- xf: simpler preprocessing (queries are naturally short)\n\nDesign: Trait-based with a default implementation, customizable per consumer.\n\npub trait Canonicalizer: Send + Sync {\n    fn canonicalize(&self, text: &str) -> String;\n    fn canonicalize_query(&self, query: &str) -> String {\n        // Query canonicalization is simpler (no markdown stripping, no code collapsing)\n        self.canonicalize(query)\n    }\n}\n\nDefault implementation pipeline (from cass canonicalize.rs):\npub struct DefaultCanonicalizer {\n    pub max_chars: usize,          // Default: 2000 (MAX_EMBED_CHARS from cass)\n    pub strip_markdown: bool,      // Default: true\n    pub collapse_code_blocks: bool,// Default: true\n    pub code_head_lines: usize,    // Default: 20\n    pub code_tail_lines: usize,    // Default: 10\n    pub filter_low_signal: bool,   // Default: true\n    pub normalize_unicode: bool,   // Default: true (NFC)\n}\n\nProcessing steps (in order):\n1. Unicode NFC normalization (via unicode-normalization crate)\n   - Ensures hash stability: different Unicode representations → same bytes\n   - \"café\" (e + combining acute) → \"café\" (single precomposed char)\n\n2. Markdown stripping (pure text extraction)\n   - Remove headers (#, ##), bold/italic (**,*,_), links [text](url)→text\n   - Keep text content, remove formatting syntax\n   - Configurable (some consumers may want to preserve structure)\n\n3. Code block collapsing\n   - For code blocks > (head + tail) lines:\n     Keep first `code_head_lines` (20) + last `code_tail_lines` (10)\n     Replace middle with \"... [N lines elided] ...\"\n   - Prevents long code from dominating embedding signal\n\n4. Whitespace normalization\n   - Collapse multiple spaces/tabs/newlines to single space\n   - Trim leading/trailing whitespace\n\n5. Low-signal content filtering\n   - Filter out very short, meaningless responses\n   - Configurable list: [\"ok\", \"done\", \"got it\", \"understood\", \"sure\", \"yes\", \"no\", \"thanks\", \"thank you\"]\n   - Only filter if entire text matches (not substrings)\n   - Returns empty string for filtered content (caller decides to skip)\n\n6. Truncation to max_chars\n   - Truncate at word boundary if possible\n   - Never split mid-word\n\nAlso provide:\npub fn content_hash(text: &str) -> String {\n    // SHA-256 hex digest of the canonicalized text\n    // Used for dedup and change detection throughout the pipeline\n    use sha2::{Sha256, Digest};\n    hex::encode(Sha256::digest(text.as_bytes()))\n}\n\nQuery-specific canonicalization (simpler):\n1. Unicode NFC\n2. Whitespace normalization\n3. Truncation (shorter limit, e.g., 500 chars)\n(No markdown stripping, no code collapsing, no low-signal filtering)\n\nFile: frankensearch-core/src/canonicalize.rs\n\nDependencies:\n- unicode-normalization (for NFC) - lightweight, no transitive deps\n- sha2 + hex (for content hashing) - already used for model verification\n\nReference: cass src/search/canonicalize.rs (1,039 lines with streaming impl)\nReference: agent-mail embed_document() helper in embedder.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:23:11.362478088Z","created_by":"ubuntu","updated_at":"2026-02-13T22:00:26.462024507Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","phase1","preprocessing"],"dependencies":[{"issue_id":"bd-3un.42","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:23:11.362478088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.42","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:23:15.507609794Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":109,"issue_id":"bd-3un.42","author":"Dicklesworthstone","text":"REVISION: Text Canonicalization Implementation Details\n\n1. Unicode Edge Cases:\n   - Combining characters (e.g., e + combining acute): NFC normalization handles this\n   - Zero-width spaces (U+200B, U+FEFF): strip before processing\n   - Surrogate pairs in UTF-16 encoded content: Rust strings are always valid UTF-8, no issue\n   - Emoji sequences (skin tone modifiers, ZWJ sequences): preserve as-is (may be content)\n   - Bidirectional text (RTL markers): strip control characters, preserve text\n\n2. Code Block Detection Heuristics:\n   - Fenced blocks: triple backtick (```) or triple tilde (~~~), with optional language tag\n   - Indented blocks: 4+ spaces or 1+ tab at start of line, after a blank line\n   - Inline code: single backtick pairs (leave as-is, don't collapse)\n   - For fenced blocks longer than 30 lines: keep 20 head + 10 tail + \"[{N} lines collapsed]\"\n   - For indented blocks: same collapsing rule\n   - Preserve the language tag (e.g., \"rust\", \"python\") as it's searchable metadata\n\n3. Low-Signal Word List:\n   - Short acknowledgments: \"ok\", \"done\", \"thanks\", \"ty\", \"thx\", \"lgtm\", \"approved\"\n   - Auto-generated: \"sent from my iphone\", \"confidential notice\"\n   - Bot signatures: \"[bot]\", \"auto-reply\"\n   - The filter removes documents that consist ENTIRELY of low-signal words\n   - Documents with low-signal words mixed with real content: keep all content\n\n4. Markdown Stripping:\n   - Remove: headers (#), bold/italic (*/_ wrappers), links (keep text, drop URL)\n   - Remove: HTML tags (< >), horizontal rules (---), blockquote markers (>)\n   - Preserve: list items (strip bullet/number prefix, keep text)\n   - Preserve: table content (strip pipes, keep cell text)\n\n5. Query Canonicalization (simpler pipeline):\n   Steps 1 (NFC), 4 (whitespace), 6 (truncation) only\n   Do NOT strip markdown from queries (user may search for \"# Header\")\n   Do NOT collapse code blocks in queries (user may search for code)\n   Truncation limit for queries: 500 chars (not 2000)\n","created_at":"2026-02-13T20:57:45Z"},{"id":294,"issue_id":"bd-3un.42","author":"Dicklesworthstone","text":"REVIEW FIX — Markdown stripping, HTML entities, URL handling, and tests:\n\n1. MARKDOWN STRIPPING: Rule-based regex stripping is fragile (e.g., _underscored_variable_ in code). Consider using pulldown-cmark for robust markdown-to-text conversion:\n   - pulldown-cmark is a well-maintained, fast, pure-Rust markdown parser\n   - Handles edge cases (nested emphasis, code spans, tables) correctly\n   - For V1: regex-based rules are acceptable if well-tested\n   - For V2: switch to pulldown-cmark\n\n2. HTML ENTITY DECODING: Add a step to decode HTML entities (&amp; → &, &lt; → <, etc.) before embedding. Email content often contains HTML entities.\n\n3. URL REMOVAL: Add a step to remove or replace URLs with a placeholder token [URL]. URLs are noise for embedding models and waste token budget.\n\n4. TEST REQUIREMENTS:\n   - NFC normalization: \"café\" (combining accent) → \"café\" (precomposed)\n   - Code block collapsing: 31-line code block → 20 head + 10 tail + \"[1 line elided]\"\n   - Low-signal filter: \"thanks\" (entire text) → filtered (empty output)\n   - Low-signal non-match: \"thanks for the help with my code\" → NOT filtered\n   - Truncation at word boundary: 10000-char text truncated to max_length at word boundary\n   - Empty output: whitespace-only input → empty string\n   - Markdown stripping: \"**bold** and _italic_\" → \"bold and italic\"\n   - HTML entities: \"&amp; &lt; &gt;\" → \"& < >\"\n   - URL removal: \"check https://example.com for info\" → \"check [URL] for info\"\n   - Unicode whitespace: \\u{3000} (ideographic space) normalized to regular space\n   - Content hash determinism: same text → same SHA-256 hash always","created_at":"2026-02-13T22:00:26Z"}]}
{"id":"bd-3un.43","title":"Implement query classification and candidate budgeting","description":"Implement query classification and adaptive candidate budgeting for hybrid search. Different types of queries benefit from different retrieval strategies -- technical identifiers should lean lexical, natural language queries should lean semantic.\n\nFrom agent-mail hybrid_candidates.rs (1,234 lines):\n\nQuery Classification:\n\npub enum QueryClass {\n    /// Empty or whitespace-only query\n    Empty,\n    /// Technical identifier: \"br-123\", \"thread:abc\", mixed alpha+digits\n    Identifier,\n    /// Short keyword phrase: 1-2 short tokens\n    ShortKeyword,\n    /// Natural language: 3+ tokens or long average token length\n    NaturalLanguage,\n}\n\nimpl QueryClass {\n    pub fn classify(query: &str) -> Self {\n        let tokens: Vec<&str> = query.split_whitespace().collect();\n        if tokens.is_empty() { return Self::Empty; }\n        \n        // Identifier heuristics (from agent-mail):\n        // - Starts with known prefix (\"br-\", \"thread:\", \"bd-\")\n        // - Contains underscores or slashes\n        // - Mixed alpha+digit tokens\n        // - All-hyphenated tokens\n        let looks_like_id = tokens.iter().any(|t| {\n            t.contains('_') || t.contains('/') ||\n            (t.contains('-') && t.chars().any(|c| c.is_ascii_digit())) ||\n            (t.chars().any(|c| c.is_ascii_alphabetic()) && t.chars().any(|c| c.is_ascii_digit()))\n        });\n        if looks_like_id { return Self::Identifier; }\n        \n        let avg_len = tokens.iter().map(|t| t.len()).sum::<usize>() / tokens.len();\n        if tokens.len() <= 2 && avg_len <= 10 { return Self::ShortKeyword; }\n        \n        Self::NaturalLanguage\n    }\n}\n\nCandidate Budget System:\n\npub struct CandidateConfig {\n    /// Base multiplier for lexical candidates (default: 3x)\n    pub lexical_multiplier: f32,\n    /// Base multiplier for semantic candidates (default: 3x)\n    pub semantic_multiplier: f32,\n    /// Minimum candidates per source (default: 20)\n    pub min_per_source: usize,\n    /// Maximum candidates per source (default: 1000)\n    pub max_per_source: usize,\n    /// Maximum total candidates (default: 2000)\n    pub max_combined: usize,\n}\n\nQuery-class adjustments (from agent-mail):\n- Identifier: lex_mult *= 1.5, sem_mult *= 0.5 (lean lexical for exact matches)\n- ShortKeyword: lex_mult *= 1.25, sem_mult *= 0.75 (slight lexical preference)\n- NaturalLanguage: lex_mult *= 0.9, sem_mult *= 1.35 (lean semantic for meaning)\n- Empty: lex_mult *= 1.0, sem_mult = 0.0 (lexical only, no semantic)\n\npub struct CandidateBudget {\n    pub lexical_count: usize,\n    pub semantic_count: usize,\n    pub combined_limit: usize,\n    pub query_class: QueryClass,\n}\n\nimpl CandidateBudget {\n    pub fn derive(\n        requested_limit: usize,\n        config: &CandidateConfig,\n        query: &str,\n    ) -> Self {\n        let class = QueryClass::classify(query);\n        let (lex_adj, sem_adj) = class.multiplier_adjustments();\n        let lex = ((requested_limit as f32) * config.lexical_multiplier * lex_adj)\n            .ceil() as usize;\n        let sem = ((requested_limit as f32) * config.semantic_multiplier * sem_adj)\n            .ceil() as usize;\n        let lex = lex.clamp(config.min_per_source, config.max_per_source);\n        let sem = sem.clamp(config.min_per_source, config.max_per_source);\n        CandidateBudget {\n            lexical_count: lex,\n            semantic_count: sem,\n            combined_limit: (lex + sem).min(config.max_combined),\n            query_class: class,\n        }\n    }\n}\n\nThis replaces the simple CANDIDATE_MULTIPLIER=3 constant in bd-3un.20 (RRF) with a query-aware system that adapts to what the user is searching for.\n\nFile: frankensearch-fusion/src/candidates.rs\n\nReference:\n- agent-mail: crates/mcp-agent-mail-search-core/src/hybrid_candidates.rs (1,234 lines)\n- cass: query.rs HYBRID_CANDIDATE_MULTIPLIER=3 (simple version)\n- xf: hybrid.rs CANDIDATE_MULTIPLIER=3 (simple version)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:23:42.760610565Z","created_by":"ubuntu","updated_at":"2026-02-13T21:58:44.693774970Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fusion","phase6","query"],"dependencies":[{"issue_id":"bd-3un.43","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:23:42.760610565Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.43","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:50:22.110816139Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":110,"issue_id":"bd-3un.43","author":"Dicklesworthstone","text":"REVISION: Query Classification Implementation Details\n\n1. Classification Heuristics:\n   - Empty: len == 0 after canonicalization\n   - Identifier: single token, contains [A-Z], [-_], or [0-9], or matches pattern like\n     \"br-123\", \"POL-358\", \"SearchError\", \"bd-3un.24\", \"SHA-256\", file paths\n   - ShortKeyword: 1-3 tokens, all lowercase, no special chars (e.g., \"rust async\")\n   - NaturalLanguage: 4+ tokens, or contains question words (who/what/where/when/why/how)\n\n2. Multiplier Tuning:\n   The candidate budget multipliers are initial values based on source codebase analysis:\n   - Identifier: 1.5x lexical, 0.5x semantic (exact match matters more)\n   - ShortKeyword: 1.2x lexical, 1.0x semantic (balanced)\n   - NaturalLanguage: 0.9x lexical, 1.35x semantic (meaning matters more)\n   - Empty: 0x both (return empty immediately)\n\n   These multipliers should be CONFIGURABLE via TwoTierConfig, not hardcoded.\n   Default values are reasonable for code search and documentation search.\n   Domain-specific tuning may be needed (e.g., e-commerce search would differ).\n\n3. Performance Impact:\n   Classification is pure string analysis: < 1us per query (negligible vs embedding).\n   No regex compilation needed (simple char checks and token counting).\n   Classification result is logged at DEBUG level.\n\n4. Multilingual Considerations:\n   - CJK text: word boundaries are different (no whitespace between words)\n   - Arabic/Hebrew: RTL text, different character classes\n   - For V1: treat non-ASCII text as NaturalLanguage by default\n   - For V2: consider language detection (e.g., whatlang crate) for better classification\n   - potion-128M is multilingual, so semantic search handles this regardless\n\n5. Integration with TwoTierSearcher:\n   Classification happens FIRST, before any embedding or search.\n   The QueryClass is stored in TwoTierMetrics for monitoring.\n   CandidateBudget feeds into the RRF candidate_multiplier parameter.\n   If lexical feature is disabled, lexical multiplier is ignored.\n","created_at":"2026-02-13T20:57:46Z"},{"id":250,"issue_id":"bd-3un.43","author":"Dicklesworthstone","text":"DEPENDENCY FIX: Replaced bd-3un.5 with bd-3un.2\n\nQueryClass and CandidateBudget are self-contained types defined in this bead.\nThey do NOT use ScoredResult, VectorHit, FusedHit, or any other types from\nbd-3un.5 (core result types):\n\n  - QueryClass: enum with 4 string-analysis variants\n  - CandidateBudget: struct with usize counts and QueryClass\n\nThe only external dependency is SearchError (for validation errors),\nwhich comes from bd-3un.2 (error types).\n\nIMPACT: This shortens the critical path to bd-3un.24 (TwoTierSearcher):\n\nBEFORE: bd-3un.1 -> bd-3un.2 -> bd-3un.5 -> bd-3un.43 -> bd-3un.24\nAFTER:  bd-3un.1 -> bd-3un.2 -> bd-3un.43 -> bd-3un.24\n\nQuery classification can now start as soon as error types are defined,\nwithout waiting for all result types to be finalized. This is correct\nbecause classification is pure string analysis, independent of search\nresult structures.\n","created_at":"2026-02-13T21:50:53Z"},{"id":276,"issue_id":"bd-3un.43","author":"Dicklesworthstone","text":"REVIEW FIX — Identifier heuristic, ShortKeyword count, Empty behavior, and tests:\n\n1. IDENTIFIER HEURISTIC TOO BROAD: \"contains underscores or slashes\" misclassifies natural language like \"machine_learning\" or \"input/output\" as Identifier. \n\n   RESOLUTION: Tighten the heuristic. Classify as Identifier only if:\n   - Contains both special characters AND digits (e.g., \"br-123\", \"user_42\", \"src/main.rs\")\n   - OR matches known code patterns: camelCase, PascalCase, snake_case with >1 segment\n   - OR starts with a path-like prefix (., /, ~)\n   - AND does NOT contain spaces (identifiers don't have spaces)\n   \n   \"machine_learning\" → NaturalLanguage (has underscore but no digits, single semantic unit)\n   \"br-123\" → Identifier (has dash AND digits)\n   \"src/lib.rs\" → Identifier (path-like)\n\n2. ShortKeyword TOKEN COUNT: Body says 1-2 tokens, revision says 1-3. \n   RESOLUTION: 1-3 tokens (the revision is more practical — \"rust async await\" is a short keyword query, not NaturalLanguage).\n\n3. EMPTY QUERY BEHAVIOR: Body says \"lexical only\", revision says \"return empty immediately.\"\n   RESOLUTION: Return empty immediately. An empty query has no semantic or lexical content to search. Returning empty is faster and more correct than running BM25 on \"\".\n\n4. CONFIGURABLE MULTIPLIERS: The multipliers (sem_mult, lex_mult per QueryClass) should be fields in TwoTierConfig, not hardcoded. Add to bd-3un.22:\n   pub query_class_budgets: HashMap<QueryClass, CandidateBudget>\n   with sensible defaults that can be overridden.\n\n5. TEST REQUIREMENTS:\n   - Identifier classification: \"br-123\" → Identifier, \"src/main.rs\" → Identifier\n   - NOT Identifier: \"machine_learning\" → NaturalLanguage (no digits, single unit)\n   - ShortKeyword: \"rust\" → ShortKeyword, \"rust async\" → ShortKeyword, \"rust async await\" → ShortKeyword\n   - NaturalLanguage: \"how do I implement async in rust\" → NaturalLanguage\n   - Empty: \"\" → Empty, \"   \" (whitespace only) → Empty\n   - Unicode: \"机器学习\" → NaturalLanguage (non-ASCII default)\n   - CandidateBudget multipliers: Identifier gets higher lex_mult, NaturalLanguage gets balanced\n   - Boundary: 4-token query → NaturalLanguage (not ShortKeyword)","created_at":"2026-02-13T21:58:44Z"}]}
{"id":"bd-3un.5","title":"Define core result types (ScoredResult, VectorHit, FusedHit)","description":"Define the core result types that flow through the entire search pipeline. These need to be generic enough to work with any document type (tweets, agent sessions, mail messages) while carrying enough scoring metadata for fusion and display.\n\n/// A scored search result from any search phase.\npub struct ScoredResult {\n    /// Opaque document identifier (caller-defined).\n    pub doc_id: String,\n    /// Primary relevance score (normalized 0.0-1.0 after fusion).\n    pub score: f32,\n    /// Optional reranker score (set by rerank step).\n    pub rerank_score: Option<f32>,\n    /// Which sources contributed to this result.\n    pub sources: SourceContribution,\n    /// Arbitrary metadata (caller can attach doc-specific data).\n    pub metadata: Option<serde_json::Value>,\n}\n\n/// A raw hit from vector similarity search.\npub struct VectorHit {\n    /// Index into the vector store.\n    pub index: usize,\n    /// Cosine similarity score (raw, not normalized).\n    pub score: f32,\n    /// Document identifier.\n    pub doc_id: String,\n}\n\n/// A hit from hybrid fusion (lexical + semantic combined).\npub struct FusedHit {\n    /// Document identifier.\n    pub doc_id: String,\n    /// RRF-fused score.\n    pub rrf_score: f64,\n    /// Individual source scores for debugging/display.\n    pub lexical_rank: Option<usize>,\n    pub semantic_rank: Option<usize>,\n    pub lexical_score: Option<f32>,\n    pub semantic_score: Option<f32>,\n}\n\n/// Tracks which retrieval sources contributed to a result.\npub struct SourceContribution {\n    pub lexical: bool,\n    pub semantic_fast: bool,\n    pub semantic_quality: bool,\n    pub reranked: bool,\n}\n\n/// Search mode selector.\npub enum SearchMode {\n    Lexical,    // BM25 keyword matching only\n    Semantic,   // Embedding similarity only\n    Hybrid,     // RRF fusion of lexical + semantic\n    TwoTier,    // Progressive: fast semantic → quality refinement + lexical fusion\n}\n\n/// Progressive search phases for two-tier display.\npub enum SearchPhase {\n    /// Initial results from fast tier (displayed immediately).\n    Initial {\n        results: Vec<ScoredResult>,\n        latency_ms: u64,\n    },\n    /// Refined results after quality tier completes.\n    Refined {\n        results: Vec<ScoredResult>,\n        latency_ms: u64,\n    },\n    /// Quality refinement failed; initial results remain valid.\n    RefinementFailed {\n        error: SearchError,\n    },\n}\n\nAll types should derive: Debug, Clone, Serialize, Deserialize (where appropriate).\n\nReference implementations:\n- cass: src/search/two_tier_search.rs (SearchPhase, ScoredResult)\n- xf: src/hybrid.rs (FusedHit), src/model.rs (SearchResult)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs (SearchPhase)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:57.813894307Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:59.634677982Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1","types"],"dependencies":[{"issue_id":"bd-3un.5","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:57.813894307Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.5","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:00.758278275Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":54,"issue_id":"bd-3un.5","author":"Dicklesworthstone","text":"REVISION: SearchPhase Data Semantics\n\nSearchPhase enum needs explicit documentation of what data each variant carries:\n\n1. SearchPhase::Initial(Vec<ScoredResult>):\n   - Contains fast-tier results after RRF fusion of lexical + fast semantic\n   - Scores are RRF scores (not raw similarity), range ~0.01-0.03\n   - Results are sorted by RRF score descending, with deterministic tie-breaking\n   - Available within ~15ms of query submission\n\n2. SearchPhase::Refined(Vec<ScoredResult>):\n   - Contains quality-blended results after Phase 1 processing\n   - Scores are blended RRF scores (not raw similarity)\n   - Results may have different ordering than Initial (quality reranking)\n   - ScoredResult.rerank_score is Some(_) if reranker was applied\n   - Available within ~200ms of query submission\n\n3. SearchPhase::RefinementFailed { initial: Vec<ScoredResult>, error: SearchError }:\n   - Carries the ORIGINAL Initial results unchanged (consumer can use them as-is)\n   - error field explains why refinement failed (timeout, model error, etc.)\n   - Consumer should display Initial results and log/display the error\n   - This is NOT an error state -- it's graceful degradation\n\nThe iterator contract:\n- Always yields Initial first\n- Then yields either Refined or RefinementFailed (never both)\n- Iterator is fused after yielding 2 phases (next() returns None)\n- Consumer can stop after Initial if latency-sensitive (skip Phase 1)\n\nDocument these semantics in the SearchPhase doc comments with examples showing\nhow a TUI consumer would handle each variant.\n","created_at":"2026-02-13T20:45:32Z"},{"id":225,"issue_id":"bd-3un.5","author":"Dicklesworthstone","text":"REVIEW FIX — SearchPhase reconciliation and type consistency:\n\n1. SEARCHPHASE CANONICAL DEFINITION (reconciling body and revision):\n\n   pub enum SearchPhase {\n       /// Fast-tier results ready. Yielded first (~15ms).\n       Initial {\n           results: Vec<ScoredResult>,\n           latency: Duration,          // Changed from u64 ms to Duration for consistency\n           metrics: PhaseMetrics,      // Embedder used, vector count, lexical count\n       },\n       /// Quality-refined results ready. Yielded second (~150ms).\n       Refined {\n           results: Vec<ScoredResult>,\n           latency: Duration,\n           metrics: PhaseMetrics,\n           rank_changes: RankChanges,  // promoted/demoted/stable counts vs Initial\n       },\n       /// Quality refinement failed. Initial results are still valid.\n       RefinementFailed {\n           initial_results: Vec<ScoredResult>,  // CRITICAL: carry forward Initial results\n           error: SearchError,\n           latency: Duration,                    // How long we waited before failing\n       },\n   }\n\n   RATIONALE:\n   - `initial_results` in RefinementFailed is ESSENTIAL for users — when quality embedding times out, the consumer still has the fast results to display. Without this, RefinementFailed is useless.\n   - `latency` as Duration (not u64) for type safety and consistency with std.\n   - PhaseMetrics captures diagnostic info per phase (which embedder, how many vectors searched, etc.)\n\n2. SCORE TYPE CONSISTENCY:\n   - FusedHit.rrf_score: f64 (KEEP — RRF accumulates many small 1/(K+rank+1) values; f64 prevents precision loss)\n   - ScoredResult.score: f32 (KEEP — final user-facing score, f32 is sufficient)\n   - DOCUMENT the truncation: \"rrf_score is computed in f64 for precision during fusion, then truncated to f32 when producing the final ScoredResult\"\n\n3. METADATA TYPE:\n   - KEEP `serde_json::Value` for metadata. serde_json is already a workspace dep (used throughout for config, model manifests, etc.). The alternative (Box<dyn Any>) loses serializability which is needed for tracing/logging.\n   - Note: if a zero-dep core is desired, make metadata generic: `ScoredResult<M = serde_json::Value>` with a type alias `pub type DefaultScoredResult = ScoredResult<serde_json::Value>;`\n\n4. ADD IndexableDocument (missing type):\n   pub struct IndexableDocument {\n       pub id: String,              // Unique document identifier\n       pub text: String,            // Full text content for embedding and lexical indexing\n       pub title: Option<String>,   // Optional title (gets BM25 boost in Tantivy)\n       pub metadata: Option<serde_json::Value>,  // Arbitrary metadata passed through to results\n   }\n\n   This is needed by bd-3un.17 (Tantivy indexing), bd-3un.13 (vector index), and the TwoTierIndex.\n\n5. TEST REQUIREMENTS for this bead:\n   - SearchPhase::Initial can be constructed and destructured\n   - SearchPhase::RefinementFailed carries initial_results correctly\n   - FusedHit ordering: higher rrf_score sorts first\n   - FusedHit tie-breaking: in_both_sources wins ties\n   - ScoredResult: metadata round-trips through serde\n   - IndexableDocument: id is required, title and metadata are optional\n   - VectorHit: NaN scores are handled (total_cmp ordering)\n   - RankChanges: promoted + demoted + stable = total","created_at":"2026-02-13T21:46:29Z"},{"id":290,"issue_id":"bd-3un.5","author":"Dicklesworthstone","text":"REVIEW FIX (cross-cutting) — ScoredResult field inventory reconciliation:\n\nCROSS-CUTTING ISSUE: Multiple beads assume fields on ScoredResult that may not exist:\n- bd-3un.26 assumes .text (DOES NOT EXIST — text must be looked up separately)\n- bd-3un.24 assumes VectorHit has .index for positional lookup (DOES EXIST)\n\nCANONICAL ScoredResult DEFINITION (reconciled across all beads):\npub struct ScoredResult {\n    pub doc_id: String,              // Unique document identifier\n    pub score: f32,                  // Primary score (RRF or blended)\n    pub source: ScoreSource,         // Which search backend produced this\n    pub fast_score: Option<f32>,     // Score from fast-tier search\n    pub quality_score: Option<f32>,  // Score from quality-tier search\n    pub lexical_score: Option<f32>,  // BM25 score (if lexical was used)\n    pub rerank_score: Option<f32>,   // Cross-encoder score (if reranked)\n    pub metadata: Option<serde_json::Value>, // Document metadata (from Tantivy stored fields)\n}\n\nNOTE: ScoredResult intentionally does NOT carry document text. Text is expensive to store and most consumers only need doc_id + scores. When text is needed (e.g., for reranking), it must be looked up from the document store via doc_id.","created_at":"2026-02-13T21:59:59Z"}]}
{"id":"bd-3un.6","title":"Implement FNV-1a hash embedder (always-available fallback)","description":"Implement the FNV-1a hash-based embedder in frankensearch-embed. This is the zero-dependency, always-available fallback that produces deterministic (but non-semantic) embeddings. It's critical as the baseline that works even when no ML models are downloaded.\n\nAlgorithm (from cass src/search/hash_embedder.rs):\n1. Tokenize: lowercase input, split on non-alphanumeric chars, filter tokens < 2 chars\n2. Hash each token with FNV-1a (u64):\n   - offset_basis = 0xcbf29ce484222325\n   - prime = 0x100000001b3\n   - For each byte: hash ^= byte; hash = hash.wrapping_mul(prime)\n3. Project into embedding space:\n   - index = hash % dimension (default 384)\n   - sign = if (hash >> 63) == 1 { 1.0 } else { -1.0 }\n   - embedding[index] += sign\n4. L2 normalize to unit length\n\nKey properties:\n- Dimension: 384 (matching MiniLM for index compatibility, configurable)\n- No model files required (pure algorithmic)\n- ~0.07ms per embedding (fastest possible)\n- ID: 'fnv1a-{dimension}' (e.g., 'fnv1a-384')\n- ModelCategory::HashEmbedder\n- is_semantic() returns false\n- Deterministic: same input always produces same output\n\nImplementation notes:\n- No external dependencies needed\n- Include unit tests with known input→output pairs for regression\n- The hash embedder serves as the 'test double' for all pipeline testing\n- In cass, this is always the fallback when ML models aren't available\n\nThis goes in frankensearch-embed/src/hash_embedder.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:48:40.235364554Z","created_by":"ubuntu","updated_at":"2026-02-13T21:46:30.174419819Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","hash","phase2"],"dependencies":[{"issue_id":"bd-3un.6","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:40.235364554Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.6","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:07.536936514Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":25,"issue_id":"bd-3un.6","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. CONFIGURABLE DIMENSION: The hash embedder should default to 384 (matching MiniLM for compatibility) but allow any dimension via constructor:\n   pub fn new(dimension: usize) -> Self\n   pub fn default_384() -> Self { Self::new(384) }\n   pub fn default_256() -> Self { Self::new(256) }  // for fast-tier compatibility\n\n2. REGRESSION TEST VALUES: Include deterministic test cases:\n   - HashEmbedder::default_384().embed(\"hello world\") must produce the exact same vector every time\n   - Document the expected output for this input in the test so any algorithm change is detected\n\n3. TOKEN MINIMUM LENGTH: From cass hash_embedder.rs, filter tokens with length < 2 chars. This removes noise from single-character tokens.\n\n4. THE HASH EMBEDDER IS ALSO THE TEST DOUBLE: In integration tests, the hash embedder serves as both fast AND quality tier (since we can't download ML models in CI). The 2-tier pipeline works with hash as both tiers -- quality \"refinement\" just produces the same rankings, which is fine for testing the pipeline mechanics.\n","created_at":"2026-02-13T20:26:31Z"},{"id":40,"issue_id":"bd-3un.6","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Hash Embedder)\n\n## Upgrading the \"Dumb\" Embedder with Principled Hashing\n\nThe FNV-1a hash embedder is the zero-dependency fallback. It produces non-semantic embeddings. But we can make it MUCH better with principled random projection theory, while keeping zero dependencies.\n\n### 1. Random Hyperplane Hashing (Johnson-Lindenstrauss Projection)\n\nInstead of FNV-1a hash → modular projection, use a seeded random hyperplane approach:\n\n  For each token:\n    1. Hash token to u64 seed (FNV-1a, as now)\n    2. Use seed to generate d random signs via xorshift64:\n       for each dimension j:\n         bit = (seed >> (j % 64)) & 1\n         embedding[j] += if bit == 1 { 1.0 } else { -1.0 }\n    3. L2 normalize\n\nWHY THIS IS BETTER: The Johnson-Lindenstrauss lemma guarantees that random projections preserve pairwise distances with high probability. Specifically, for n points in R^D projected to R^d:\n\n  (1-epsilon) * ||u-v||^2 <= ||f(u)-f(v)||^2 <= (1+epsilon) * ||u-v||^2\n\nwith d = O(log(n) / epsilon^2). For d=384 and epsilon=0.3, this works for up to n=10^17 tokens.\n\nThe current modular projection (hash % dimension) creates collisions at rate 1/d. The random hyperplane approach spreads each token's contribution across ALL dimensions, which is provably better for preserving distance structure.\n\n### 2. Locality-Sensitive Hashing (LSH) Variant\n\nFor the hash embedder to be useful as a pre-filter (e.g., \"quickly find candidate docs before semantic search\"), we can use SimHash:\n\n  SimHash(text) = sign(sum of random_hyperplane_embedding(token) for token in text)\n\nSimHash has the property that:\n  P(SimHash(a) == SimHash(b)) = 1 - angle(a,b)/pi\n\nThis means the hash embedder's cosine similarity APPROXIMATES the true angular distance between documents' token distributions. It's not semantic, but it captures lexical overlap with formal guarantees.\n\n### 3. Weighted Token Contribution\n\nInstead of equal weight per token, use IDF-like weighting:\n\n  weight(token) = 1.0 / log(1.0 + estimated_frequency(token))\n\nEstimate frequency using the hash itself (tokens that hash to common buckets are likely common). This is a rough heuristic but mathematically motivated by TF-IDF theory.\n\n### 4. Keep It Simple\n\nThese improvements are all zero-dependency and add < 50 lines of code. The hash embedder stays fast (~0.07ms) and deterministic. The JL projection is the highest-value change — it's a single-line algorithmic improvement with formal guarantees from random matrix theory.\n","created_at":"2026-02-13T20:33:28Z"},{"id":226,"issue_id":"bd-3un.6","author":"Dicklesworthstone","text":"REVIEW FIX — Hash embedder algorithm reconciliation and entropy fix:\n\n1. ALGORITHM DECISION: The body describes simple FNV-1a modular projection. The ALIEN-ARTIFACT comment proposes JL random hyperplane hashing. These are fundamentally different algorithms with different outputs.\n\n   RESOLUTION: Keep FNV-1a modular projection as the DEFAULT implementation (simple, fast, deterministic, well-tested). The JL/SimHash variant becomes an OPTIONAL mode selectable via constructor:\n\n   pub enum HashAlgorithm {\n       /// FNV-1a with modular projection. Default. Deterministic, fast, simple.\n       FnvModular,\n       /// Johnson-Lindenstrauss random hyperplane projection. Better quality, still fast.\n       JLProjection { seed: u64 },\n   }\n\n   pub struct HashEmbedder {\n       dimension: usize,\n       algorithm: HashAlgorithm,\n   }\n\n   impl HashEmbedder {\n       /// Default: FNV-1a, 384 dimensions\n       pub fn default_384() -> Self { Self { dimension: 384, algorithm: HashAlgorithm::FnvModular } }\n       /// JL projection with seed for reproducibility\n       pub fn jl_384(seed: u64) -> Self { Self { dimension: 384, algorithm: HashAlgorithm::JLProjection { seed } } }\n   }\n\n   This preserves backwards compatibility and deterministic regression tests while offering the higher-quality JL variant.\n\n2. XORSHIFT ENTROPY BUG FIX: The ALIEN-ARTIFACT code `bit = (seed >> (j % 64)) & 1` only uses 64 bits of entropy for all dimensions. For 384 dimensions, bits 0-63 repeat ~6 times, creating correlated dimensions.\n\n   FIXED JL implementation:\n   fn jl_embed(&self, text: &str, seed: u64) -> Vec<f32> {\n       let hash = fnv1a_hash(text.as_bytes());\n       let mut vec = vec![0.0f32; self.dimension];\n       let mut rng_state = seed ^ hash;  // Combine seed with content hash\n\n       for j in 0..self.dimension {\n           // Advance xorshift64 state for EACH dimension (not just shift same seed)\n           rng_state ^= rng_state << 13;\n           rng_state ^= rng_state >> 7;\n           rng_state ^= rng_state << 17;\n\n           // Random sign: +1 or -1\n           let sign = if (rng_state & 1) == 0 { 1.0 } else { -1.0 };\n           vec[j] = sign / (self.dimension as f32).sqrt();  // 1/sqrt(d) scaling (JL guarantee)\n       }\n       l2_normalize(&mut vec);\n       vec\n   }\n\n   Each dimension now gets independent random bits from a properly-advancing PRNG.\n\n3. ASUPERSYNC NOTE: Hash embedding is pure computation (~0.07ms). The async wrapper is trivial:\n   async fn embed(&self, cx: &Cx, text: &str) -> Result<Vec<f32>, SearchError> {\n       // No cx.checkpoint() needed — too fast to warrant cancellation check\n       Ok(self.embed_sync(text))\n   }\n\n4. TEST REQUIREMENTS:\n   - Deterministic regression: HashEmbedder::default_384().embed_sync(\"hello world\") produces exact same Vec<f32> every run\n   - Dimension: output.len() == requested dimension\n   - L2 normalization: |norm - 1.0| < 1e-6\n   - Different inputs produce different embeddings\n   - Empty string produces a valid (non-zero, normalized) embedding\n   - Very long input (100KB) doesn't panic or allocate excessively\n   - JL variant: different seeds produce different embeddings for same text\n   - JL variant: same seed + same text = same embedding (deterministic)\n   - JL variant: cosine similarity of random pairs ≈ 0 (orthogonality check on 1000 random pairs, mean < 0.1)","created_at":"2026-02-13T21:46:30Z"}]}
{"id":"bd-3un.7","title":"Implement Model2Vec embedder (potion-128M fast tier)","description":"Implement the Model2Vec static embedder for the fast tier. This wraps potion-multilingual-128M (and optionally potion-retrieval-32M) which are static token embedding models — they look up pre-computed embeddings per token and mean-pool them, with no transformer inference needed.\n\nArchitecture (from xf src/model2vec_embedder.rs and agent-mail src/model2vec.rs):\n1. Load BPE tokenizer from HuggingFace tokenizer.json\n2. Load static embedding matrix from safetensors file\n3. For each input text:\n   a. Tokenize with BPE → token IDs\n   b. Look up embedding vector for each token ID in the matrix\n   c. Mean-pool all token embeddings\n   d. L2 normalize\n4. Return f32 vector\n\nModels to support:\n- potion-multilingual-128M: 256 dims, ~128MB, ~0.5-0.9ms\n  - HuggingFace: minishlab/potion-multilingual-128M\n  - This is the PRIMARY fast-tier model\n- potion-retrieval-32M: 512 dims, ~32MB, ~0.9ms (optional)\n\nDependencies:\n- tokenizers = '0.21' (HuggingFace BPE tokenizer)\n- safetensors = '0.5' (loading model weights)\n\nFeature gating: Behind 'model2vec' feature flag in frankensearch-embed\n\nKey design decisions:\n- The tokenizer and embedding matrix are loaded once and held in memory\n- Thread-safe via immutable state (no Mutex needed after init)\n- Supports MRL (Matryoshka) truncation for flexible dimension reduction\n- ModelCategory::StaticEmbedder, ModelTier::Fast\n\nFile location: frankensearch-embed/src/model2vec_embedder.rs\n\nBakeoff results (from xf results/bakeoff/BAKEOFF_REPORT.md):\n- potion-multilingual-128M: 0.574ms p50, 52,144 embeddings/sec, 223x faster than MiniLM\n- Good enough semantics for initial results that get refined\n\nReference implementations:\n- xf: src/model2vec_embedder.rs\n- agent-mail: crates/mcp-agent-mail-search-core/src/model2vec.rs\n- cass: not directly (uses daemon forwarding)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:48:40.314647546Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:33.840913851Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","fast-tier","model2vec","phase2"],"dependencies":[{"issue_id":"bd-3un.7","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:40.314647546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.7","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:07.628008904Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":6,"issue_id":"bd-3un.7","author":"Dicklesworthstone","text":"MODEL2VEC / POTION CONTEXT: Model2Vec (from the minishlab project) represents a clever middle ground between hash embeddings and full transformers. Instead of running attention at inference time, it pre-computes per-token embeddings during training and stores them as a static lookup table.\n\nAt inference time, it's just: tokenize → lookup → mean pool → normalize. No matrix multiplies, no attention, no GPU needed. This gives 223x speedup over MiniLM while retaining meaningful semantic similarity.\n\npotion-multilingual-128M specifically:\n- 128M parameters (the embedding table itself)\n- 256 output dimensions\n- Multilingual (works across languages)\n- ~0.57ms per embedding on CPU\n- Semantic quality: good enough for initial results that will be refined\n\nThe safetensors format is used for the embedding weights (efficient memory-mapped loading). The tokenizer is standard HuggingFace BPE (tokenizer.json).\n\nThis is the 'secret sauce' that makes the 2-tier system viable — without a fast-enough embedding model, you'd just use MiniLM and accept the latency.","created_at":"2026-02-13T17:56:49Z"},{"id":24,"issue_id":"bd-3un.7","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. TENSOR DISCOVERY: The safetensors file may use different tensor names across model versions. From xf model2vec_embedder.rs, search in order: \"embeddings\", \"embedding\", \"word_embeddings\", \"embed\", \"emb\". If only one tensor exists in the file, use it regardless of name. This prevents breakage when model authors rename tensors.\n\n2. REQUIRED FILES: Only 2 files needed (from xf):\n   REQUIRED_FILES = [\"tokenizer.json\", \"model.safetensors\"]\n   Not the 5+ files that ONNX models need. This is a key advantage of Model2Vec.\n\n3. MEMORY LAYOUT: The embedding matrix is loaded as Vec<Vec<f32>> with shape [vocab_size x dimension]. For potion-128M with vocab ~32K and dim 256, this is ~32MB resident. The matrix is immutable after load so no Mutex needed.\n\n4. EXPECTED TENSOR SHAPE: Expect a 2D f32 tensor of shape [vocab_size, dimensions]. Validate both dimensions on load and return SearchError::ModelLoadFailed if mismatched.\n","created_at":"2026-02-13T20:26:17Z"},{"id":229,"issue_id":"bd-3un.7","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.7 (Model2Vec):\n- Tokenization correctness: known input produces expected token IDs\n- Mean pooling verification: manual calculation matches embedder output\n- L2 normalization: |norm - 1.0| < 1e-6\n- Dimension validation: output.len() == 256\n- Tensor shape mismatch error: wrong dimension in safetensors → clear error\n- Model not found: missing files → SearchError::ModelNotFound\n- is_semantic() returns true\n- is_ready() returns true after load, false before\n- Empty string produces valid (non-zero) embedding\n- Thread safety: immutable state after load, no Mutex needed — verify Send + Sync","created_at":"2026-02-13T21:47:33Z"}]}
{"id":"bd-3un.8","title":"Implement FastEmbed embedder (MiniLM-L6-v2 quality tier)","description":"Implement the FastEmbed/ONNX-based embedder for the quality tier. This wraps all-MiniLM-L6-v2 via the fastembed crate (which uses ONNX Runtime under the hood). This is the 'gold standard' embedding model for search quality.\n\nArchitecture (from cass src/search/fastembed_embedder.rs and xf src/fastembed_embedder.rs):\n1. Load ONNX model from local directory (model.onnx + tokenizer.json + config.json)\n2. Create ONNX Runtime session (CPU execution provider)\n3. For each input:\n   a. Tokenize with WordPiece tokenizer\n   b. Run ONNX inference (attention + pooling)\n   c. Mean-pool hidden states\n   d. L2 normalize\n\nModel details:\n- all-MiniLM-L6-v2: 384 dims, ~90MB model file, ~128ms inference\n  - HuggingFace: sentence-transformers/all-MiniLM-L6-v2\n  - Revision: c9745ed1d9f207416be6d2e6f8de32d1f16199bf (pinned)\n  - Note: model moved to onnx/ subdirectory in 2026-01 restructuring\n\nDependencies:\n- fastembed = '4.9' with features ['ort-download-binaries']\n  - OR use ort directly for more control\n\nFeature gating: Behind 'fastembed' feature flag\n\nKey design decisions:\n- Model wrapped in Mutex<TextEmbedding> because ONNX sessions aren't Send+Sync\n- Batch support via embed_batch() for 3x throughput during indexing\n- ModelCategory::TransformerEmbedder, ModelTier::Quality\n- Load time: ~100ms (one-time cost)\n\nRequired model files (per cass src/search/model_download.rs):\n- onnx/model.onnx (90MB, SHA256: 6fd5d72fe4589f189f8ebc006442dbb529bb7ce38f8082112682524616046452)\n- tokenizer.json (466KB)\n- config.json\n- special_tokens_map.json\n- tokenizer_config.json\n\nFile location: frankensearch-embed/src/fastembed_embedder.rs\n\nBakeoff results:\n- all-MiniLM-L6-v2: 128ms p50, 228 embeddings/sec (baseline)\n- Best quality-to-speed ratio of all tested models\n- Significantly better semantics than static embedders\n\nReference implementations:\n- cass: src/search/fastembed_embedder.rs (210 lines)\n- xf: src/fastembed_embedder.rs\n- agent-mail: crates/mcp-agent-mail-search-core/src/fastembed.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:48:40.395258044Z","created_by":"ubuntu","updated_at":"2026-02-13T21:47:33.940845176Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","fastembed","phase2","quality-tier"],"dependencies":[{"issue_id":"bd-3un.8","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:40.395258044Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.8","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:07.716683310Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":44,"issue_id":"bd-3un.8","author":"Dicklesworthstone","text":"REVISION: FastEmbed Embedder Hardening\n\nCritical implementation details for ONNX Runtime integration:\n\n1. Sigmoid Activation on Reranker Outputs:\n   While FastEmbed produces embeddings (not logit scores), this bead should document that\n   embeddings are L2-normalized post-inference. If any future cross-encoder mode is added,\n   raw ONNX logits need sigmoid activation (see bd-3un.25 for the reranker case).\n\n2. ONNX Error Handling:\n   - Model not found: Return SearchError::EmbeddingError with model path\n   - Inference failure: Catch ort::Error, wrap in SearchError, log at WARN with query length\n   - Session creation failure: Fail fast at init, not at first embed() call\n   - Thread pool exhaustion: ort uses rayon internally; if calling from rayon, use spawn_blocking\n     to avoid thread starvation (nested rayon deadlock)\n\n3. Mutex Contention Under Concurrent Access:\n   TextEmbedding is !Send + !Sync, hence Mutex wrapping. Under high concurrency:\n   - Single-threaded embedding is the bottleneck (~128ms per call)\n   - Consider multiple sessions (2-4) behind a round-robin or channel-based pool\n   - For V1: single Mutex is fine; document the contention risk for V2\n\n4. Batch Processing:\n   - batch_size=32 is optimal for MiniLM (fits in L2 cache for typical token lengths)\n   - Batch embed reduces per-call overhead from session lock acquisition\n   - Pre-allocate output Vec with exact capacity (batch_size * dimension)\n   - Track batch timing: log at DEBUG level \"embedded {n} docs in {ms}ms ({per_doc}ms/doc)\"\n\n5. Memory Footprint:\n   - ONNX model: ~90MB resident after load\n   - Session creation: one-time ~2s startup cost\n   - GraphOptimizationLevel::Level3 for production (Level1 for faster startup in tests)\n   - Document in tracing: INFO \"quality_model_loaded\" with model_size_bytes, load_time_ms\n\n6. Model File Verification:\n   - SHA256 check on model.onnx at load time (cross-reference bd-3un.10 manifest)\n   - If verification fails: log ERROR, return EmbeddingError, fall back to fast tier\n","created_at":"2026-02-13T20:44:48Z"},{"id":150,"issue_id":"bd-3un.8","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MODERATE REVISION (Mutex + session pool):\n\nBEFORE:\n  - std::sync::Mutex<TextEmbedding> wrapping ONNX session\n  - Comment suggests channel-based pool for V2\n\nAFTER:\n  - asupersync::sync::Mutex<TextEmbedding> (cancel-aware lock acquisition)\n  - For V2: asupersync::sync::Pool<OrtSession> for session pooling (built-in)\n\nKEY CHANGE: asupersync::sync::Mutex is cancel-aware. If a task holding the Mutex is cancelled, the Mutex is properly released (no poison). If a task WAITING for the Mutex is cancelled, it stops waiting cleanly (no deadlock).\n\nREVISED:\n  pub struct FastEmbedEmbedder {\n      session: asupersync::sync::Mutex<ort::Session>,\n      // OR for V2:\n      // session_pool: asupersync::sync::Pool<ort::Session>,\n  }\n\n  impl FastEmbedEmbedder {\n      pub async fn embed(&self, cx: &Cx, text: &str) -> asupersync::Result<Vec<f32>> {\n          let session = self.session.lock(cx).await?;  // Cancel-aware\n          cx.checkpoint()?;\n          // Run inference (CPU-bound, synchronous)\n          let result = session.run(inputs)?;\n          Ok(result)\n      }\n  }\n\n  // V2 pool variant:\n  impl FastEmbedEmbedder {\n      pub async fn embed(&self, cx: &Cx, text: &str) -> asupersync::Result<Vec<f32>> {\n          let session = self.session_pool.checkout(cx).await?;  // Cancel-aware checkout\n          let result = session.run(inputs)?;\n          self.session_pool.return_resource(cx, session)?;  // Return to pool\n          Ok(result)\n      }\n  }\n\nNOTE: ort (ONNX Runtime) is retained — it's the inference engine, not an async runtime. The Mutex/Pool wrapping changes to asupersync's cancel-aware versions.\n\nTESTING: Lab runtime + ContendedMutex for contention testing:\n  - asupersync::sync::ContendedMutex tracks lock contention metrics\n  - Lab runtime can deterministically reproduce contention scenarios","created_at":"2026-02-13T21:06:12Z"},{"id":230,"issue_id":"bd-3un.8","author":"Dicklesworthstone","text":"REVIEW FIX — Test requirements for bd-3un.8 (FastEmbed):\n- Embedding dimension: output.len() == 384\n- Batch vs single-embed equivalence: embed(\"hello\") == embed_batch([\"hello\"])[0]\n- Mutex contention: 4 threads calling embed() concurrently → no panic, no deadlock\n- Model verification: SHA256 mismatch → clear error before loading\n- ONNX inference error: malformed input → SearchError, not panic\n- is_semantic() returns true\n- is_ready() returns true after successful load\n- Cancel-aware: asupersync Mutex lock cancelled mid-wait → clean Cancelled error","created_at":"2026-02-13T21:47:33Z"}]}
{"id":"bd-3un.9","title":"Implement embedder auto-detection and fallback chain","description":"Implement automatic embedder detection and graceful fallback chain. When a consumer creates a search context, the system should automatically detect which models are available and build the appropriate embedder stack.\n\nFallback chain (priority order):\n1. Quality: MiniLM-L6-v2 (if ONNX model files present)\n2. Fast: potion-multilingual-128M (if safetensors files present)\n3. Hash: FNV-1a (always available, zero deps)\n\nAvailability detection:\n- Check model directory for required files (model.onnx, tokenizer.json, etc.)\n- Use platform-specific data dirs (dirs crate) for model cache location\n- Environment variable overrides: FRANKENSEARCH_MODEL_DIR\n- Log which models are available/unavailable at startup (tracing)\n\npub enum TwoTierAvailability {\n    Full,         // Both fast (potion) + quality (MiniLM) available\n    FastOnly,     // Only potion available → no quality refinement\n    QualityOnly,  // Only MiniLM → slower but no fast phase\n    HashOnly,     // Only hash → lexical-dominant search, no real semantics\n}\n\npub struct EmbedderStack {\n    fast: Arc<dyn Embedder>,\n    quality: Option<Arc<dyn Embedder>>,\n    availability: TwoTierAvailability,\n}\n\nimpl EmbedderStack {\n    pub fn auto_detect(model_dir: &Path) -> Self { ... }\n    pub fn fast(&self) -> &dyn Embedder { ... }\n    pub fn quality(&self) -> Option<&dyn Embedder> { ... }\n}\n\nThis is the 'resolver' that consumers call instead of manually instantiating embedders.\n\nReference: agent-mail crates/mcp-agent-mail-search-core/src/auto_init.rs (TwoTierContext, TwoTierAvailability)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:48:53.173644672Z","created_by":"ubuntu","updated_at":"2026-02-13T21:46:47.781533411Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","fallback","phase2"],"dependencies":[{"issue_id":"bd-3un.9","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:53.173644672Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.10","type":"blocks","created_at":"2026-02-13T21:46:47.781484259Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.6","type":"blocks","created_at":"2026-02-13T17:55:07.798634Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.7","type":"blocks","created_at":"2026-02-13T17:55:07.881752898Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.8","type":"blocks","created_at":"2026-02-13T17:55:07.963140142Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":19,"issue_id":"bd-3un.9","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. MRL DIMENSION REDUCTION WRAPPER: Add a DimReduceEmbedder that transparently applies Matryoshka Representation Learning (MRL) dimension reduction. From xf model_registry.rs:\n\npub struct DimReduceEmbedder {\n    inner: Arc<dyn Embedder>,\n    target_dim: usize,\n}\n\nimpl Embedder for DimReduceEmbedder {\n    fn embed(&self, text: &str) -> SearchResult<Vec<f32>> {\n        let full = self.inner.embed(text)?;\n        // Take first target_dim elements, then L2 normalize\n        Ok(l2_normalize(&full[..self.target_dim]))\n    }\n    fn dimension(&self) -> usize { self.target_dim }\n    fn id(&self) -> &str { /* format!(\"{}-mrl{}\", self.inner.id(), self.target_dim) */ }\n    fn supports_mrl(&self) -> bool { true }\n    // delegate all other methods to self.inner\n}\n\nConstruction validation:\n- inner.supports_mrl() must be true (error if not)\n- target_dim must be in [1, inner.dimension()] (error if out of range)\n\nThe EmbedderStack should auto-wrap with DimReduceEmbedder when:\n- User requests a specific dimension via config\n- The best available model supports MRL\n- Requested dimension is smaller than model's native dimension\n\nThis makes MRL \"just work\" -- users set target_dim in config and the stack handles everything.\n\n2. AUTO-DETECT LOGGING: Log detailed availability info at startup:\n   INFO \"embedder_detected\" { model, tier, dimension, path, load_time_ms }\n   WARN \"embedder_unavailable\" { model, tier, reason, checked_paths }\n   INFO \"embedder_stack_ready\" { availability, fast_model, quality_model }\n","created_at":"2026-02-13T20:25:03Z"},{"id":34,"issue_id":"bd-3un.9","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Embedder Auto-Detection & Quality)\n\n## Mathematical Upgrade: Formal Quality Assessment with Conformal Prediction\n\nThe current EmbedderStack auto-detects available models but has no formal quality measurement. Add principled quality assessment that provides distribution-free guarantees.\n\n### 1. Conformal Prediction for Score Reliability\n\nGiven a calibration set of (query, relevant_docs) pairs, conformal prediction provides sets C(q) such that:\n\n  P(relevant_doc ∈ top_k(search(q))) ≥ 1 - α\n\nfor any α. No distributional assumptions needed. Implementation:\n\n  pub struct ConformalCalibration {\n      nonconformity_scores: Vec<f32>,  // Sorted from calibration\n      alpha: f32,                       // Desired coverage (default: 0.1)\n  }\n\n  impl ConformalCalibration {\n      /// Calibrate using a set of (query, known_relevant_doc) pairs\n      pub fn calibrate(searcher: &TwoTierSearcher, cal_set: &[(String, String)]) -> Self {\n          let scores: Vec<f32> = cal_set.iter().map(|(query, relevant_doc_id)| {\n              let results = searcher.search_flat(query, 100);\n              // Nonconformity = rank of the relevant doc (lower = better)\n              results.iter().position(|r| r.doc_id == *relevant_doc_id)\n                  .map(|r| r as f32)\n                  .unwrap_or(f32::INFINITY)\n          }).collect();\n          let mut sorted = scores;\n          sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());\n          Self { nonconformity_scores: sorted, alpha: 0.1 }\n      }\n\n      /// For a new query, how many results to return to guarantee coverage\n      pub fn required_k(&self) -> usize {\n          let quantile_idx = ((1.0 - self.alpha) * self.nonconformity_scores.len() as f32).ceil() as usize;\n          self.nonconformity_scores[quantile_idx.min(self.nonconformity_scores.len() - 1)] as usize + 1\n      }\n  }\n\nThis tells you: \"to guarantee 90% probability of including the relevant document, you need to return at least k results.\" FORMAL coverage guarantee with no distributional assumptions.\n\n### 2. Bayesian A/B Testing for Embedder Comparison (Bakeoff)\n\nInstead of point estimates (avg NDCG), use Bayesian A/B testing with Beta posteriors:\n\n  // For each embedder pair (A, B), on each query:\n  //   If A ranks relevant doc higher: A_wins += 1\n  //   If B ranks relevant doc higher: B_wins += 1\n\n  // P(A better than B) = P(Beta(A_wins+1, B_wins+1) > 0.5)\n  // = regularized incomplete beta function\n\n  // Decision: if P(A > B) > 0.95, declare A the winner\n  //           if P(A > B) < 0.05, declare B the winner\n  //           else: need more queries (continue testing)\n\nThis provides:\n- Formal stopping criterion (don't over-test or under-test)\n- Probability of correctness (not just \"statistically significant\")\n- Natural handling of ties and near-ties\n\n### 3. Mutual Information for Feature Selection\n\nWhen choosing which embedder for which query type, compute mutual information:\n\n  MI(embedder, relevance | query_type) = Σ p(e,r|q) log(p(e,r|q) / p(e|q)p(r|q))\n\nThis tells you which embedder is most informative for each query type. High MI = the embedder's rankings correlate with actual relevance. Low MI = the embedder adds noise for this query type.\n\nUse this in EmbedderStack to ROUTE queries to the best embedder per query type, rather than always using the same fast/quality pair.\n\n### Implementation Priority\n\n1. Conformal calibration: add as optional constructor on TwoTierSearcher (requires calibration data)\n2. Bayesian A/B: add to bakeoff infrastructure (bd-3un.12)\n3. MI-based routing: add to EmbedderStack as adaptive mode (requires usage data)\n\nAll are optional enhancements that don't change the core API.\n","created_at":"2026-02-13T20:29:56Z"},{"id":227,"issue_id":"bd-3un.9","author":"Dicklesworthstone","text":"REVIEW FIX — Embedder auto-detection semantics and missing dependency:\n\n1. QualityOnly CLARIFICATION: When only the quality embedder (MiniLM) is available but no fast embedder (potion), the hash embedder should serve as the fast tier. The hash embedder is ALWAYS available (zero dependencies). Therefore:\n\n   TwoTierAvailability::QualityOnly should be REMOVED. Instead:\n\n   pub enum TwoTierAvailability {\n       /// Both semantic tiers available (ideal)\n       FullTwoTier { fast: Arc<dyn SendEmbedder>, quality: Arc<dyn SendEmbedder> },\n       /// Only quality available; hash embedder serves as fast tier\n       QualityWithHashFast { fast: Arc<dyn SendEmbedder>, quality: Arc<dyn SendEmbedder> },\n       /// Only fast tier available; no quality refinement\n       FastOnly { fast: Arc<dyn SendEmbedder> },\n       /// Only hash embedder available (no ML models)\n       HashOnly { fast: Arc<dyn SendEmbedder> },\n   }\n\n   In QualityWithHashFast, the fast field IS the hash embedder. The consumer doesn't need to know — the EmbedderStack API is the same.\n\n   Actually, simpler: just always fill both slots:\n\n   pub struct EmbedderStack {\n       pub fast: Arc<dyn SendEmbedder>,     // Best available fast embedder (potion > hash)\n       pub quality: Option<Arc<dyn SendEmbedder>>,  // Quality embedder if available\n       pub availability: Availability,       // For diagnostics/logging\n   }\n\n   pub enum Availability {\n       Full,              // potion + MiniLM\n       QualityAndHash,    // hash + MiniLM (potion unavailable)\n       FastOnly,          // potion only (MiniLM unavailable)\n       HashOnly,          // hash only (no ML models found)\n   }\n\n   The `fast` field ALWAYS has a value (at minimum, hash). The `quality` field is None when no quality model is available.\n\n2. DETECTION ORDER (clarified):\n   1. Check for MiniLM-L6-v2 → quality candidate\n   2. Check for potion-128M → fast candidate\n   3. Hash embedder → always available as fallback fast\n\n   Stack construction:\n   - fast = potion if available, else hash\n   - quality = MiniLM if available, else None\n\n3. MISSING DEPENDENCY: Add bd-3un.10 (model manifest) as a dependency. Auto-detection should use model manifests to verify model integrity (SHA256) before declaring a model \"available\". Without this, a corrupted model file would be detected as \"available\" but fail at runtime.\n\n4. ASUPERSYNC NOTE: auto_detect() should be async since model loading may involve file I/O:\n   pub async fn auto_detect(cx: &Cx, model_dir: &Path) -> Result<EmbedderStack, SearchError>\n\n5. TEST REQUIREMENTS:\n   - Empty model directory → HashOnly\n   - Only MiniLM present → QualityAndHash (hash as fast)\n   - Only potion present → FastOnly\n   - Both present → Full\n   - Corrupted model file → falls back gracefully (doesn't panic)\n   - Model directory doesn't exist → HashOnly (not an error)\n   - DimReduceEmbedder wrapping: embed dimension matches requested MRL dimension","created_at":"2026-02-13T21:46:35Z"}]}
{"id":"bd-3w1","title":"Epic: FrankenSQLite + pervasive RaptorQ integration","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-13T20:36:01.147613828Z","created_by":"ubuntu","updated_at":"2026-02-13T20:36:29.796657411Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["epic","frankensqlite","raptorq"],"comments":[{"id":42,"issue_id":"bd-3w1","author":"Dicklesworthstone","text":"EPIC OVERVIEW: FrankenSQLite + Pervasive RaptorQ Integration\n\nIntegrate FrankenSQLite (clean-room Rust SQLite reimplementation from /dp/frankensqlite) into the frankensearch crate for two synergistic purposes:\n\n1. FrankenSQLite as the document metadata store and persistent embedding job queue, replacing ad-hoc in-memory structures with crash-safe MVCC-capable SQL storage\n2. Applying the 'pervasive RaptorQ' concept (RFC 6330 fountain codes) to frankensearch's own persistent artifacts (FSVI vector indices, Tantivy segments) for self-healing durability\n\nBACKGROUND ON FRANKENSQLITE:\n- 24-crate Cargo workspace, clean-room Rust reimplementation of SQLite\n- #![forbid(unsafe_code)] throughout, Rust edition 2024 nightly\n- Two architectural innovations:\n  a) Page-level MVCC concurrent writers (multiple writers commit simultaneously)\n  b) RaptorQ-pervasive durability (RFC 6330 fountain codes at every persistent layer)\n- 100% file format compatibility with C SQLite in Compatibility mode\n- Native mode with content-addressed ECS (Erasure-Coded Stream) objects\n- Built-in FTS5, R-Tree, JSON1, Session extensions\n\nWHAT \"PERVASIVE RAPTORQ\" MEANS:\n- RaptorQ (RFC 6330) is a fountain code: source data splits into K symbols, repair symbols generated (can be infinite), any K of (K+R) symbols recover the original\n- \"Pervasive\" means fountain codes woven into EVERY persistent layer, not bolted on:\n  - WAL: Each committed frame group gets repair symbols in .wal-fec sidecar\n  - ECS Objects: Every durable object is content-addressed with BLAKE3, carries erasure coding\n  - Snapshot Transfer: Rateless coding for bandwidth-optimal replication\n  - Deterministic Repair: Given object + repair count R, symbols always identical (seed = xxh3_64 of object_id)\n- Key config: PRAGMA raptorq_repair_symbols = N (default: 2), DEFAULT_OVERHEAD_PERCENT = 20%\n\nKEY FRANKENSQLITE APIS:\n  Connection::open(path) -> Result<Self>\n  conn.execute(sql, params) -> Result<u64>\n  conn.prepare(sql) -> Result<PreparedStatement>\n  conn.transaction() -> Result<Transaction>\n\nRAPTORQ INTEGRATION TRAITS:\n  trait SymbolCodec: Send + Sync {\n      fn encode(&self, source_data: &[u8], symbol_size: u32, repair_overhead: f64) -> Result<CodecEncodeResult>;\n      fn decode(&self, symbols: &[(u32, Vec<u8>)], k_source: u32, symbol_size: u32) -> Result<CodecDecodeResult>;\n  }\n\nINTEGRATION TIERS:\n- TIER 1 (Document Store, P1): FrankenSQLite tables for doc metadata, content hashes, embedding status, persistent job queues\n- TIER 2 (Self-Healing Indices, P1): RaptorQ repair symbol trailers on FSVI files and Tantivy wrappers\n- TIER 3 (FTS5 Alternative, P2): FrankenSQLite FTS5 as alternative/fallback lexical engine\n- TIER 4 (Future: Native Mode, P3): ECS commits, time-travel, quorum durability\n\nThis epic is a SIBLING of bd-3un (the main frankensearch epic). Tasks here either add new capabilities or enhance existing bd-3un tasks with FrankenSQLite integration.\n","created_at":"2026-02-13T20:36:29Z"}]}
{"id":"bd-3w1.1","title":"Add frankensearch-storage crate for FrankenSQLite integration","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:36:36.342688123Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:03.026607651Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation","frankensqlite","scaffold","tier1"],"dependencies":[{"issue_id":"bd-3w1.1","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T20:37:07.676851691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.1","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:37:07.796680615Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.1","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:36:36.342688123Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":43,"issue_id":"bd-3w1.1","author":"Dicklesworthstone","text":"TASK: Add a new frankensearch-storage sub-crate to the workspace.\n\nThis crate is the bridge between frankensearch and FrankenSQLite. It owns all SQL schema, document metadata persistence, and the embedding job queue backed by FrankenSQLite tables.\n\nCRATE STRUCTURE:\n  crates/frankensearch-storage/\n    Cargo.toml\n    src/\n      lib.rs           -- Public API re-exports\n      connection.rs    -- FrankenSQLite connection pool and initialization\n      schema.rs        -- SQL schema definitions and migrations\n      document.rs      -- Document metadata CRUD operations\n      job_queue.rs     -- Persistent embedding job queue\n      content_hash.rs  -- SHA-256 content dedup tracking\n      metrics.rs       -- Storage-level metrics (atomic counters)\n\nCARGO.TOML DEPENDENCIES:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite = { path = \"/dp/frankensqlite/fsqlite\" }  # The facade crate\n  sha2 = \"0.10\"       # SHA-256 for content hashing\n  tracing = \"0.1\"     # Structured logging\n  serde = { version = \"1\", features = [\"derive\"] }\n  serde_json = \"1\"\n\nFEATURE FLAG INTEGRATION:\n  The storage crate should be feature-gated in the workspace:\n  [features]\n  storage = [\"dep:frankensearch-storage\"]\n\n  This keeps FrankenSQLite optional for consumers who only need in-memory search.\n  But 'full' and 'hybrid' bundles should include 'storage' by default.\n\nDESIGN DECISIONS:\n1. FrankenSQLite is used as a LOCAL dependency (path dep), not a crates.io dep, because it's a sibling project\n2. The crate owns the Connection lifecycle -- consumers never touch raw SQL\n3. All tables use WAL mode by default for concurrent read/write\n4. Schema versioning via a 'schema_version' table for safe migrations\n5. Thread-safe: Connection wrapped in Arc for sharing across threads\n\nINITIALIZATION PATTERN:\n  pub struct StorageConfig {\n      pub db_path: PathBuf,\n      pub wal_mode: bool,            // Default: true\n      pub busy_timeout_ms: u64,      // Default: 5000\n      pub raptorq_repair_symbols: u32, // Default: 2 (FrankenSQLite PRAGMA)\n      pub cache_size_pages: i32,     // Default: 2000 (~8MB at 4KB pages)\n  }\n\n  pub struct Storage {\n      conn: Connection,   // FrankenSQLite connection\n      config: StorageConfig,\n  }\n\n  impl Storage {\n      pub fn open(config: StorageConfig) -> SearchResult<Self>;\n      pub fn open_in_memory() -> SearchResult<Self>;  // For tests\n      pub fn connection(&self) -> &Connection;\n      pub fn transaction(&self) -> SearchResult<Transaction>;\n  }\n\nWHY FRANKENSQLITE OVER RUSQLITE:\n1. MVCC concurrent writers: embedding workers and search queries don't block each other\n2. Pervasive RaptorQ: automatic self-healing for the metadata store itself\n3. FTS5 built-in: can serve as alternative lexical engine (Tier 3)\n4. Same Rust ecosystem: safe Rust, no FFI, no C SQLite dependency\n5. Future: Native mode enables time-travel queries and distributed replication\n","created_at":"2026-02-13T20:37:03Z"},{"id":113,"issue_id":"bd-3w1.1","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. TRANSACTION API: FrankenSQLite does NOT have a Connection::transaction() method. Transactions must use raw SQL strings:\n   conn.execute(\"BEGIN\")?;\n   // ... operations ...\n   conn.execute(\"COMMIT\")?;\n   // Or on error: conn.execute(\"ROLLBACK\")?;\n\n   The Storage wrapper should provide a transaction() helper that wraps this:\n   pub fn transaction<F, T>(&self, f: F) -> SearchResult<T>\n   where F: FnOnce(&Connection) -> SearchResult<T>\n   {\n       self.conn.execute(\"BEGIN\")?;\n       match f(&self.conn) {\n           Ok(v) => { self.conn.execute(\"COMMIT\")?; Ok(v) }\n           Err(e) => { let _ = self.conn.execute(\"ROLLBACK\"); Err(e) }\n       }\n   }\n\n   For concurrent access, use \"BEGIN CONCURRENT\" (FrankenSQLite MVCC mode):\n   self.conn.execute(\"BEGIN CONCURRENT\")?;\n\n2. PARAMETER BINDING: Connection uses SqliteValue enum for params, not generic impl Params:\n   conn.execute_with_params(sql, &[SqliteValue::Text(\"hello\".into())])?;\n   conn.query_with_params(sql, &[SqliteValue::Integer(42)])?;\n   The Storage wrapper should provide ergonomic helpers that convert Rust types to SqliteValue.\n\n3. ROW ACCESS: Row::get(index) returns Option<&SqliteValue>, not typed extraction:\n   let row = conn.query_row(\"SELECT count(*) FROM documents\")?;\n   match row.get(0) {\n       Some(SqliteValue::Integer(n)) => Ok(*n as usize),\n       _ => Err(SearchError::StorageError(\"unexpected type\".into())),\n   }\n   Consider a typed helper: fn get_i64(row: &Row, idx: usize) -> SearchResult<i64>\n\n4. CONNECTION LIFECYCLE: Connection must be explicitly closed via close() or dropped. No connection pooling. For thread safety, wrap in Arc<Mutex<Connection>> or use one connection per thread.\n\n5. FSQLITE DEPENDENCY PATH: The correct path dependency is:\n   fsqlite = { path = \"/dp/frankensqlite/fsqlite\" }\n   NOT fsqlite-core. The facade crate re-exports Connection from fsqlite-core internally.\n\n6. ALSO NEED asupersync: The actual RaptorQ implementation lives in /dp/asupersync, which fsqlite-core depends on. The durability crate may need to depend on asupersync directly for the SymbolCodec trait implementation, or re-use fsqlite-core's abstraction.\n","created_at":"2026-02-13T20:58:03Z"}]}
{"id":"bd-3w1.10","title":"Implement FTS5 alternative lexical engine adapter","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:37:28.727069396Z","created_by":"ubuntu","updated_at":"2026-02-13T21:55:10.079157036Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","fts5","lexical","tier3"],"dependencies":[{"issue_id":"bd-3w1.10","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:47:07.572949758Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:28.727069396Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:29.234471157Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:38.878690Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":63,"issue_id":"bd-3w1.10","author":"Dicklesworthstone","text":"TASK: Implement FrankenSQLite FTS5 as an alternative lexical search engine.\n\nFrankenSQLite includes a full FTS5 implementation with BM25 ranking. This task creates an adapter that implements frankensearch's LexicalIndex trait using FTS5, providing an alternative to Tantivy for text search.\n\nWHY FTS5 AS ALTERNATIVE:\n1. Single dependency: FrankenSQLite already provides FTS5, no need for separate Tantivy dep\n2. Smaller binary: FTS5 is embedded in the SQLite engine vs Tantivy (~5MB added binary size)\n3. Transactional consistency: FTS5 index and document metadata are in the SAME database, atomically consistent\n4. MVCC: concurrent readers and writers without blocking\n5. Simpler deployment: one .db file contains everything (data + FTS index + vector metadata)\n\nTRADE-OFFS vs TANTIVY:\n| Feature | Tantivy | FTS5 |\n|---------|---------|------|\n| BM25 ranking | Yes | Yes |\n| Phrase queries | Yes | Yes |\n| Prefix queries | Yes (edge n-grams) | Yes (prefix*) |\n| Boolean operators | AND, OR, NOT | AND, OR, NOT |\n| Tokenizer customization | Full control | unicode61, porter, trigram |\n| Performance (search) | Faster (purpose-built) | Adequate (embedded) |\n| Performance (index) | Faster (batch-oriented) | Adequate (row-at-a-time) |\n| Snippet generation | Via tantivy-highlights | Built-in highlight(), snippet() |\n| Binary size impact | ~5MB | 0 (already in FrankenSQLite) |\n| Concurrent write | Requires IndexWriter lock | MVCC (fully concurrent) |\n\nADAPTER API:\n\n  pub struct Fts5LexicalIndex {\n      storage: Arc<Storage>,\n      table_name: String,  // FTS5 virtual table name\n  }\n\n  impl Fts5LexicalIndex {\n      pub fn create(storage: Arc<Storage>, config: Fts5Config) -> SearchResult<Self>;\n\n      /// Create FTS5 virtual table with content sync\n      /// CREATE VIRTUAL TABLE {name} USING fts5(\n      ///     doc_id,\n      ///     title,\n      ///     content,\n      ///     content_preview,\n      ///     tokenize='unicode61 remove_diacritics 2',\n      ///     content=documents,          -- External content from documents table\n      ///     content_rowid=rowid\n      /// );\n      fn create_fts5_table(&self) -> SearchResult<()>;\n  }\n\n  // Implement the same LexicalIndex trait that Tantivy uses\n  impl LexicalIndex for Fts5LexicalIndex {\n      fn search(&self, query: &str, limit: usize, offset: usize) -> SearchResult<Vec<LexicalHit>>;\n      fn index_document(&self, doc: &IndexableDocument) -> SearchResult<()>;\n      fn index_batch(&self, docs: &[IndexableDocument]) -> SearchResult<usize>;\n      fn delete_document(&self, doc_id: &str) -> SearchResult<bool>;\n      fn document_count(&self) -> SearchResult<usize>;\n      fn optimize(&self) -> SearchResult<()>;\n  }\n\n  pub struct Fts5Config {\n      pub tokenizer: Fts5Tokenizer,      // Default: Unicode61\n      pub content_mode: Fts5ContentMode, // Default: External (references documents table)\n      pub prefix_sizes: Vec<usize>,      // Default: [2, 3] (for prefix queries)\n  }\n\n  pub enum Fts5Tokenizer {\n      Unicode61 { remove_diacritics: bool },\n      Porter,                             // English stemming\n      Trigram,                            // Substring search (slower but more flexible)\n  }\n\n  pub enum Fts5ContentMode {\n      Regular,                            // FTS5 stores its own copy\n      External,                           // References documents table (saves space)\n      Contentless,                        // Index-only (no snippets)\n  }\n\nSEARCH QUERY TRANSLATION:\n  frankensearch queries need translation to FTS5 syntax:\n  - Simple terms: \"hello world\" -> 'hello world' (implicit AND in FTS5)\n  - Phrase: '\"exact match\"' -> '\"exact match\"'\n  - Boolean: 'cat OR dog' -> 'cat OR dog'\n  - Prefix: 'hel*' -> 'hel*'\n  - Column filter: 'title:hello' -> 'title:hello'\n\n  FTS5 returns: doc_id, rank (BM25 score), snippet\n\nCONTENT SYNC:\n  When using External content mode, the FTS5 index references the documents table.\n  On document insert/update/delete, FTS5 must be notified:\n  - INSERT: INSERT INTO fts5_table(rowid, ...) VALUES (...)\n  - DELETE: INSERT INTO fts5_table(fts5_table, rowid, ...) VALUES ('delete', ...)\n  - UPDATE: DELETE then INSERT (FTS5 doesn't support in-place update)\n\n  This sync is handled in the document upsert flow (bd-3w1.2).\n\nFEATURE FLAG:\n  This adapter is gated behind 'fts5' feature (separate from 'lexical' which is Tantivy):\n  [features]\n  fts5 = [\"dep:frankensearch-storage\"]  # FTS5 requires the storage crate (FrankenSQLite)\n  lexical = [\"dep:tantivy\"]              # Tantivy (existing)\n\n  Consumers choose one or both. The fusion layer (RRF) works with either.\n\nFile: frankensearch-storage/src/fts5_adapter.rs\n","created_at":"2026-02-13T20:46:11Z"},{"id":122,"issue_id":"bd-3w1.10","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. CRITICAL: FTS5 ContentMode IS BINARY: FrankenSQLite's FTS5 only supports Stored and Contentless modes, NOT External content mode. The bead incorrectly specifies content=documents (external content referencing the documents table). This means:\n   - Stored mode: FTS5 stores its own copy of the content (duplicates storage but simpler)\n   - Contentless mode: FTS5 only stores the index, no content retrieval (no snippets)\n\n   For our use case, we should use STORED mode because:\n   a) We need snippet generation (highlight(), snippet())\n   b) The storage overhead is acceptable (FTS5 content is compressed internally)\n   c) External content mode requires complex trigger-based sync which isn't available\n\n   UPDATED Fts5ContentMode enum:\n   pub enum Fts5ContentMode {\n       Stored,       // FTS5 stores its own copy (default, recommended)\n       Contentless,  // Index-only, no snippet support (saves space)\n   }\n\n   Remove the \"External\" variant from the bead's design.\n\n2. VIRTUAL TABLE CREATION: FrankenSQLite FTS5 uses VirtualTable and VirtualTableCursor traits from fsqlite_func::vtab. The CREATE VIRTUAL TABLE syntax should work but the internal wiring is different from C SQLite's xCreate/xConnect. Verify that fsqlite-ext-fts5 is correctly registered as an extension.\n\n3. DELETE SEMANTICS: FTS5 has a DeleteAction enum: Reject | Tombstone | PhysicalPurge. When deleting documents from the search index, use Tombstone for soft delete (cheaper, eventual cleanup) or PhysicalPurge for immediate removal. This should be configurable in Fts5Config.\n\n4. DEPENDENCY FIX: bd-3w1.10 currently depends on bd-3un.18 (Tantivy query parsing). This is WRONG. FTS5 is an ALTERNATIVE to Tantivy, not dependent on it. The dependency should be on the LexicalIndex TRAIT (which should be defined in frankensearch-core, not in the Tantivy crate).\n\n   However, looking at the bead structure: bd-3un.18 is \"Implement Tantivy query parsing and search execution\" which includes defining the LexicalIndex trait. The trait should probably be in frankensearch-core (bd-3un.5 or a separate types bead), but as currently structured, the trait is defined alongside Tantivy. This is an architectural smell but not blocking — the FTS5 adapter needs the trait definition, which happens to live in the lexical crate.\n\n   BETTER APPROACH: Extract the LexicalIndex trait to frankensearch-core so both Tantivy and FTS5 can implement it without depending on each other. But this is a refactoring concern for bd-3un.18, not this bead.\n\n5. FTS5 TOKENIZER: FrankenSQLite FTS5 supports unicode61, ascii, porter, and trigram tokenizers. For frankensearch:\n   - Default: unicode61 with remove_diacritics=2 (best for multilingual)\n   - Porter: add for English stemming queries\n   - Trigram: add for substring matching (slower but more flexible)\n\n   Note: There is NO custom tokenizer API equivalent to Tantivy's RemoveLongFilter(256). FTS5 tokenizers are built-in only.\n","created_at":"2026-02-13T20:58:14Z"},{"id":241,"issue_id":"bd-3w1.10","author":"Dicklesworthstone","text":"DEPENDENCY FIX: Removed bd-3w1.10 -> bd-3un.18\n\nFTS5 is an ALTERNATIVE to Tantivy, not a consumer. Having FTS5 depend on\nTantivy query parsing (bd-3un.18) is architecturally wrong — it creates\na false dependency between competing implementations.\n\nFTS5 now depends on:\n- bd-3un.5 (core result types, including LexicalHit) — already present\n- bd-3w1.1 (storage crate scaffold) — already present\n- bd-3w1.2 (document metadata schema) — already present\n\nBoth Tantivy (bd-3un.17/18) and FTS5 (bd-3w1.10) implement the\nLexicalIndex trait from frankensearch-core. Neither depends on the other.\n\nPer bd-3un.18 comment: the LexicalIndex trait should be extracted from\nthe Tantivy crate to frankensearch-core during bd-3un.5 implementation,\nso both backends can implement it without cross-dependencies.\n","created_at":"2026-02-13T21:50:39Z"},{"id":259,"issue_id":"bd-3w1.10","author":"Dicklesworthstone","text":"REVISION (review pass 7 - dependency correction):\n\nREMOVED bd-3un.18 (Tantivy query parsing) as a blocking dependency. FTS5 is an ALTERNATIVE to Tantivy, not dependent on it. The LexicalIndex trait is defined in frankensearch-core (per bd-3un.1 revision), so FTS5 depends only on the trait, not on the Tantivy implementation.\n\nThe FTS5 adapter implements the same LexicalIndex trait as the Tantivy adapter, but neither should depend on the other. This enables consumers to choose one or both without unnecessary coupling.\n","created_at":"2026-02-13T21:55:10Z"}]}
{"id":"bd-3w1.11","title":"Implement index metadata persistence in FrankenSQLite","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:29.786720687Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:10.198216774Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","metadata","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.11","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:29.786720687Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.11","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.954346477Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.11","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:39.837876315Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":64,"issue_id":"bd-3w1.11","author":"Dicklesworthstone","text":"TASK: Implement index metadata persistence in FrankenSQLite.\n\nStore all index-related metadata in FrankenSQLite tables so the system can track what's been indexed, with which models, and when. This replaces the sentinel file approach from bd-3un.41 with a proper relational schema.\n\nSQL SCHEMA:\n\n  CREATE TABLE IF NOT EXISTS index_metadata (\n      index_name TEXT PRIMARY KEY,          -- e.g., \"vector.fast\", \"vector.quality\", \"lexical\"\n      index_type TEXT NOT NULL,             -- \"fsvi\" | \"tantivy\" | \"fts5\" | \"hnsw\"\n      embedder_id TEXT,                     -- For vector indices: which model\n      embedder_revision TEXT,               -- Model commit SHA\n      dimension INTEGER,                    -- For vector indices: embedding dimension\n      record_count INTEGER NOT NULL DEFAULT 0,\n      file_path TEXT,                       -- Absolute path to index file\n      file_size_bytes INTEGER,\n      file_hash TEXT,                       -- xxh3_64 hex of index file (for staleness)\n      schema_version TEXT,                  -- e.g., \"tantivy-schema-v1-frankensearch\"\n      built_at INTEGER NOT NULL,            -- Unix timestamp millis\n      build_duration_ms INTEGER,            -- How long the build took\n      source_doc_count INTEGER,             -- How many source documents existed at build time\n      config_json TEXT,                     -- Serialized build config for reproducibility\n      fec_path TEXT,                        -- Path to .fec sidecar (if durability enabled)\n      fec_size_bytes INTEGER,\n      last_verified_at INTEGER,             -- When durability was last verified\n      last_repair_at INTEGER,               -- When last repair occurred (null if never)\n      repair_count INTEGER DEFAULT 0        -- Total repairs performed on this index\n  );\n\n  CREATE TABLE IF NOT EXISTS index_build_history (\n      build_id INTEGER PRIMARY KEY AUTOINCREMENT,\n      index_name TEXT NOT NULL REFERENCES index_metadata(index_name),\n      built_at INTEGER NOT NULL,\n      build_duration_ms INTEGER,\n      record_count INTEGER,\n      source_doc_count INTEGER,\n      trigger TEXT,                          -- \"initial\" | \"content_change\" | \"model_update\" | \"corruption_repair\" | \"schema_migration\" | \"manual\"\n      config_json TEXT,\n      notes TEXT\n  );\n\nAPI:\n\n  impl Storage {\n      /// Record that an index was built/rebuilt\n      pub fn record_index_build(&self, meta: &IndexBuildRecord) -> SearchResult<()>;\n\n      /// Get current metadata for an index\n      pub fn get_index_metadata(&self, index_name: &str) -> SearchResult<Option<IndexMetadata>>;\n\n      /// Check if an index needs rebuilding (content changed since last build)\n      pub fn check_index_staleness(&self, index_name: &str) -> SearchResult<StalenessCheck>;\n\n      /// Record a durability verification event\n      pub fn record_verification(&self, index_name: &str, result: &VerifyResult) -> SearchResult<()>;\n\n      /// Record a repair event\n      pub fn record_repair(&self, index_name: &str, result: &RepairResult) -> SearchResult<()>;\n\n      /// Get build history for an index\n      pub fn get_build_history(&self, index_name: &str, limit: usize) -> SearchResult<Vec<IndexBuildRecord>>;\n  }\n\n  pub struct StalenessCheck {\n      pub is_stale: bool,\n      pub reason: Option<StalenessReason>,\n      pub docs_since_build: usize,          -- Documents added/changed since last build\n      pub index_age: Duration,\n      pub source_doc_count: usize,          -- Current document count\n      pub index_record_count: usize,        -- Records in the index\n  }\n\n  pub enum StalenessReason {\n      NewDocuments { count: usize },\n      ContentChanged { count: usize },\n      ModelUpdated { old_rev: String, new_rev: String },\n      SchemaChanged { old: String, new: String },\n      IndexMissing,\n      IndexCorrupted,\n  }\n\nSTALENESS DETECTION QUERY:\n  -- Count documents added/changed since the index was built\n  SELECT COUNT(*) FROM documents d\n  WHERE d.updated_at > (SELECT built_at FROM index_metadata WHERE index_name = ?name)\n  OR NOT EXISTS (\n      SELECT 1 FROM embedding_status es\n      WHERE es.doc_id = d.doc_id\n      AND es.embedder_id = ?embedder_id\n      AND es.status = 'embedded'\n  );\n\n  This is O(1) with the partial index on embedding_status.\n\nINTEGRATION WITH bd-3un.41 (Staleness Detection):\n  This bead provides the STORAGE LAYER for staleness detection.\n  bd-3un.41 provides the CACHE AND POLICY layer that decides what to do about staleness.\n  The two work together:\n  - This bead: \"is the index stale?\" (database query)\n  - bd-3un.41: \"what should we do about it?\" (auto-rebuild, prompt, ignore)\n\nBUILD HISTORY VALUE:\n  The index_build_history table enables:\n  1. Debugging: \"when was this index last rebuilt and why?\"\n  2. Performance tracking: \"are builds getting slower?\"\n  3. Audit: \"how many times has this index been repaired?\"\n  4. Capacity planning: \"what's the growth rate of indexed documents?\"\n\nFile: frankensearch-storage/src/schema.rs (extension of bd-3w1.2 schema)\n","created_at":"2026-02-13T20:46:15Z"},{"id":123,"issue_id":"bd-3w1.11","author":"Dicklesworthstone","text":"REVISION (review pass - cross-codebase verification):\n\n1. STALENESS QUERY CORRECTNESS: The staleness detection SQL has a logic issue. The OR NOT EXISTS clause checks for missing embedding_status rows, but a document could be updated_at AFTER the build AND already have an embedding. The correct approach separates the two signals:\n   -- Signal 1: Documents modified since the index was built\n   SELECT COUNT(*) FROM documents d\n   WHERE d.updated_at > (SELECT built_at FROM index_metadata WHERE index_name = ?name);\n\n   -- Signal 2: Documents missing embeddings for this embedder\n   SELECT COUNT(*) FROM documents d\n   WHERE NOT EXISTS (\n       SELECT 1 FROM embedding_status es\n       WHERE es.doc_id = d.doc_id\n       AND es.embedder_id = ?embedder_id\n       AND es.status = 'embedded'\n   );\n\n   These are reported as separate StalenessReason variants (ContentChanged vs NewDocuments).\n\n2. INDEX_NAME CONVENTION: Define a canonical naming convention for index_name:\n   - \"vector.fast.potion-128M\" (tier.model for vector indices)\n   - \"vector.quality.minilm-l6-v2\"\n   - \"lexical.tantivy\" or \"lexical.fts5\"\n   This makes queries like \"all vector indices\" trivial: WHERE index_name LIKE 'vector.%'\n\n3. BUILD HISTORY RETENTION: The index_build_history table can grow unbounded. Add a cleanup policy: keep the last N builds per index_name (default: 100). Purge older records during record_index_build():\n   DELETE FROM index_build_history\n   WHERE index_name = ?name\n   AND build_id NOT IN (\n       SELECT build_id FROM index_build_history\n       WHERE index_name = ?name\n       ORDER BY built_at DESC LIMIT 100\n   );\n\n4. SQLITEVALUE BINDING: All queries in this module must use SqliteValue parameter binding (not string interpolation). The check_index_staleness() method requires careful construction of the parameter arrays since the SQL references both index_metadata and documents tables.\n\n5. FILE_HASH COMPUTATION TIMING: Computing xxh3_64 of a large index file (73MB) takes ~10ms. This should be done ONCE during record_index_build() and stored, not recomputed on every staleness check. The stored file_hash enables fast \"has the file changed on disk?\" checks without reading the FrankenSQLite database.\n","created_at":"2026-02-13T21:01:29Z"},{"id":136,"issue_id":"bd-3w1.11","author":"Dicklesworthstone","text":"REVISION: Index Metadata Persistence Details\n\n1. Schema Design:\n   Two core tables needed:\n   - `index_builds`: id (INTEGER PK), embedder_id (TEXT), embedder_revision (TEXT),\n     dimension (INTEGER), doc_count (INTEGER), built_at (TEXT ISO8601),\n     build_duration_ms (INTEGER), fsvi_path (TEXT), tantivy_path (TEXT),\n     status (TEXT: 'building'|'ready'|'stale'|'corrupt')\n   - `index_config`: key (TEXT PK), value (TEXT) — for TwoTierConfig snapshot\n\n   Indexes: CREATE INDEX idx_builds_embedder ON index_builds(embedder_id);\n   This schema mirrors the FSVI header fields to detect drift without reading the binary file.\n\n2. Staleness Detection Integration:\n   The staleness detector (bd-3un.41) needs two timestamps:\n   - last_build_at: from index_builds.built_at\n   - last_doc_change_at: from document metadata (bd-3w1.2)\n   Storage-backed staleness (bd-3w1.12) queries both and computes delta.\n   KL divergence for embedding drift requires storing per-build embedding stats\n   (mean vector norm, variance) as BLOB columns in index_builds.\n\n3. Consistency with FSVI Header:\n   On load, verify index_builds metadata matches FSVI header fields:\n   - dimension, doc_count, embedder_revision\n   If mismatch detected: log WARN and update the DB record from the FSVI header\n   (the binary file is the source of truth, DB is advisory cache).\n\n4. Concurrent Access:\n   FrankenSQLite's page-level MVCC handles concurrent readers + single writer.\n   The RefreshWorker (bd-3un.28) is the only writer to index_builds.\n   Multiple TwoTierSearcher instances can read concurrently without coordination.\n\n5. Migration Strategy:\n   Use a `schema_version` row in index_config table.\n   On open, check version and apply forward-only migrations.\n   V1 = initial schema. Future versions add columns (never remove).\n","created_at":"2026-02-13T21:04:58Z"},{"id":277,"issue_id":"bd-3w1.11","author":"Dicklesworthstone","text":"REVISION (review pass 7 - schema consolidation):\n\nTWO CONFLICTING SCHEMA DESIGNS exist in prior comments. CANONICAL RESOLUTION:\n\nUse the ORIGINAL body's schema (index_metadata + index_build_history tables). The second revision comment (index_builds + index_config) was a parallel draft that diverges in table names, column names, and design choices. The original is more complete and already referenced by bd-3w1.12 (staleness detector) and bd-3w1.15 (unit tests).\n\nRECONCILIATION:\n- Table names: index_metadata (NOT index_builds), index_build_history (NOT index_config)\n- Column for model: embedder_id + embedder_revision (from original, more granular)\n- Status field: OMIT — status is derived, not stored. An index is \"stale\" if documents.updated_at > built_at. An index is \"corrupt\" if verify() fails. Storing mutable status creates cache invalidation problems.\n- Config storage: Add config_json TEXT column to index_metadata (from original) rather than a separate key-value table.\n- Embedding stats: Add mean_norm REAL and variance REAL columns to index_build_history for KL drift detection (from second revision's insight).\n\nIGNORE the second revision's schema wherever it conflicts with the original body's schema. The original body schema is the source of truth.\n","created_at":"2026-02-13T21:59:10Z"}]}
{"id":"bd-3w1.12","title":"Implement storage-backed staleness detector integration","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:37:30.614377855Z","created_by":"ubuntu","updated_at":"2026-02-13T21:04:59.010303487Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","staleness","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.12","depends_on_id":"bd-3un.41","type":"blocks","created_at":"2026-02-13T20:42:29.597868661Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.12","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:30.614377855Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.12","depends_on_id":"bd-3w1.11","type":"blocks","created_at":"2026-02-13T20:42:29.478253046Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":65,"issue_id":"bd-3w1.12","author":"Dicklesworthstone","text":"TASK: Implement storage-backed staleness detector integration.\n\nThis bridges the FrankenSQLite storage layer (bd-3w1.11) with the IndexCache staleness detection system (bd-3un.41). Instead of relying solely on file timestamps, the staleness detector queries the document metadata database for precise change detection.\n\nINTEGRATION API:\n\n  pub struct StorageBackedStaleness {\n      storage: Arc<Storage>,\n      config: StalenessConfig,\n  }\n\n  pub struct StalenessConfig {\n      /// Minimum number of new/changed documents before triggering rebuild\n      pub min_change_threshold: usize,       // Default: 10\n      /// Maximum age of index before forced rebuild (even if no changes detected)\n      pub max_index_age_secs: Option<u64>,   // Default: None (no age limit)\n      /// Check model revision changes (embedder update)\n      pub check_model_revision: bool,        // Default: true\n      /// Check schema version changes\n      pub check_schema_version: bool,        // Default: true\n  }\n\n  impl StorageBackedStaleness {\n      /// Comprehensive staleness check combining all signals\n      pub fn check(&self, index_name: &str, current_embedder_rev: Option<&str>, current_schema: Option<&str>) -> SearchResult<StalenessReport>;\n\n      /// Quick check: just count pending embeddings (fast, no full scan)\n      pub fn quick_check(&self, embedder_id: &str) -> SearchResult<QuickStalenessCheck>;\n  }\n\n  pub struct StalenessReport {\n      pub is_stale: bool,\n      pub reasons: Vec<StalenessReason>,\n      pub confidence: f32,                   // 0.0 to 1.0\n      pub recommended_action: RecommendedAction,\n      pub stats: StalenessStats,\n  }\n\n  pub struct StalenessStats {\n      pub total_documents: usize,\n      pub indexed_documents: usize,\n      pub pending_documents: usize,\n      pub failed_documents: usize,\n      pub docs_changed_since_build: usize,\n      pub index_age: Duration,\n      pub last_build_duration: Option<Duration>,\n  }\n\n  pub enum RecommendedAction {\n      NoAction,                              // Index is fresh\n      IncrementalUpdate { doc_count: usize }, // Only embed new/changed docs\n      FullRebuild { reason: String },         // Schema change, model update, etc.\n  }\n\nQUICK CHECK SQL (O(1)):\n  SELECT COUNT(*) as pending FROM embedding_status\n  WHERE embedder_id = ?embedder AND status = 'pending';\n\n  This uses the partial index from bd-3w1.2 and is effectively instant.\n\nINTEGRATION WITH IndexCache (bd-3un.41):\n  The IndexCache holds OnceLock<Option<VectorIndex>> for each tier.\n  The staleness detector is called:\n  1. On cache initialization (first access)\n  2. Periodically (configurable interval)\n  3. After document ingestion (new content triggers check)\n\n  When staleness is detected:\n  - IncrementalUpdate: queue the pending documents for embedding, update vector index in-place\n  - FullRebuild: clear the cache, rebuild all indices from the document store\n\n  The document store (FrankenSQLite) is the SOURCE OF TRUTH. Vector indices and Tantivy\n  indices are derived/cached artifacts that can always be rebuilt from the source.\n\nCONFIDENCE SCORING:\n  Confidence reflects how certain we are that rebuilding would improve results:\n  - 0.0: Index is perfectly fresh (no changes)\n  - 0.3: A few documents changed (< min_threshold)\n  - 0.7: Significant content change (> 10% of documents)\n  - 0.9: Model revision changed (embeddings are from wrong model version)\n  - 1.0: Schema changed or index missing (rebuild is mandatory)\n\nFile: frankensearch-storage/src/staleness.rs\n","created_at":"2026-02-13T20:46:16Z"},{"id":124,"issue_id":"bd-3w1.12","author":"Dicklesworthstone","text":"REVISION (review pass - integration verification):\n\n1. QUICK CHECK IS ESSENTIAL: The quick_check() method (COUNT of pending embedding_status rows) should be the DEFAULT check used by the RefreshWorker (bd-3un.28). The full check() method (which examines model revision, schema version, index age, etc.) should only run periodically or on explicit user request, since it touches multiple tables.\n\n2. CONFIDENCE SCORING SIMPLIFICATION: The confidence field (0.0-1.0) conflates two concepts: certainty that staleness exists, and severity of staleness. Consider splitting into:\n   - is_stale: bool (binary: are there changes that warrant action?)\n   - severity: StalenessLevel::None | Minor | Significant | Critical\n   Where: Minor = <10 new docs, Significant = >10% changed, Critical = model/schema change or missing index.\n   This is clearer than a float that means different things at different thresholds.\n\n3. RECOMMENDED ACTION SHOULD CONSIDER COST: IncrementalUpdate is cheap (embed N docs), FullRebuild is expensive (embed ALL docs). The threshold between them should account for:\n   - Number of changed docs vs total docs\n   - Embedder latency (potion-128M is ~0.57ms/doc, MiniLM is ~128ms/doc)\n   - For quality tier with 10K docs: FullRebuild takes ~21 minutes (expensive!)\n   Suggestion: IncrementalUpdate when changed_docs < 30% of total, FullRebuild otherwise.\n\n4. STORAGE-BACKED vs FILE-BASED STALENESS: When the 'storage' feature is disabled, staleness detection falls back to file timestamps (bd-3un.41). This bead should NOT override bd-3un.41 but rather IMPLEMENT the same trait. Define:\n   pub trait StalenessDetector: Send + Sync {\n       fn check(&self, index_name: &str) -> SearchResult<StalenessReport>;\n       fn quick_check(&self) -> SearchResult<bool>;\n   }\n   #[cfg(feature = \"storage\")]\n   impl StalenessDetector for StorageBackedStaleness { ... }\n   #[cfg(not(feature = \"storage\"))]\n   impl StalenessDetector for FileBasedStaleness { ... }\n\n5. DOCUMENT STORE AS SOURCE OF TRUTH: The statement \"The document store (FrankenSQLite) is the SOURCE OF TRUTH\" is correct and critical. When storage feature is enabled, the vector index and Tantivy index are DERIVED caches. They can always be rebuilt from the document store. This means staleness detection's worst-case action (FullRebuild) is always viable.\n","created_at":"2026-02-13T21:01:30Z"},{"id":137,"issue_id":"bd-3w1.12","author":"Dicklesworthstone","text":"REVISION: Storage-Backed Staleness Detector Details\n\n1. Query Pattern:\n   The detector runs a single SQL query to compute staleness:\n   SELECT\n     ib.built_at AS last_build,\n     MAX(dm.updated_at) AS last_change,\n     ib.doc_count AS indexed_count,\n     COUNT(dm.id) AS current_count\n   FROM index_builds ib\n   CROSS JOIN documents dm\n   WHERE ib.status = 'ready'\n   GROUP BY ib.id\n   ORDER BY ib.built_at DESC LIMIT 1;\n\n   Stale if: last_change > last_build OR current_count != indexed_count.\n\n2. Integration with IndexCache (bd-3un.41):\n   The in-memory staleness detector uses file modification times.\n   The storage-backed version replaces this with DB timestamps.\n   Both implement the same StalenessDetector trait, selected by feature flag.\n   The storage-backed version is more reliable (no filesystem clock skew issues).\n\n3. Embedding Drift Detection:\n   Store per-build statistics in index_builds:\n   - mean_norm REAL (mean L2 norm of embeddings)\n   - norm_variance REAL (variance of L2 norms)\n   Compare current build stats vs previous: if KL divergence > threshold,\n   flag as \"drift detected\" even if documents haven't changed.\n   This catches model updates that change embedding distributions.\n\n4. Polling vs Push:\n   Default: poll every 30 seconds (configurable via TwoTierConfig).\n   The RefreshWorker (bd-3un.28) calls check_staleness() on each iteration.\n   No separate staleness polling thread — piggyback on the refresh cycle.\n   If staleness detected: set index_builds.status = 'stale', trigger rebuild.\n\n5. Metrics:\n   Emit tracing events:\n   - staleness_check_duration_us: query time (should be <1ms)\n   - staleness_detected: bool\n   - doc_count_delta: current_count - indexed_count\n   - embedding_drift_kl: KL divergence value (if drift detection enabled)\n","created_at":"2026-02-13T21:04:59Z"}]}
{"id":"bd-3w1.13","title":"Wire FrankenSQLite storage into EmbeddingJobRunner pipeline","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:31.798119798Z","created_by":"ubuntu","updated_at":"2026-02-13T21:06:33.428447538Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","integration","pipeline","tier1"],"dependencies":[{"issue_id":"bd-3w1.13","depends_on_id":"bd-3un.27","type":"blocks","created_at":"2026-02-13T20:42:30.872447497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T20:46:45.175740433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:31.798119798Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:42:26.073091423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1.3","type":"blocks","created_at":"2026-02-13T20:42:26.195471543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1.4","type":"blocks","created_at":"2026-02-13T20:46:45.026328741Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":66,"issue_id":"bd-3w1.13","author":"Dicklesworthstone","text":"TASK: Wire FrankenSQLite storage into the EmbeddingJobRunner pipeline.\n\nThis is the integration bead that connects the persistent storage (bd-3w1.1-4) with the embedding pipeline (bd-3un.27, bd-3un.28). The EmbeddingJobRunner's workflow changes from in-memory queue to:\n\nUPDATED PIPELINE:\n\n  1. INGEST: Document arrives via public API\n     -> Canonicalize text (bd-3un.42)\n     -> Compute SHA-256 content hash (bd-3w1.4)\n     -> Upsert into documents table (bd-3w1.2)\n     -> Check dedup: if content unchanged, skip\n     -> If new/changed: enqueue embedding job (bd-3w1.3)\n\n  2. EMBED: EmbeddingJobRunner processes queue\n     -> Claim batch from persistent queue (bd-3w1.3)\n     -> Read canonical text from documents table\n     -> Embed via fast-tier embedder (bd-3un.7)\n     -> Write embedding to FSVI vector index (bd-3un.13)\n     -> Mark job as completed in queue\n     -> If quality-tier available: enqueue quality embedding job\n\n  3. REFRESH: IndexRefreshWorker (bd-3un.28) periodically\n     -> Check staleness via storage (bd-3w1.12)\n     -> If stale: trigger incremental or full rebuild\n     -> Protect new indices with RaptorQ (bd-3w1.7, bd-3w1.8)\n\nINTEGRATION CODE:\n\n  pub struct StorageBackedJobRunner {\n      storage: Arc<Storage>,\n      queue: Arc<PersistentJobQueue>,\n      canonicalizer: Arc<Canonicalizer>,     // From bd-3un.42\n      content_hasher: ContentHasher,         // From bd-3w1.4\n      fast_embedder: Arc<dyn Embedder>,\n      quality_embedder: Option<Arc<dyn Embedder>>,\n      vector_writer: Arc<Mutex<VectorIndexWriter>>,\n      protector: Option<Arc<FileProtector>>, // From bd-3w1.9, if durability enabled\n      metrics: Arc<PipelineMetrics>,\n  }\n\n  impl StorageBackedJobRunner {\n      /// Ingest a document into the full pipeline\n      pub fn ingest(&self, doc_id: &str, text: &str, metadata: Option<serde_json::Value>) -> SearchResult<IngestResult>;\n\n      /// Ingest a batch of documents\n      pub fn ingest_batch(&self, docs: &[IngestRequest]) -> SearchResult<BatchIngestResult>;\n\n      /// Process one batch of embedding jobs from the persistent queue\n      pub fn process_batch(&self) -> SearchResult<BatchProcessResult>;\n\n      /// Run the embedding worker loop (blocks, processes batches continuously)\n      pub fn run_worker(&self, shutdown: Arc<AtomicBool>) -> SearchResult<WorkerReport>;\n  }\n\n  pub struct IngestResult {\n      pub doc_id: String,\n      pub action: IngestAction,\n  }\n\n  pub enum IngestAction {\n      New,                                    // New document, jobs enqueued\n      Updated,                                // Content changed, re-embedding queued\n      Unchanged,                              // Content hash matched, skipped\n      Skipped { reason: String },             // Low-signal content after canonicalization\n  }\n\n  pub struct BatchProcessResult {\n      pub jobs_claimed: usize,\n      pub jobs_completed: usize,\n      pub jobs_failed: usize,\n      pub jobs_skipped: usize,\n      pub embed_time: Duration,\n      pub total_time: Duration,\n  }\n\nHASH-ONLY SKIP (from agent-mail embedding_jobs.rs line 664):\n  When the embedder is the hash embedder (id starts with \"fnv1a-\"), skip writing to\n  the vector index. Hash embeddings are computed on-the-fly during search because:\n  1. They're instant to compute (~0.01ms)\n  2. They change if the hash function changes (no value in storing)\n  3. They save disk space and I/O\n\n  if embedder.id().starts_with(\"fnv1a-\") {\n      queue.skip(job_id, \"hash embeddings computed on-the-fly\")?;\n      continue;\n  }\n\nTWO-TIER EMBEDDING FLOW:\n  For each document, TWO embedding jobs are enqueued:\n  1. Fast tier (potion-128M, priority=1): processed immediately for instant results\n  2. Quality tier (MiniLM-L6-v2, priority=0): processed in background for refinement\n\n  The priority field ensures fast-tier jobs are claimed first, so the search system\n  has fast results available as quickly as possible.\n\nCRASH RECOVERY:\n  On startup, the runner calls queue.reclaim_stale_jobs() to reset any jobs that were\n  being processed when the previous process died. These get requeued automatically.\n  No work is lost.\n\nTRANSACTIONAL CONSISTENCY (MVCC):\n  FrankenSQLite's MVCC ensures:\n  - ingest_batch() runs in a single transaction (all-or-nothing)\n  - process_batch() can run concurrently with ingest() without blocking\n  - Multiple workers can call claim_batch() concurrently (disjoint batches guaranteed)\n  - search queries see a consistent snapshot even during bulk ingestion\n\nFile: frankensearch-storage/src/pipeline.rs (or frankensearch-fusion/src/storage_pipeline.rs)\n","created_at":"2026-02-13T20:46:16Z"},{"id":125,"issue_id":"bd-3w1.13","author":"Dicklesworthstone","text":"REVISION (review pass - pipeline integration verification):\n\n1. HASH EMBEDDER SKIP LOGIC: The comment about skipping hash embedder jobs is correct but should be handled at ENQUEUE time, not process time. When the embedder_id starts with \"fnv1a-\", don't enqueue a job at all. The hash embedder computes on-the-fly during search, so there's no value in even creating a job record. This saves database writes and queue depth.\n\n2. INGEST vs EMBED THREAD SEPARATION: The ingest() method and process_batch() method should be callable from DIFFERENT threads. Ingest runs on the request-handling thread; process_batch() runs on background worker threads. FrankenSQLite's MVCC ensures they don't block each other, but the struct fields must be thread-safe:\n   - storage: Arc<Storage> (shared across threads)\n   - queue: Arc<PersistentJobQueue> (same Storage underneath, or separate connection)\n   - vector_writer: Must use Arc<Mutex<VectorIndexWriter>> (FSVI writes are NOT concurrent)\n\n3. BATCH SIZE TUNING: The process_batch() method claims a batch from the queue. The batch size should be configurable and match the embedder's optimal batch size:\n   - potion-128M (Model2Vec): batch_size=64 (small model, fast inference)\n   - MiniLM-L6-v2 (FastEmbed/ONNX): batch_size=32 (larger model, GPU batching helps)\n   These should come from TwoTierConfig, not be hardcoded.\n\n4. ERROR HANDLING IN PIPELINE: If embedding fails for a single document, it should NOT abort the entire batch. Process each document independently:\n   for job in claimed_jobs {\n       match self.embed_one(&job) {\n           Ok(_) => self.queue.complete(job.job_id)?,\n           Err(e) => {\n               tracing::warn!(doc_id = %job.doc_id, error = %e, \"embedding failed\");\n               self.queue.fail(job.job_id, &e.to_string())?;\n           }\n       }\n   }\n\n5. FILE LOCATION: The comment suggests \"frankensearch-storage/src/pipeline.rs (or frankensearch-fusion/src/storage_pipeline.rs)\". It should be in frankensearch-storage because it orchestrates storage components (queue, dedup, document table) and only touches the embedder through a trait. The fusion crate handles score combination, not data pipeline orchestration.\n\n6. PROTECTOR INTEGRATION: The optional FileProtector should be called AFTER vector index writes, not during. The sequence is:\n   a) Write new vectors to FSVI file\n   b) fsync the FSVI file\n   c) Re-protect the updated FSVI file (recompute .fec sidecar)\n   Step (c) is expensive for large files. Consider a threshold: only re-protect after accumulating N new vectors (e.g., 1000), not after every batch.\n","created_at":"2026-02-13T21:01:31Z"},{"id":138,"issue_id":"bd-3w1.13","author":"Dicklesworthstone","text":"REVISION: Pipeline Integration Details\n\n1. Integration Architecture:\n   The EmbeddingJobRunner (bd-3un.27) currently uses an in-memory queue.\n   This bead wires in FrankenSQLite-backed alternatives:\n   - PersistentJobQueue (bd-3w1.3) replaces the in-memory BTreeMap\n   - DocumentStore (bd-3w1.2) provides metadata persistence\n   - ContentHashDedup (bd-3w1.4) gates the queue to avoid re-embedding\n\n   The runner should accept a trait object for the queue, so both\n   in-memory and persistent backends can be used interchangeably.\n\n2. Transaction Boundaries:\n   Critical invariant: dedup check + queue insert must be atomic.\n   Wrap in a FrankenSQLite transaction:\n     conn.transaction(|tx| {\n       if !dedup.exists(tx, &content_hash)? { queue.enqueue(tx, job)?; }\n       Ok(())\n     })\n   This prevents race conditions where two concurrent indexing requests\n   could both pass the dedup check and double-embed the same document.\n\n3. Feature Gating:\n   The persistent pipeline requires feature = \"storage\".\n   Without it, the in-memory queue (bd-3un.27) is used as fallback.\n   The facade's auto() method detects available features and selects\n   the appropriate backend automatically.\n\n4. Backpressure:\n   When the persistent queue exceeds configurable threshold (default: 10K jobs),\n   new enqueue calls should return Err(SearchError::QueueFull) rather than\n   blocking. The caller (document indexing API) surfaces this to the user.\n   This prevents unbounded DB growth during bulk ingestion.\n\n5. Recovery on Restart:\n   On startup, scan the persistent queue for jobs with status = 'processing'\n   that have exceeded the visibility timeout. Reset them to 'pending'.\n   This handles the case where a previous process crashed mid-embedding.\n   Log each recovered job at WARN level with the original enqueue timestamp.\n","created_at":"2026-02-13T21:04:59Z"},{"id":160,"issue_id":"bd-3w1.13","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — LIGHT REVISION (pipeline integration):\n\nbd-3w1.13 wires FrankenSQLite storage into EmbeddingJobRunner. The asupersync migration replaces Arc<Mutex<VectorIndexWriter>> with asupersync::sync::Mutex:\n\nBEFORE:\n  - vector_writer: Arc<std::sync::Mutex<VectorIndexWriter>>\n  - References crossbeam channels\n\nAFTER:\n  - vector_writer: Arc<asupersync::sync::Mutex<VectorIndexWriter>>\n  - Pipeline orchestration via asupersync region for structured lifecycle\n\n  pub async fn run_pipeline(cx: &Cx, config: PipelineConfig) -> asupersync::Outcome<(), SearchError> {\n      cx.region(|scope| async {\n          // Worker 1: Dequeue jobs from persistent queue\n          // Worker 2: Run embedding inference\n          // Worker 3: Write vectors to index\n          // All workers owned by region; clean shutdown on cancel\n          scope.spawn(|cx| dequeue_worker(cx, &queue, &embed_tx));\n          scope.spawn(|cx| embed_worker(cx, &embed_rx, &write_tx));\n          scope.spawn(|cx| write_worker(cx, &write_rx, &writer));\n      }).await\n  }","created_at":"2026-02-13T21:06:33Z"}]}
{"id":"bd-3w1.14","title":"Update Cargo feature flags for storage and durability features","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:35.858268398Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:11.161114354Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","features","frankensqlite","raptorq"],"dependencies":[{"issue_id":"bd-3w1.14","depends_on_id":"bd-3un.29","type":"blocks","created_at":"2026-02-13T20:42:31.231418826Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.14","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:35.858268398Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.14","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:30.989149348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.14","depends_on_id":"bd-3w1.5","type":"blocks","created_at":"2026-02-13T20:42:31.111700999Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":67,"issue_id":"bd-3w1.14","author":"Dicklesworthstone","text":"TASK: Update Cargo feature flags for storage and durability features.\n\nThis extends the feature flag system (bd-3un.29) with new features for the FrankenSQLite and RaptorQ integrations.\n\nUPDATED FEATURE MAP:\n\n  [features]\n  default = [\"hash\"]\n\n  # Existing features (unchanged)\n  hash = []\n  model2vec = [\"dep:safetensors\", \"dep:tokenizers\", \"dep:dirs\"]\n  fastembed = [\"dep:fastembed\"]\n  lexical = [\"dep:tantivy\"]\n  rerank = [\"dep:ort\", \"dep:tokenizers\"]\n  ann = [\"dep:hnsw_rs\"]\n  download = [\"dep:reqwest\"]\n\n  # NEW: FrankenSQLite storage features\n  storage = [\"dep:frankensearch-storage\"]     # Document store, job queue, content dedup\n  fts5 = [\"storage\"]                          # FTS5 lexical engine (requires storage)\n\n  # NEW: Durability features\n  durability = [\"dep:frankensearch-durability\"]  # RaptorQ self-healing indices\n\n  # Updated bundles\n  semantic = [\"hash\", \"model2vec\", \"fastembed\"]\n  hybrid = [\"semantic\", \"lexical\"]\n  persistent = [\"hybrid\", \"storage\"]                    # Hybrid search with persistent storage\n  durable = [\"persistent\", \"durability\"]                # Persistent + self-healing\n  full = [\"durable\", \"rerank\", \"ann\", \"download\"]       # Everything\n  full-fts5 = [\"full\", \"fts5\"]                          # Everything + FTS5 alternative\n\nWORKSPACE-LEVEL FEATURE FORWARDING:\n\n  In the facade crate (frankensearch/Cargo.toml):\n  [dependencies]\n  frankensearch-core = { path = \"../crates/frankensearch-core\" }\n  frankensearch-embed = { path = \"../crates/frankensearch-embed\" }\n  frankensearch-index = { path = \"../crates/frankensearch-index\" }\n  frankensearch-lexical = { path = \"../crates/frankensearch-lexical\", optional = true }\n  frankensearch-fusion = { path = \"../crates/frankensearch-fusion\" }\n  frankensearch-rerank = { path = \"../crates/frankensearch-rerank\", optional = true }\n  frankensearch-storage = { path = \"../crates/frankensearch-storage\", optional = true }      # NEW\n  frankensearch-durability = { path = \"../crates/frankensearch-durability\", optional = true } # NEW\n\nCONDITIONAL COMPILATION:\n\n  In frankensearch-storage/Cargo.toml:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite = { path = \"/dp/frankensqlite/fsqlite\" }\n  sha2 = \"0.10\"\n  tracing = \"0.1\"\n  serde = { version = \"1\", features = [\"derive\"] }\n  serde_json = \"1\"\n\n  In frankensearch-durability/Cargo.toml:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite-core = { path = \"/dp/frankensqlite/crates/fsqlite-core\" }\n  xxhash-rust = { version = \"0.8\", features = [\"xxh3\"] }\n  crc32fast = \"1.4\"\n  tracing = \"0.1\"\n  serde = { version = \"1\", features = [\"derive\"] }\n\nDESIGN RATIONALE:\n1. storage and durability are INDEPENDENT features: you can have storage without durability (no .fec files) or durability without storage (protect files without SQL metadata)\n2. fts5 REQUIRES storage because FTS5 runs inside FrankenSQLite\n3. The 'persistent' bundle is the recommended production configuration: hybrid search with crash-safe metadata\n4. The 'durable' bundle adds self-healing on top of persistent\n5. 'full' now includes durability by default (it's the kitchen-sink bundle)\n6. 'full-fts5' adds FTS5 for consumers who want both Tantivy AND FTS5\n\nCONSUMER USAGE:\n  # Minimal (testing): hash embedder only\n  frankensearch = { version = \"0.1\" }\n\n  # Production (typical): hybrid search with persistence\n  frankensearch = { version = \"0.1\", features = [\"persistent\"] }\n\n  # Production (maximum durability): self-healing indices\n  frankensearch = { version = \"0.1\", features = [\"durable\"] }\n\n  # Everything\n  frankensearch = { version = \"0.1\", features = [\"full\"] }\n\nFile: Updates to frankensearch/Cargo.toml and each sub-crate's Cargo.toml\n","created_at":"2026-02-13T20:46:17Z"},{"id":126,"issue_id":"bd-3w1.14","author":"Dicklesworthstone","text":"REVISION (review pass - feature flag architecture):\n\n1. STORAGE AND DURABILITY INDEPENDENCE: The design correctly makes storage and durability independent features. However, durability WITHOUT storage means protecting raw index files but having no metadata about repairs in a database. This is fine -- the repair log (JSONL from bd-3w1.9) provides non-database audit trail. Confirm this is the intended design.\n\n2. FTS5 REQUIRES STORAGE IS CORRECT: FTS5 runs inside FrankenSQLite, so it inherently requires the storage feature. This dependency chain is sound.\n\n3. FSQLITE PATH DEPENDENCY: The Cargo.toml shows:\n   fsqlite = { path = \"/dp/frankensqlite/fsqlite\" }\n   This is an ABSOLUTE path dependency, which works for development but won't work for published crates. Since frankensearch is an internal crate (not crates.io), absolute paths are acceptable. Add a comment in Cargo.toml explaining this:\n   # Internal dependency: FrankenSQLite is a sibling project in /dp/\n   # For external distribution, this would need to be a git dependency or workspace member\n\n4. FEATURE BUNDLE NAMING: The bundles \"persistent\" and \"durable\" have good, intuitive names. \"full-fts5\" is slightly awkward. Consider \"full-alt-lexical\" or just keep \"full-fts5\" since it's self-explanatory.\n\n5. CONSUMER USAGE EXAMPLES: The consumer usage section is excellent. Add one more example for the agent-mail use case (the original consumer):\n   # For mcp_agent_mail_rust (the original consumer)\n   frankensearch = { version = \"0.1\", features = [\"persistent\", \"rerank\"] }\n   This shows real-world usage, not just abstract bundles.\n\n6. COMPILE-TIME FEATURE VERIFICATION: Add a #[cfg] check that prevents nonsensical combinations:\n   #[cfg(all(feature = \"fts5\", not(feature = \"storage\")))]\n   compile_error!(\"fts5 feature requires storage feature\");\n   This is redundant with the Cargo feature dependency chain but provides a clear error message if someone manually specifies features incorrectly.\n","created_at":"2026-02-13T21:01:32Z"},{"id":139,"issue_id":"bd-3w1.14","author":"Dicklesworthstone","text":"REVISION: Feature Flag Design Details\n\n1. New Feature Flags:\n   storage = ['dep:frankensqlite']         # FrankenSQLite document store\n   durability = ['dep:frankensqlite']      # RaptorQ self-healing indices\n   fts5 = ['storage', 'dep:frankensqlite'] # FTS5 alternative to Tantivy\n   persistent = ['storage']                # Persistent job queue\n   durable = ['durability']                # Self-healing FSVI + Tantivy\n\n   Bundle flags:\n   full = ['hybrid', 'rerank', 'ann', 'download', 'persistent', 'durable']\n   storage-full = ['storage', 'persistent', 'fts5']\n\n2. Conditional Compilation Strategy:\n   Each new crate uses #[cfg(feature = \"storage\")] at the re-export level\n   in the facade crate. Internal crate code does NOT use cfg — the crate\n   either compiles or doesn't based on whether it's included as a dependency.\n\n   In Cargo.toml workspace:\n   frankensearch-storage = { path = \"crates/frankensearch-storage\", optional = true }\n   frankensearch-durability = { path = \"crates/frankensearch-durability\", optional = true }\n\n3. Interaction with Existing Flags:\n   - `persistent` implies `storage` (can't have persistent queue without the DB)\n   - `fts5` implies `storage` (FTS5 lives inside FrankenSQLite)\n   - `durable` implies `durability` (repair symbols need the codec)\n   - `fts5` and `lexical` are alternatives, not additive — if both enabled,\n     the facade should expose both and let the user choose via TwoTierConfig\n\n4. Default Feature Set:\n   The default = ['hash'] stays unchanged. Storage features are opt-in.\n   This keeps the base dependency footprint minimal for users who only\n   need in-memory vector search.\n\n5. CI Matrix:\n   Test matrix must cover:\n   - default (hash only)\n   - semantic (all embedders)\n   - hybrid (semantic + lexical)\n   - full (everything)\n   - storage-full (all storage features)\n   Each combination must compile and pass tests independently.\n","created_at":"2026-02-13T21:05:00Z"},{"id":278,"issue_id":"bd-3w1.14","author":"Dicklesworthstone","text":"REVISION (review pass 7 - stale reqwest + consolidation):\n\n1. STALE REFERENCE: The original body shows `download = [\"dep:reqwest\"]`. This is WRONG — asupersync migration mandates `download = ['asupersync/tls']` (NO reqwest). This was already fixed in bd-3un.29 but the copy in this bead's body was not updated.\n\n2. SCHEMA CONSOLIDATION: Two revision comments overlap. CANONICAL feature map:\n\n  [features]\n  default = [\"hash\"]\n  hash = []\n  model2vec = [\"dep:safetensors\", \"dep:tokenizers\", \"dep:dirs\"]\n  fastembed = [\"dep:fastembed\"]\n  lexical = [\"dep:tantivy\"]\n  rerank = [\"dep:ort\", \"dep:tokenizers\"]\n  ann = [\"dep:hnsw_rs\"]\n  download = [\"asupersync/tls\"]                          # NOT dep:reqwest\n  storage = [\"dep:frankensearch-storage\"]\n  durability = [\"dep:frankensearch-durability\"]\n  fts5 = [\"storage\"]\n  semantic = [\"hash\", \"model2vec\", \"fastembed\"]\n  hybrid = [\"semantic\", \"lexical\"]\n  persistent = [\"hybrid\", \"storage\"]\n  durable = [\"persistent\", \"durability\"]\n  full = [\"durable\", \"rerank\", \"ann\", \"download\"]\n  full-fts5 = [\"full\", \"fts5\"]\n\n3. SECOND REVISION'S unique contributions to retain:\n  - Conditional compilation strategy (facade-level #[cfg], not crate-internal)\n  - CI matrix coverage (default/semantic/hybrid/full/storage-full)\n  - fts5+lexical coexistence note (both exposed, user chooses via TwoTierConfig)\n\nAll other overlapping content: defer to THIS consolidated feature map.\n","created_at":"2026-02-13T21:59:11Z"}]}
{"id":"bd-3w1.15","title":"Write unit tests for FrankenSQLite storage layer","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:37.023828606Z","created_by":"ubuntu","updated_at":"2026-02-13T21:54:45.045104001Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","testing","unit-tests"],"dependencies":[{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:37.023828606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.11","type":"blocks","created_at":"2026-02-13T20:46:48.378006324Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:42:32.564674392Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.3","type":"blocks","created_at":"2026-02-13T20:46:48.234412533Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.4","type":"blocks","created_at":"2026-02-13T20:42:32.687086602Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":68,"issue_id":"bd-3w1.15","author":"Dicklesworthstone","text":"TASK: Write unit tests for the FrankenSQLite storage layer.\n\nComprehensive test suite covering all storage operations with detailed tracing.\n\nTEST MODULES:\n\n1. schema_tests:\n   - test_create_tables: Verify schema creation on fresh database\n   - test_schema_migration: Verify schema migration when version changes\n   - test_concurrent_schema_init: Multiple threads calling open() simultaneously\n   - test_in_memory_database: Storage::open_in_memory() works correctly\n   - test_wal_mode_enabled: Verify WAL mode is active after open\n\n2. document_tests:\n   - test_insert_document: Basic insert and retrieve\n   - test_upsert_unchanged: Same content_hash returns false (no-op)\n   - test_upsert_changed: Different content_hash updates and resets embedding status\n   - test_delete_document: Cascades to embedding_status\n   - test_batch_upsert: Atomic batch of 100 documents\n   - test_batch_upsert_rollback: If one doc in batch fails, none persist\n   - test_get_nonexistent: Returns None, not error\n   - test_content_preview: First 400 chars stored correctly\n   - test_metadata_json: Arbitrary JSON roundtrips correctly\n   - test_unicode_doc_id: UTF-8 doc IDs work correctly\n\n3. embedding_status_tests:\n   - test_mark_embedded: Status transitions to 'embedded'\n   - test_mark_failed: Records error message, increments retry_count\n   - test_list_pending: Only returns pending items for specified embedder\n   - test_multi_tier_status: Same doc can have different status for fast vs quality\n   - test_count_by_status: Correct counts for each status\n   - test_embedding_status_cascade: Deleting doc removes all embedding status\n\n4. job_queue_tests:\n   - test_enqueue_and_claim: Basic enqueue, claim, complete cycle\n   - test_dedup_same_hash: Second enqueue with same hash is skipped\n   - test_dedup_different_hash: Second enqueue with different hash replaces\n   - test_claim_batch_disjoint: Two workers get disjoint batches (CRITICAL)\n   - test_visibility_timeout: Stale jobs reclaimed after timeout\n   - test_retry_on_failure: Failed job requeued up to max_retries\n   - test_max_retries_exceeded: Job stays 'failed' after max retries\n   - test_priority_ordering: Higher priority jobs claimed first\n   - test_fifo_within_priority: Same priority jobs claimed in submission order\n   - test_backpressure: is_backpressured() returns true above threshold\n   - test_queue_depth: Correct counts by status\n   - test_concurrent_claim: 4 threads claiming simultaneously (no double-claim)\n   - test_hash_only_skip: fnv1a embedder jobs skipped\n   - test_metrics_tracking: All atomic counters increment correctly\n\n5. content_hash_tests:\n   - test_hash_deterministic: Same text always produces same hash\n   - test_hash_differs: Different text produces different hash\n   - test_dedup_new: New doc_id returns DeduplicationDecision::New\n   - test_dedup_unchanged: Same hash returns Skip\n   - test_dedup_changed: Different hash returns Changed\n   - test_batch_dedup: Batch check returns correct decisions for mixed input\n\n6. index_metadata_tests:\n   - test_record_build: Build metadata persisted correctly\n   - test_staleness_no_changes: Fresh index not stale\n   - test_staleness_new_documents: New docs trigger stale\n   - test_staleness_model_change: Changed embedder_revision triggers stale\n   - test_build_history: Multiple builds recorded chronologically\n   - test_verification_recording: Durability verification events logged\n   - test_repair_recording: Repair events logged with details\n\nLOGGING IN TESTS:\n  All tests use tracing-test to capture and verify log output:\n  #[test]\n  fn test_upsert_changed() {\n      let (storage, _guard) = test_storage_with_tracing();\n      // ... test logic ...\n      // Verify specific log lines were emitted\n      assert!(logs_contain(\"document content changed, resetting embedding status\"));\n  }\n\nSHARED TEST FIXTURES:\n  fn test_storage_with_tracing() -> (Storage, tracing::subscriber::DefaultGuard) {\n      let subscriber = tracing_subscriber::fmt().with_test_writer().finish();\n      let guard = tracing::subscriber::set_default(subscriber);\n      let storage = Storage::open_in_memory().unwrap();\n      (storage, guard)\n  }\n\n  fn sample_document(doc_id: &str) -> DocumentRecord {\n      DocumentRecord {\n          doc_id: doc_id.to_string(),\n          source_path: Some(\"/test/path\".into()),\n          content_preview: \"This is test content for unit testing...\".into(),\n          content_hash: ContentHasher::hash(\"This is test content for unit testing...\"),\n          content_length: 42,\n          created_at: 1707840000000,\n          updated_at: 1707840000000,\n          metadata: None,\n      }\n  }\n\nMVCC CONCURRENCY TESTS:\n  - test_concurrent_read_write: Writer inserts while reader queries (no blocking)\n  - test_concurrent_writers: Two writers insert different docs simultaneously\n  - test_snapshot_isolation: Reader sees consistent snapshot even during writes\n\nFile: frankensearch-storage/src/tests/ (inline #[cfg(test)] modules in each source file)\n","created_at":"2026-02-13T20:46:17Z"},{"id":127,"issue_id":"bd-3w1.15","author":"Dicklesworthstone","text":"REVISION (review pass - test coverage analysis):\n\n1. TEST COVERAGE IS COMPREHENSIVE: The test list covers all major operations (CRUD, queue, dedup, metadata, staleness). The concurrency tests (MVCC tests) are particularly valuable since FrankenSQLite's page-level MVCC is a key differentiator.\n\n2. MISSING TEST: test_large_batch_upsert: Test with 10,000+ documents in a single upsert_batch() call. This validates that FrankenSQLite's transaction handling doesn't degrade with large transactions. The WAL should handle this, but it's worth verifying.\n\n3. MISSING TEST: test_reopened_database_persistence: Close and reopen the database, verify all data survives. This is trivial but catches issues with WAL checkpointing and fsync behavior.\n\n4. MISSING TEST: test_concurrent_claim_no_double_process: Spawn 8 threads, each calling claim_batch(10). With 50 pending jobs, verify that exactly 50 jobs are claimed total (no job claimed by two workers). This is the most critical correctness property of the persistent queue.\n\n5. IN-MEMORY vs ON-DISK: Most tests use Storage::open_in_memory(). Add at least 3 tests that use a tempdir on-disk database to verify WAL mode, fsync, and file-based persistence. The in-memory tests miss file I/O issues.\n\n6. SQLITEVALUE TYPE COERCION: Add tests that verify the typed row extraction helpers handle edge cases:\n   - get_i64 when column is actually TEXT containing a number (SQLite type affinity)\n   - get_text when column is NULL (should return Err, not panic)\n   - get_blob when column is empty BLOB (zero-length Vec<u8>)\n\n7. TRACING ASSERTION: The logs_contain() pattern is good but fragile (string matching on log messages). Consider using tracing-test's more robust approach:\n   #[traced_test]\n   fn test_upsert_changed() {\n       // ... test ...\n       assert!(logs_contain(\"document content changed\"));\n   }\n   This captures logs per-test and avoids cross-test log pollution.\n","created_at":"2026-02-13T21:01:33Z"},{"id":140,"issue_id":"bd-3w1.15","author":"Dicklesworthstone","text":"REVISION: Storage Layer Unit Test Details\n\n1. Test Categories:\n   a) Schema tests: verify table creation, column types, constraints\n   b) CRUD tests: insert/read/update/delete for documents and index_builds\n   c) Dedup tests: content-hash collision, canonicalization variants\n   d) Queue tests: enqueue/dequeue/visibility timeout/recovery\n   e) Concurrency tests: multiple readers + single writer under MVCC\n\n2. Critical Edge Cases:\n   - Empty database (first run): tables auto-created, no panics\n   - Corrupt database file: graceful error, not panic\n   - Disk full during write: transaction rollback, error surfaced\n   - Unicode document content: NFC-normalized before hashing\n   - Very large documents (>1MB): truncation applied before storage\n   - Duplicate content_hash: second insert is no-op (dedup working)\n\n3. Test Isolation:\n   Each test gets a fresh tempdir with its own FrankenSQLite database.\n   Use #[test] not #[tokio::test] — FrankenSQLite is synchronous.\n   Clean up is automatic via tempdir Drop.\n\n4. Assertion Patterns:\n   - Round-trip: insert document, read back, assert all fields match\n   - Ordering: query with ORDER BY, assert deterministic sort\n   - Counting: after N inserts, COUNT(*) = N\n   - Deletion: soft-delete sets status, hard-delete removes row\n   - Timestamps: assert ISO8601 format, monotonically increasing\n\n5. Tracing in Tests:\n   Initialize tracing_subscriber::fmt::init() in test setup.\n   Every test logs at DEBUG level so failures produce diagnostic output.\n   Use #[tracing::instrument] on test helper functions.\n","created_at":"2026-02-13T21:05:01Z"},{"id":256,"issue_id":"bd-3w1.15","author":"Dicklesworthstone","text":"REVISION (review pass 7 - missing transaction tests):\n\nCRITICAL: The unit tests bead is missing tests for the hand-rolled transaction wrapper (BEGIN/COMMIT/ROLLBACK pattern from bd-3w1.1).\n\nADD these test cases to bd-3w1.15:\n\n1. test_transaction_commit: BEGIN, INSERT, COMMIT, verify row exists\n2. test_transaction_rollback_on_error: BEGIN, INSERT, intentional error, ROLLBACK, verify row does NOT exist\n3. test_transaction_rollback_on_drop: BEGIN, INSERT, drop connection without COMMIT, verify row does NOT exist\n4. test_nested_transaction_rejection: BEGIN, BEGIN again → verify error (no nested transactions)\n5. test_concurrent_transactions: Two connections, both BEGIN, one writes, other reads → verify isolation (MVCC if FrankenSQLite supports it)\n6. test_transaction_after_close: Close connection, try BEGIN → verify error\n\nThese are the riskiest code paths in the storage layer because incorrect transaction handling causes silent data loss or corruption. The hand-rolled BEGIN/COMMIT/ROLLBACK (instead of a safe transaction() wrapper) is especially prone to bugs where ROLLBACK is missed on error paths.\n","created_at":"2026-02-13T21:54:45Z"}]}
{"id":"bd-3w1.16","title":"Write unit tests for RaptorQ durability layer","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:37.892553351Z","created_by":"ubuntu","updated_at":"2026-02-13T21:05:02.890015686Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["raptorq","testing","unit-tests"],"dependencies":[{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:37.892553351Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:32.805806450Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.7","type":"blocks","created_at":"2026-02-13T20:42:32.966379927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.8","type":"blocks","created_at":"2026-02-13T20:46:53.246208996Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:46:53.334816175Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":69,"issue_id":"bd-3w1.16","author":"Dicklesworthstone","text":"TASK: Write unit tests for the RaptorQ durability layer.\n\nComprehensive test suite for the erasure coding, repair trailer, and self-healing pipelines.\n\nTEST MODULES:\n\n1. codec_tests:\n   - test_encode_decode_roundtrip: Encode data, decode from source symbols only\n   - test_repair_from_partial: Remove some source symbols, decode from remaining + repair\n   - test_repair_20pct_corruption: Corrupt 20% of symbols, verify successful repair\n   - test_repair_exceeds_capacity: Corrupt >20% of symbols, verify graceful failure\n   - test_deterministic_symbols: Same input always produces identical repair symbols\n   - test_different_inputs_different_symbols: Different data produces different symbols\n   - test_empty_input: Zero-length source data handled correctly\n   - test_small_input: Source smaller than one symbol (edge case)\n   - test_large_input: 100MB source data (realistic index size)\n   - test_symbol_size_alignment: Non-aligned source padded correctly\n   - test_metrics_increment: Encode/decode counters update correctly\n\n2. repair_trailer_tests:\n   - test_write_read_fec_sidecar: Write .fec, read it back, verify contents\n   - test_fec_header_validation: Corrupt header magic, verify rejection\n   - test_fec_crc_validation: Corrupt CRC footer, verify rejection\n   - test_source_hash_verification: Modified source detected by hash mismatch\n   - test_fec_file_naming: .fsvi -> .fsvi.fec, .idx -> .idx.fec\n   - test_atomic_write: .fec written via temp + rename (crash-safe)\n\n3. file_protector_tests:\n   - test_protect_and_verify_intact: Protect file, verify returns Intact\n   - test_detect_single_bit_flip: Flip one bit in protected file, detect corruption\n   - test_repair_single_bit_flip: Flip one bit, repair successfully\n   - test_detect_zeroed_block: Zero out a 4KB block, detect corruption\n   - test_repair_zeroed_block: Zero out a 4KB block, repair successfully\n   - test_detect_appended_data: Extra bytes appended, detect corruption\n   - test_repair_multiple_blocks: Corrupt 3 non-adjacent blocks, repair all\n   - test_unprotected_file: verify() returns Unprotected when no .fec exists\n   - test_verify_on_open_config: Configurable verify-on-open behavior\n   - test_directory_protection: Protect all files in a directory\n   - test_directory_verification: Verify all protected files in a directory\n\n4. fsvi_protector_tests:\n   - test_protect_real_fsvi: Create a real FSVI file, protect it, verify\n   - test_corrupt_fsvi_header: Corrupt FSVI magic bytes, detect and repair\n   - test_corrupt_fsvi_vectors: Corrupt vector slab, detect and repair\n   - test_corrupt_fsvi_string_table: Corrupt doc IDs, detect and repair\n   - test_fsvi_open_with_auto_repair: VectorIndex::open() repairs corrupted file automatically\n   - test_fsvi_open_unrecoverable: Corruption beyond capacity triggers error\n\n5. tantivy_wrapper_tests:\n   - test_protect_tantivy_segments: Commit documents, verify segments protected\n   - test_corrupt_tantivy_postings: Corrupt .idx file, detect and repair\n   - test_corrupt_tantivy_store: Corrupt .store file, detect and repair\n   - test_post_merge_protection: After merge, new segment protected, old .fec cleaned up\n   - test_segment_health_report: Full report with per-segment status\n\n6. performance_tests:\n   - test_encode_throughput: Measure encode speed (expect > 100MB/s)\n   - test_decode_throughput: Measure decode speed (expect > 100MB/s)\n   - test_verify_fast_path: xxh3_64 verification (expect < 1ms per 100MB)\n   - test_repair_latency: Single-block repair (expect < 10ms)\n\nCORRUPTION SIMULATION UTILITIES:\n  fn corrupt_bytes(data: &mut [u8], offset: usize, count: usize) {\n      for i in offset..offset+count {\n          data[i] ^= 0xFF;  // Flip all bits in range\n      }\n  }\n\n  fn zero_block(data: &mut [u8], block_idx: usize, block_size: usize) {\n      let start = block_idx * block_size;\n      let end = (start + block_size).min(data.len());\n      data[start..end].fill(0);\n  }\n\n  fn random_corruption(data: &mut [u8], percent: f32, rng: &mut impl Rng) {\n      let count = (data.len() as f32 * percent / 100.0) as usize;\n      for _ in 0..count {\n          let idx = rng.gen_range(0..data.len());\n          data[idx] ^= rng.gen::<u8>();\n      }\n  }\n\nTRACING IN TESTS:\n  Every test verifies that appropriate log events are emitted:\n  - Protection: INFO \"file protected\" { path, source_size, repair_size, overhead_ratio }\n  - Verification: DEBUG \"file integrity check\" { path, result }\n  - Corruption: WARN \"corruption detected\" { path, corrupted_symbols, total_symbols }\n  - Repair: INFO \"file repaired\" { path, symbols_repaired, decode_time_ms }\n  - Failure: ERROR \"repair failed\" { path, reason }\n\nFile: frankensearch-durability/src/tests/ (inline #[cfg(test)] modules)\n","created_at":"2026-02-13T20:46:17Z"},{"id":128,"issue_id":"bd-3w1.16","author":"Dicklesworthstone","text":"REVISION (review pass - durability test analysis):\n\n1. TEST COVERAGE IS EXCELLENT: The test suite covers the full spectrum from unit (codec roundtrip) through integration (file protector with real corruption) to performance (throughput measurement). The corruption simulation utilities are well-designed.\n\n2. DETERMINISTIC REPAIR SYMBOLS: test_deterministic_symbols verifies that same input produces identical repair symbols. This is CRITICAL for the distributed search future (bd-3w1.19) where replicas must produce identical protection. However, RaptorQ encoding MAY include randomness in symbol selection (RFC 6330 allows implementation freedom). Verify that asupersync's implementation is deterministic for identical inputs. If not, document this and adjust the test.\n\n3. MISSING TEST: test_concurrent_protect_and_search: While test 4 in bd-3w1.18 (e2e) covers concurrent corruption+search, the unit tests should also cover: one thread protects a file while another reads it. The .fec file must be written atomically (temp + rename) so readers never see a partial .fec.\n\n4. MISSING TEST: test_fec_version_forward_compat: Write a .fec file with a future version number in the header. Verify that the current code detects the version mismatch and returns a clear error (not a corrupt-data panic). This prepares for future format evolution.\n\n5. PERFORMANCE TEST THRESHOLDS: The expected values (>100MB/s encode, >150MB/s decode) should be calibrated against the actual asupersync performance on the target hardware. RaptorQ performance varies significantly by CPU (AVX2 vs non-AVX2). Consider making thresholds relative rather than absolute, or gating performance tests behind a #[cfg(not(ci))] flag if CI runners are slow.\n\n6. LARGE INPUT TEST: test_large_input (100MB) may be too slow for CI. Gate it behind #[ignore] and run explicitly in performance CI. The default test suite should complete in <30 seconds.\n\n7. TANTIVY WRAPPER TESTS: test_post_merge_protection is complex because Tantivy's merge behavior is non-deterministic (depends on MergePolicy and segment sizes). Force a merge by setting a very aggressive merge policy (max_merge_docs=1) to make the test deterministic.\n","created_at":"2026-02-13T21:01:34Z"},{"id":141,"issue_id":"bd-3w1.16","author":"Dicklesworthstone","text":"REVISION: RaptorQ Durability Unit Test Details\n\n1. Codec Round-Trip Tests:\n   - Encode a known byte sequence, decode, assert identical\n   - Encode with different repair symbol counts (1, 2, 5, 10), all decode correctly\n   - Encode empty input: graceful error or zero-length output\n   - Encode very large input (10MB): completes within timeout, correct output\n\n2. Repair Tests:\n   - Corrupt N bytes at random positions, repair, verify output matches original\n   - Corrupt exactly at repair symbol boundary: repair succeeds\n   - Corrupt more bytes than repair symbols can handle: repair fails gracefully\n   - Corrupt the repair trailer itself: detection works, repair fails with clear error\n\n3. FSVI Sidecar Tests (.fec):\n   - Write FSVI, generate .fec sidecar, verify both files exist\n   - Corrupt FSVI data section, repair from .fec, verify round-trip\n   - Delete .fec file: FSVI still loads (degraded mode, no repair available)\n   - Corrupt .fec file: detection works, falls through to rebuild\n\n4. Tantivy Segment Tests (.seg.fec):\n   - Index 100 documents, generate per-segment .seg.fec files\n   - Corrupt one segment, repair from .seg.fec, verify search results unchanged\n   - Add new documents (new segment), verify old .seg.fec files still valid\n   - After Tantivy merge: old .seg.fec files cleaned up, new ones generated\n\n5. Performance Assertions:\n   - Encode 90MB (MiniLM model size): < 500ms\n   - Repair single corruption in 90MB: < 200ms\n   - Overhead ratio: repair data < 25% of original (default 20%)\n   - Memory: encoder peak < 2x input size\n","created_at":"2026-02-13T21:05:02Z"}]}
{"id":"bd-3w1.17","title":"Write integration tests for storage + search pipeline","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:39.177479366Z","created_by":"ubuntu","updated_at":"2026-02-13T21:06:34.787768934Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","integration","raptorq","testing"],"dependencies":[{"issue_id":"bd-3w1.17","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:46:54.264223285Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.17","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:39.177479366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.17","depends_on_id":"bd-3w1.13","type":"blocks","created_at":"2026-02-13T20:42:33.132719447Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.17","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:42:33.276104247Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":70,"issue_id":"bd-3w1.17","author":"Dicklesworthstone","text":"TASK: Write integration tests for the combined storage + search pipeline.\n\nThese tests verify the full end-to-end flow: document ingestion through FrankenSQLite, embedding, vector index creation with RaptorQ protection, and search with results.\n\nINTEGRATION TEST SCENARIOS:\n\n1. full_pipeline_test:\n   - Create Storage (in-memory FrankenSQLite)\n   - Ingest 100 documents via StorageBackedJobRunner\n   - Process all embedding jobs (hash embedder for CI)\n   - Build FSVI vector index from embeddings\n   - Protect index with RaptorQ\n   - Search with TwoTierSearcher\n   - Verify results match expected ground truth\n   - Verify all metrics counters are correct\n\n2. incremental_update_test:\n   - Ingest initial 50 documents\n   - Build index\n   - Ingest 20 more documents\n   - Check staleness -> reports 20 new docs\n   - Process incremental embedding jobs\n   - Update vector index (append new vectors)\n   - Search verifies new documents found\n\n3. content_change_detection_test:\n   - Ingest 50 documents\n   - Build index\n   - Update 10 documents with new content (same doc_ids, different text)\n   - Check dedup -> reports 10 changed\n   - Re-embed changed documents\n   - Search verifies updated content reflected\n\n4. crash_recovery_test:\n   - Ingest 50 documents\n   - Enqueue embedding jobs\n   - Claim a batch of 10 jobs (simulating worker start)\n   - \"Crash\" (drop the runner without completing)\n   - Create new runner with same Storage\n   - Call reclaim_stale_jobs()\n   - Verify 10 jobs reclaimed and reprocessed\n   - Verify no duplicate embeddings\n\n5. corruption_and_repair_test:\n   - Build FSVI index from 100 documents\n   - Protect with RaptorQ\n   - Corrupt 5% of the index file (random byte flips)\n   - Open the index (should detect corruption and auto-repair)\n   - Search produces correct results (identical to pre-corruption)\n\n6. fts5_and_tantivy_comparison_test:\n   - Index same 100 documents with both Tantivy AND FTS5\n   - Run identical queries against both\n   - Compare result sets (should be similar, not necessarily identical due to different tokenizers)\n   - Verify RRF fusion works with either lexical backend\n   - Log score distributions for analysis\n\n7. concurrent_ingest_and_search_test:\n   - Spawn 2 threads: one ingesting documents, one searching\n   - Verify search never blocks on ingest (MVCC)\n   - Verify search sees progressively more results as ingest proceeds\n   - No panics, no deadlocks, no data corruption\n\n8. two_tier_with_storage_test:\n   - Ingest 100 documents\n   - Build fast-tier index (hash embedder, 384d)\n   - Build quality-tier index (hash embedder, 384d -- same in CI, different in prod)\n   - Search via TwoTierSearcher\n   - Verify SearchPhase::Initial returns fast results\n   - Verify SearchPhase::Refined returns blended results\n   - Verify metrics (TwoTierMetrics) populated correctly\n\n9. durability_full_cycle_test:\n   - Ingest, embed, build all indices\n   - Protect all indices\n   - Verify all indices intact\n   - Corrupt vector index -> repair -> verify\n   - Corrupt Tantivy segment -> repair -> verify\n   - Search still produces correct results\n   - Verify repair events logged in index_metadata table\n\n10. storage_metrics_test:\n    - Run full pipeline\n    - Check all atomic counters: enqueued, completed, failed, skipped\n    - Check queue depth at various stages\n    - Check index build history\n    - Verify no metrics counter is zero (all paths exercised)\n\nSHARED TEST INFRASTRUCTURE:\n\n  /// Create a full test pipeline with in-memory storage and hash embedders\n  fn test_pipeline() -> (StorageBackedJobRunner, Storage, TwoTierSearcher) {\n      let storage = Storage::open_in_memory().unwrap();\n      let fast_embedder = Arc::new(HashEmbedder::default_256());\n      let quality_embedder = Arc::new(HashEmbedder::default_384());\n      // ... wire everything together ...\n  }\n\n  /// Generate test corpus: 100 documents with 5 clusters and known ground truth\n  fn test_corpus() -> Vec<(String, String)> {\n      // Reuses bd-3un.38 test fixture corpus\n      frankensearch_test_fixtures::corpus_100_5cluster()\n  }\n\nLOGGING:\n  All integration tests use tracing-subscriber with RUST_LOG=debug.\n  Key events logged:\n  - Pipeline stages: \"ingest_batch\" -> \"canonicalize\" -> \"hash_content\" -> \"enqueue_jobs\"\n  - Embedding: \"claim_batch\" -> \"embed\" -> \"write_vector\" -> \"complete_job\"\n  - Search: \"search_fast\" -> \"search_quality\" -> \"rrf_fuse\" -> \"return_results\"\n  - Durability: \"protect_index\" -> \"verify_index\" -> \"repair_index\"\n\nFile: tests/integration/storage_pipeline_test.rs\n","created_at":"2026-02-13T20:46:18Z"},{"id":129,"issue_id":"bd-3w1.17","author":"Dicklesworthstone","text":"REVISION (review pass - integration test analysis):\n\n1. TEST INFRASTRUCTURE REUSE: test_pipeline() should reuse bd-3un.38 test fixture corpus for consistency across all test suites. Don't create separate test data generators. The ground truth relevance data from bd-3un.38 enables meaningful recall/precision assertions.\n\n2. CRASH RECOVERY TEST: The \"crash\" simulation (drop the runner without completing) correctly tests the visibility timeout mechanism. However, also test the case where the worker crashes AFTER embedding but BEFORE marking the job complete. This is the most dangerous failure mode because it can cause duplicate embeddings if not handled correctly. The idempotency check (content_hash dedup) should prevent actual duplicates.\n\n3. FTS5 AND TANTIVY COMPARISON TEST: This test is valuable but should NOT block the test suite if FTS5 is not enabled (feature-gate it). Also, the comparison is informational, not a pass/fail test. Log the differences rather than asserting exact equality:\n   tracing::info!(\n       query = %query,\n       tantivy_count = tantivy_results.len(),\n       fts5_count = fts5_results.len(),\n       overlap = overlap_count,\n       jaccard = overlap_count as f32 / union_count as f32,\n       \"lexical engine comparison\"\n   );\n\n4. CONCURRENT INGEST AND SEARCH TEST: This test needs a termination condition. Don't use an infinite loop. Instead:\n   - Ingest exactly 200 documents over 2 seconds (100/sec)\n   - Search continuously until ingest completes\n   - Then do a final search and verify all 200 docs are findable\n   The test should complete in <5 seconds.\n\n5. HASH EMBEDDER FOR CI: All tests use hash embedder in CI (correct -- ONNX models shouldn't be CI dependencies). But add a note that the hash embedder produces RANDOM-LOOKING embeddings (FNV hash of text -> vector), so ground truth recall assertions should use a lenient threshold (recall >= 0.3, not >= 0.9). Semantic quality assertions belong in the e2e tests with real models (bd-3w1.18 or bd-3un.40).\n\n6. FEATURE GATING: Gate storage-specific tests behind #[cfg(feature = \"storage\")] and durability tests behind #[cfg(feature = \"durability\")]. Tests should compile and pass regardless of which features are enabled.\n\n7. IN-MEMORY DATABASE FOR SPEED: All integration tests should use in-memory FrankenSQLite (Storage::open_in_memory()) for speed, EXCEPT the crash recovery test which must use an on-disk database to verify WAL persistence.\n","created_at":"2026-02-13T21:01:36Z"},{"id":142,"issue_id":"bd-3w1.17","author":"Dicklesworthstone","text":"REVISION: Integration Test Details\n\n1. Full Pipeline Test:\n   Use shared test fixture corpus (bd-3un.38, 100 documents).\n   a) Create FrankenSQLite database in tempdir\n   b) Index all 100 documents through persistent pipeline\n   c) Verify dedup: re-index same 100 docs, queue stays empty\n   d) Search with 5 ground-truth queries, verify recall >= 0.8\n   e) Check index_builds metadata matches FSVI header\n\n2. Storage + Durability Combined Test:\n   a) Index documents, build FSVI + Tantivy indices\n   b) Generate RaptorQ sidecars (.fec, .seg.fec)\n   c) Corrupt FSVI data section (flip 10 random bytes)\n   d) Run repair pipeline, verify search results unchanged\n   e) Verify index_builds.status transitions: 'ready' -> 'corrupt' -> 'ready'\n\n3. Feature Flag Isolation:\n   - With storage + without durability: persistent queue works, no .fec files\n   - With durability + without storage: in-memory queue, .fec files generated\n   - With both: full pipeline including repair\n   - With neither: pure in-memory, no FrankenSQLite dependency\n\n4. Crash Recovery Simulation:\n   a) Enqueue 50 embedding jobs\n   b) Process 25, then \"crash\" (drop the runner without cleanup)\n   c) Create new runner, verify 25 in-flight jobs recovered to 'pending'\n   d) Process remaining 50, verify all documents indexed\n\n5. Cross-Reference with bd-3un.38 Fixtures:\n   Integration tests MUST use the shared fixture corpus, not create their own.\n   This ensures consistency between in-memory and persistent pipeline tests.\n   Import fixtures via a shared test utility crate or module.\n","created_at":"2026-02-13T21:05:03Z"},{"id":161,"issue_id":"bd-3w1.17","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MODERATE REVISION (concurrent integration tests):\n\nbd-3w1.17 has integration tests that spawn 2 std threads for concurrent ingest+search. Replace with asupersync LabRuntime for deterministic concurrent testing:\n\nBEFORE:\n  - std::thread::spawn for concurrent ingest and search threads\n  - Non-deterministic race conditions\n\nAFTER:\n  - LabRuntime for deterministic scheduling of concurrent tasks\n  - cx.region(|scope| { scope.spawn(ingest); scope.spawn(search); })\n  - LabRuntime oracles verify: no deadlocks, no obligation leaks, no orphan tasks\n  - DPOR schedule explorer can systematically explore interleaving space\n\n  #[test]\n  fn concurrent_ingest_and_search() {\n      let lab = LabRuntime::new(LabConfig::new(42));\n      lab.run(|cx| async {\n          cx.region(|scope| async {\n              scope.spawn(|cx| async {\n                  // Ingest 100 documents\n                  for doc in test_docs() {\n                      queue.enqueue(&cx, doc).await.unwrap();\n                  }\n              });\n              scope.spawn(|cx| async {\n                  // Search while ingest is running\n                  for query in test_queries() {\n                      let results = searcher.search(&cx, &query, 10).await;\n                      // Verify: search never panics, returns valid results\n                  }\n              });\n          }).await;\n      });\n      // Oracles automatically verify correctness\n      assert!(lab.quiescence_oracle().is_ok());\n      assert!(lab.obligation_leak_oracle().is_ok());\n  }","created_at":"2026-02-13T21:06:34Z"}]}
{"id":"bd-3w1.18","title":"Write e2e corruption-and-recovery test suite","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:44.025461125Z","created_by":"ubuntu","updated_at":"2026-02-13T21:05:04.584894318Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","e2e","raptorq","testing"],"dependencies":[{"issue_id":"bd-3w1.18","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:44.025461125Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.18","depends_on_id":"bd-3w1.17","type":"blocks","created_at":"2026-02-13T20:46:55.666225301Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.18","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:42:33.399117653Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":71,"issue_id":"bd-3w1.18","author":"Dicklesworthstone","text":"TASK: Write e2e corruption-and-recovery test suite.\n\nDedicated end-to-end tests that simulate real-world corruption scenarios and verify the self-healing pipeline recovers correctly. These tests are the \"proof that pervasive RaptorQ works.\"\n\nTEST SCENARIOS:\n\n1. power_loss_during_index_write:\n   - Begin writing FSVI vector index\n   - Simulate power loss mid-write (truncate file at random offset)\n   - Attempt to open the truncated file\n   - If .fec sidecar exists from previous build: repair and open\n   - If no .fec: detect corruption, trigger full rebuild from FrankenSQLite\n\n2. bit_rot_simulation:\n   - Create and protect a complete search index (FSVI + Tantivy)\n   - Simulate gradual bit rot: flip 1 random bit every \"day\" for 100 \"days\"\n   - After each day, verify and repair if needed\n   - Count how many days of bit rot the index survives (should be >95 with 20% overhead)\n   - Log repair events with timestamps\n\n3. storage_medium_failure:\n   - Create indices on disk\n   - Zero out a random 4KB block (simulating a bad sector)\n   - Verify detection and repair\n   - Zero out 10 random 4KB blocks (simulating multiple bad sectors)\n   - Verify detection and repair (should succeed up to 20% of file)\n   - Zero out 25% of file (exceeds repair capacity)\n   - Verify graceful failure with clear error message\n\n4. concurrent_corruption_and_search:\n   - Start search workload (continuous queries)\n   - In background, corrupt 1 vector index block\n   - Verify search detects corruption, repairs, and retries automatically\n   - Verify search results are correct after repair\n   - Verify no search queries return wrong results during corruption\n\n5. cascading_corruption:\n   - Corrupt FSVI vector index (self-heals from .fec sidecar)\n   - Then corrupt the .fec sidecar itself\n   - Then corrupt the FrankenSQLite WAL\n   - Verify FrankenSQLite's own WAL-FEC repairs the WAL\n   - Verify the document store (FrankenSQLite) can rebuild the vector index\n   - This tests the \"defense in depth\" property of pervasive RaptorQ\n\n6. full_rebuild_from_storage:\n   - Create complete search setup (storage + indices)\n   - Delete ALL index files (FSVI, Tantivy, .fec sidecars)\n   - Verify the system detects missing indices\n   - Trigger full rebuild from FrankenSQLite document store\n   - Verify rebuilt indices produce identical search results\n   - This proves FrankenSQLite IS the source of truth\n\n7. fec_sidecar_corruption:\n   - Protect an index, producing .fec sidecar\n   - Corrupt the .fec sidecar (not the index)\n   - Verify that verification still detects this (CRC footer check)\n   - Regenerate .fec from the intact index\n   - Verify new .fec is identical to original (deterministic repair symbols)\n\n8. partial_index_recovery:\n   - Write 200 vectors to FSVI, protect\n   - Corrupt the last 50 vectors (tail of file)\n   - Repair: only the corrupted portion is regenerated\n   - Verify: first 150 vectors unchanged, last 50 recovered\n   - This tests that repair is surgical, not a full rewrite\n\nVALIDATION FRAMEWORK:\n\n  /// Verify that a search index produces correct results after repair\n  fn verify_search_integrity(\n      searcher: &TwoTierSearcher,\n      queries: &[(String, Vec<String>)],  // (query, expected_doc_ids)\n  ) -> Vec<IntegrityCheckResult> {\n      queries.iter().map(|(query, expected)| {\n          let results = searcher.search_fast_only(query, expected.len());\n          let actual_ids: Vec<_> = results.iter().map(|r| r.doc_id.clone()).collect();\n          IntegrityCheckResult {\n              query: query.clone(),\n              expected_count: expected.len(),\n              actual_count: actual_ids.len(),\n              recall: compute_recall(&actual_ids, expected),\n              passed: compute_recall(&actual_ids, expected) >= 0.9,\n          }\n      }).collect()\n  }\n\nLOGGING (COLORIZED):\n  Every test phase logs with clear headers:\n  tracing::info!(\"=== PHASE 1: Create and protect indices ===\");\n  tracing::info!(\"=== PHASE 2: Simulate corruption ===\");\n  tracing::info!(\"=== PHASE 3: Detect and repair ===\");\n  tracing::info!(\"=== PHASE 4: Verify search integrity ===\");\n\n  Each phase includes timing and byte-level details:\n  tracing::info!(\n      phase = \"corruption\",\n      file = %path.display(),\n      offset = corrupted_offset,\n      bytes = corrupted_bytes,\n      \"simulated corruption injected\"\n  );\n\nFile: tests/e2e/corruption_recovery_test.rs\n","created_at":"2026-02-13T20:46:18Z"},{"id":130,"issue_id":"bd-3w1.18","author":"Dicklesworthstone","text":"REVISION (review pass - e2e test analysis):\n\n1. POWER LOSS SIMULATION: test \"power_loss_during_index_write\" should use write() without fsync to simulate a partial write. Then verify that:\n   a) If .fec from a PREVIOUS successful build exists, repair from that .fec\n   b) If no .fec exists (first build), detect corruption and trigger full rebuild from FrankenSQLite\n   Note: case (a) restores to the PREVIOUS version, not the in-progress version. This is correct behavior -- the partial write never completed, so rolling back to the last good version is the right thing.\n\n2. BIT ROT SIMULATION TIMING: The \"100 days\" simulation should be fast (no actual sleeping). Use a loop counter, not time-based delays:\n   for day in 0..100 {\n       corrupt_random_bit(&mut index_data);\n       let result = protector.verify_and_repair(&index_path)?;\n       tracing::info!(day, result = ?result, \"bit rot simulation\");\n   }\n   Expected: with 20% overhead and 4KB symbol size on a 50MB file, the index should survive ~2500 single-bit flips before exhausting repair capacity.\n\n3. CASCADING CORRUPTION TEST: This is the most valuable test in the suite. It proves the \"defense in depth\" property: even if the self-healing layer's metadata (.fec) is compromised, the source of truth (FrankenSQLite with its own WAL-FEC) enables full recovery. However, this test requires FrankenSQLite's WAL-FEC to be enabled, which may require specific PRAGMA settings. Verify:\n   PRAGMA raptorq_repair_symbols = 2;  -- Enable WAL-FEC with 2 repair symbols per frame group\n\n4. FULL REBUILD FROM STORAGE: This test (scenario 6) is the ultimate correctness proof. The assertion should be EXACT match on search results (same doc_ids, same order, same scores), not approximate. Since the rebuilding uses the same documents and the same hash embedder, the rebuilt indices must be byte-identical to the originals.\n\n5. PARTIAL INDEX RECOVERY: test \"partial_index_recovery\" should verify byte-level correctness: the repaired vectors must be bitwise identical to the originals. Use memcmp (or Rust slice equality) on the recovered region.\n\n6. TEST ISOLATION: Each e2e test should use its own tempdir. Never share state between tests. Use:\n   let test_dir = tempfile::tempdir()?;\n   // All files created under test_dir.path()\n   // test_dir dropped at end, cleaning up automatically\n\n7. CI RUNTIME BUDGET: The full e2e suite should complete in <60 seconds. The bit rot simulation (100 iterations) and large file tests may be slow. Consider splitting into:\n   - e2e_fast: scenarios 1, 3, 6, 7 (< 15 seconds)\n   - e2e_slow: scenarios 2, 4, 5, 8 (< 60 seconds, run nightly)\n","created_at":"2026-02-13T21:01:37Z"},{"id":143,"issue_id":"bd-3w1.18","author":"Dicklesworthstone","text":"REVISION: E2E Corruption Test Suite Details\n\n1. Corruption Scenarios:\n   a) Single-byte flip in FSVI data section: repair succeeds, search unchanged\n   b) Multi-byte corruption (10 bytes): repair succeeds up to repair capacity\n   c) Header corruption in FSVI: detected at load time, full rebuild triggered\n   d) Tantivy segment corruption: per-segment repair from .seg.fec\n   e) Tantivy meta.json corruption: full Tantivy rebuild from documents\n   f) FrankenSQLite WAL corruption: WAL-FEC repair (if available)\n   g) Simultaneous FSVI + Tantivy corruption: both repaired independently\n\n2. Test Harness:\n   Each scenario follows the pattern:\n   1. Build clean index from fixture corpus (bd-3un.38)\n   2. Generate RaptorQ sidecars\n   3. Run ground-truth queries, save \"golden\" results\n   4. Apply corruption (deterministic: use fixed seed for byte positions)\n   5. Detect corruption (verify detection logs WARN/ERROR)\n   6. Run repair pipeline\n   7. Re-run ground-truth queries, assert results match golden\n\n3. Logging Requirements:\n   Every corruption test emits structured logs with:\n   - corruption_type: enum variant\n   - corruption_offset: byte position\n   - corruption_size: number of bytes affected\n   - repair_result: success/failure\n   - repair_duration_ms: wall clock time\n   - search_results_match: bool (golden comparison)\n   Use tracing spans: \"e2e_corruption_test\" -> \"corruption_inject\" -> \"repair\" -> \"verify\"\n\n4. Beyond-Repair Scenarios:\n   Test graceful degradation when repair fails:\n   - Corrupt more bytes than repair symbols can handle\n   - Delete both .fec and original file\n   - Corrupt .fec sidecar itself\n   In all cases: system must not panic, must log ERROR, must return\n   SearchError::IndexCorrupt with recovery guidance.\n\n5. Performance Bounds:\n   - Full repair cycle (detect + repair + verify): < 2 seconds for 100K doc index\n   - Detection only (without repair): < 100ms\n   - These are asserted in tests, not just measured\n","created_at":"2026-02-13T21:05:04Z"}]}
{"id":"bd-3w1.19","title":"Design Native Mode integration for distributed search (future)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:37:44.997509958Z","created_by":"ubuntu","updated_at":"2026-02-13T21:01:38.107769778Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","future","native-mode","tier4"],"dependencies":[{"issue_id":"bd-3w1.19","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:44.997509958Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":72,"issue_id":"bd-3w1.19","author":"Dicklesworthstone","text":"TASK: Design the Native Mode integration for distributed search (future architecture).\n\nThis is a DESIGN-ONLY bead -- no implementation, just architecture documentation for how frankensearch could leverage FrankenSQLite's Native Mode in the future.\n\nNATIVE MODE OVERVIEW:\nFrankenSQLite's Native Mode replaces the traditional SQLite file with an append-only ECS (Erasure-Coded Stream) commit stream. Every mutation is a CommitCapsule: a content-addressed, erasure-coded object identified by a BLAKE3-derived ObjectId.\n\nDISTRIBUTED SEARCH ARCHITECTURE:\n\n  1. ECS Commit Stream for Index Updates:\n     - Each document ingestion becomes a CommitCapsule\n     - CommitCapsules replicate across nodes via rateless coding\n     - Each node rebuilds its local search indices from the commit stream\n     - Index state is fully deterministic from the commit history\n\n  2. Snapshot Shipping for New Nodes:\n     - New search nodes receive a snapshot via RaptorQ-encoded transfer\n     - Bandwidth-optimal: any K of N received symbols suffice for reconstruction\n     - No coordination required: sender transmits fountain of symbols, receiver collects K\n     - Over lossy networks (unreliable UDP), this is dramatically more efficient than TCP\n\n  3. Time-Travel Queries:\n     - Query the search index as it existed at any historical commit point\n     - Use case: \"what would this query have returned yesterday?\"\n     - Implementation: maintain commit-indexed checkpoints of the search state\n     - FrankenSQLite's MVCC visibility check: V.commit_seq <= S.high\n\n  4. Quorum Durability for Search Indices:\n     - PRAGMA durability = quorum(M): index data durable across M of N replicas\n     - Each replica maintains its own local FSVI + Tantivy indices\n     - CommitMarkers (not capsules) determine commit finality\n     - Replicas can independently verify each other's indices via deterministic repair symbols\n\nEXAMPLE: 3-Node Search Cluster\n\n  Node A: Primary writer, ingests documents\n    -> CommitCapsule{doc_id: \"d1\", text: \"hello world\"} with ObjectId=BLAKE3(...)\n    -> Encodes capsule into K source symbols + R repair symbols\n    -> Streams symbols to Nodes B and C\n\n  Node B: Replica\n    -> Receives symbols, decodes CommitCapsule\n    -> Applies capsule: inserts doc into local FrankenSQLite\n    -> Triggers embedding pipeline (local embedder)\n    -> Updates local FSVI + Tantivy indices\n    -> Search queries served from local indices (zero network latency)\n\n  Node C: Replica (same as B)\n\n  Result: Each node has identical search indices, built from the same commit stream,\n  with independent RaptorQ protection. If any single node's storage fails, the other\n  two can reconstruct it from their repair symbols + commit stream.\n\nINCREMENTAL INDEX REPLICATION (ALTERNATIVE):\n  Instead of each node rebuilding indices from raw documents, share the indices directly:\n  - After index build on primary, encode the FSVI file as RaptorQ symbols\n  - Ship symbols to replicas (rateless: any K symbols suffice)\n  - Replicas decode to get identical FSVI files\n  - This is faster than re-embedding (avoids ML inference on each node)\n  - Trade-off: requires more network bandwidth but saves compute\n\nWHY THIS IS TIER 4 (FUTURE):\n  1. FrankenSQLite Native Mode is Phase 6+ (not yet wired to Connection)\n  2. Distributed search requires network protocol design\n  3. The commit stream -> index rebuild pipeline needs careful ordering guarantees\n  4. Single-node frankensearch (Tiers 1-3) is the priority\n\n  But the architecture is designed FROM THE START to be distribution-ready:\n  - FrankenSQLite as source of truth (not the index files)\n  - Deterministic repair symbols (replicas can cross-verify)\n  - Content-addressed objects (ObjectId for cache coherence)\n  - Transactional consistency (MVCC snapshots for consistent reads)\n\nNO DEPENDENCIES: This bead blocks nothing and is blocked by everything.\nIt's a north-star design document for the project's long-term vision.\n\nFile: docs/architecture/native-mode-distributed-search.md (design doc, not code)\n","created_at":"2026-02-13T20:46:18Z"},{"id":131,"issue_id":"bd-3w1.19","author":"Dicklesworthstone","text":"REVISION (review pass - design doc verification):\n\n1. DESIGN-ONLY STATUS IS CORRECT: This bead is appropriately marked as P3 (future) with no dependencies. It should NOT block any implementation work.\n\n2. ECS COMMIT STREAM FOR INDEX UPDATES: The architecture is sound. Each CommitCapsule is content-addressed (BLAKE3 ObjectId), which enables cache-coherent replication. However, note that the commit stream is NOT a WAL replay -- it's higher-level (document operations), so index rebuilding requires the full embedding pipeline, not just WAL redo.\n\n3. INCREMENTAL INDEX REPLICATION: The alternative approach (ship FSVI files directly via RaptorQ) is more practical for the initial implementation. Re-embedding on each node wastes compute (identical results, since embedding is deterministic for fixed model weights). This should be the PREFERRED approach in the initial distributed design.\n\n4. QUORUM DURABILITY: PRAGMA durability = quorum(M) requires the CommitMarker protocol, which is Phase 7+ in FrankenSQLite. This is correctly identified as very future work.\n\n5. NO CHANGES NEEDED: This bead is well-written as a north-star design document. No revisions required.\n","created_at":"2026-02-13T21:01:38Z"}]}
{"id":"bd-3w1.2","title":"Implement document metadata schema and CRUD operations","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:12.542167228Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:06.077706914Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","schema","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.2","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:12.542167228Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.2","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.599373073Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":55,"issue_id":"bd-3w1.2","author":"Dicklesworthstone","text":"TASK: Implement the SQL schema and CRUD operations for document metadata storage.\n\nThis is the core of Tier 1 integration. Every document indexed by frankensearch gets a row in FrankenSQLite that tracks its metadata, embedding status, and content fingerprint.\n\nSQL SCHEMA:\n\n  CREATE TABLE IF NOT EXISTS documents (\n      doc_id TEXT PRIMARY KEY,          -- Caller-provided unique ID\n      source_path TEXT,                 -- Optional filesystem path or URL\n      content_preview TEXT,             -- First 400 chars (for result snippets without re-reading source)\n      content_hash BLOB NOT NULL,       -- SHA-256 of canonicalized text (32 bytes)\n      content_length INTEGER NOT NULL,  -- Original text length in bytes\n      created_at INTEGER NOT NULL,      -- Unix timestamp millis\n      updated_at INTEGER NOT NULL,      -- Unix timestamp millis (last content change)\n      metadata_json TEXT                -- Arbitrary JSON metadata from caller\n  );\n\n  CREATE TABLE IF NOT EXISTS embedding_status (\n      doc_id TEXT NOT NULL REFERENCES documents(doc_id) ON DELETE CASCADE,\n      embedder_id TEXT NOT NULL,        -- e.g., \"potion-multilingual-128M\" or \"all-MiniLM-L6-v2\"\n      embedder_revision TEXT,           -- Model commit SHA for staleness detection\n      status TEXT NOT NULL DEFAULT 'pending',  -- pending | embedded | failed | skipped\n      embedded_at INTEGER,             -- When embedding was computed\n      error_message TEXT,              -- If status='failed', why\n      retry_count INTEGER DEFAULT 0,\n      PRIMARY KEY (doc_id, embedder_id)\n  );\n\n  CREATE INDEX idx_embedding_status_pending\n      ON embedding_status(status) WHERE status = 'pending';\n  CREATE INDEX idx_documents_content_hash\n      ON documents(content_hash);\n  CREATE INDEX idx_documents_updated_at\n      ON documents(updated_at);\n\nCRUD API:\n\n  pub struct DocumentRecord {\n      pub doc_id: String,\n      pub source_path: Option<String>,\n      pub content_preview: String,\n      pub content_hash: [u8; 32],\n      pub content_length: usize,\n      pub created_at: i64,\n      pub updated_at: i64,\n      pub metadata: Option<serde_json::Value>,\n  }\n\n  impl Storage {\n      /// Upsert a document. If doc_id exists with same content_hash, no-op (returns false).\n      /// If doc_id exists with different content_hash, updates and resets embedding status.\n      pub fn upsert_document(&self, doc: &DocumentRecord) -> SearchResult<bool>;\n\n      /// Get document by ID\n      pub fn get_document(&self, doc_id: &str) -> SearchResult<Option<DocumentRecord>>;\n\n      /// List documents that need embedding for a given embedder\n      pub fn list_pending_embeddings(&self, embedder_id: &str, limit: usize) -> SearchResult<Vec<String>>;\n\n      /// Mark embedding as completed for a doc/embedder pair\n      pub fn mark_embedded(&self, doc_id: &str, embedder_id: &str) -> SearchResult<()>;\n\n      /// Mark embedding as failed with error message\n      pub fn mark_failed(&self, doc_id: &str, embedder_id: &str, error: &str) -> SearchResult<()>;\n\n      /// Count documents by embedding status\n      pub fn count_by_status(&self, embedder_id: &str) -> SearchResult<StatusCounts>;\n\n      /// Delete document and cascade to embedding_status\n      pub fn delete_document(&self, doc_id: &str) -> SearchResult<bool>;\n\n      /// Batch upsert (uses FrankenSQLite transaction for atomicity)\n      pub fn upsert_batch(&self, docs: &[DocumentRecord]) -> SearchResult<BatchResult>;\n  }\n\nDESIGN RATIONALE:\n1. content_hash as BLOB (not TEXT) -- 32 bytes raw vs 64 hex chars, faster comparison\n2. Separate embedding_status table -- supports multi-tier embedding (fast + quality have different status)\n3. Partial index on status='pending' -- only pending items need scanning, keeps index small\n4. content_preview stored here -- enables result snippets without re-reading full content from external source\n5. CASCADE delete -- removing a document auto-removes all embedding status rows\n6. FrankenSQLite MVCC -- upsert_batch can run in a transaction while search queries read concurrently\n\nWHY THIS MATTERS:\nWithout persistent metadata, frankensearch has no way to:\n- Know which documents have been embedded by which tier\n- Detect content changes (content_hash comparison)\n- Resume after crash (all in-memory state lost)\n- Track embedding failures for retry\nThis table is the single source of truth for the indexing pipeline.\n\nFile: frankensearch-storage/src/document.rs\n","created_at":"2026-02-13T20:46:08Z"},{"id":114,"issue_id":"bd-3w1.2","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. SQL PARAMETER BINDING: All queries must use SqliteValue enum for parameters:\n   conn.execute_with_params(\n       \"INSERT INTO documents (doc_id, source_path, content_preview, content_hash, content_length, created_at, updated_at, metadata_json) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n       &[\n           SqliteValue::Text(doc.doc_id.clone()),\n           doc.source_path.as_ref().map_or(SqliteValue::Null, |s| SqliteValue::Text(s.clone())),\n           SqliteValue::Text(doc.content_preview.clone()),\n           SqliteValue::Blob(doc.content_hash.to_vec()),\n           SqliteValue::Integer(doc.content_length as i64),\n           SqliteValue::Integer(doc.created_at),\n           SqliteValue::Integer(doc.updated_at),\n           doc.metadata.as_ref().map_or(SqliteValue::Null, |v| SqliteValue::Text(serde_json::to_string(v).unwrap())),\n       ]\n   )?;\n\n2. ROW EXTRACTION HELPERS: Since Row::get() returns Option<&SqliteValue>, add typed extraction:\n   fn get_text(row: &Row, idx: usize) -> SearchResult<String> {\n       match row.get(idx) {\n           Some(SqliteValue::Text(s)) => Ok(s.clone()),\n           Some(SqliteValue::Null) => Err(SearchError::StorageError(\"unexpected NULL\".into())),\n           _ => Err(SearchError::StorageError(\"type mismatch\".into())),\n       }\n   }\n   fn get_optional_text(row: &Row, idx: usize) -> SearchResult<Option<String>> { ... }\n   fn get_i64(row: &Row, idx: usize) -> SearchResult<i64> { ... }\n   fn get_blob(row: &Row, idx: usize) -> SearchResult<Vec<u8>> { ... }\n\n3. TRANSACTION WRAPPING: upsert_batch() should use the Storage::transaction() helper from the bd-3w1.1 revision. Each batch is atomic:\n   storage.transaction(|conn| {\n       for doc in docs {\n           // upsert logic\n       }\n       Ok(batch_result)\n   })?;\n\n4. CONTENT_HASH AS BLOB: The schema correctly uses BLOB for content_hash (32 bytes). When querying:\n   conn.query_with_params(\"SELECT * FROM documents WHERE content_hash = ?\",\n       &[SqliteValue::Blob(hash.to_vec())])?;\n","created_at":"2026-02-13T20:58:06Z"}]}
{"id":"bd-3w1.20","title":"Add durability benchmarks (encode/decode throughput, repair latency)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:37:45.981449400Z","created_by":"ubuntu","updated_at":"2026-02-13T21:05:05.862062420Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["benchmarks","performance","raptorq"],"dependencies":[{"issue_id":"bd-3w1.20","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:45.981449400Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.20","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:34.612703593Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.20","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:42:34.734313421Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":73,"issue_id":"bd-3w1.20","author":"Dicklesworthstone","text":"TASK: Add benchmarks for RaptorQ durability operations.\n\nMeasure encode/decode throughput, repair latency, and overhead for realistic index sizes.\n\nBENCHMARK SCENARIOS:\n\n  1. encode_throughput:\n     - Source sizes: 1MB, 10MB, 50MB, 100MB, 500MB\n     - Symbol size: 4KB (default)\n     - Overhead: 20% (default)\n     - Measure: MB/s encoding throughput\n     - Expected: > 200MB/s (RaptorQ is compute-bound, scales with CPU)\n\n  2. decode_throughput:\n     - Same sizes as encode\n     - Decode from source symbols only (no corruption, fast path)\n     - Decode from source + repair symbols (simulated corruption)\n     - Measure: MB/s decoding throughput\n     - Expected: > 150MB/s for clean decode, > 100MB/s for repair decode\n\n  3. verify_fast_path:\n     - Source sizes: 1MB, 10MB, 50MB, 100MB, 500MB\n     - Measure: xxh3_64 verification time (the fast path before RaptorQ)\n     - Expected: > 5GB/s (xxh3 is extremely fast, memory-bandwidth-limited)\n\n  4. repair_latency_by_corruption:\n     - 50MB source, 20% overhead\n     - Corruption levels: 1 block, 5 blocks, 1%, 5%, 10%, 20%\n     - Measure: repair latency for each corruption level\n     - Expected: < 50ms for 1 block, < 500ms for 20% corruption\n\n  5. overhead_vs_protection:\n     - 50MB source\n     - Overhead levels: 5%, 10%, 20%, 30%, 50%\n     - Measure: .fec file size, encode time, max repairable corruption\n     - This shows the trade-off curve: more overhead = more protection = larger .fec\n\n  6. realistic_index_encode:\n     - Create a real FSVI index with 10K, 50K, 100K documents (384 dimensions, f16)\n     - Measure: time to protect the index (encode + write .fec)\n     - This benchmarks the actual production workload\n     - Expected: < 1s for 10K docs, < 5s for 100K docs\n\n  7. concurrent_verify:\n     - 4 threads verifying 4 different protected files simultaneously\n     - Measure: per-thread throughput vs single-thread\n     - Expected: near-linear scaling (verification is memory-bandwidth-bound, not CPU-bound)\n\nBENCHMARK INFRASTRUCTURE:\n\n  Use criterion for statistical benchmarks:\n\n  fn bench_encode(c: &mut Criterion) {\n      let codec = RepairCodec::new(RepairCodecConfig::default()).unwrap();\n      let data = vec![42u8; 50 * 1024 * 1024]; // 50MB\n\n      c.bench_function(\"encode_50mb\", |b| {\n          b.iter(|| codec.encode(black_box(&data)))\n      });\n  }\n\n  fn bench_verify_fast_path(c: &mut Criterion) {\n      let data = vec![42u8; 100 * 1024 * 1024]; // 100MB\n      let expected_hash = xxh3_64(&data);\n\n      c.bench_function(\"verify_fast_100mb\", |b| {\n          b.iter(|| {\n              let hash = xxh3_64(black_box(&data));\n              assert_eq!(hash, expected_hash);\n          })\n      });\n  }\n\nPERFORMANCE BUDGET (HARD LIMITS):\n  These are budgets that should cause CI failure if exceeded:\n  - Encode 50MB: < 2 seconds (25 MB/s minimum)\n  - Decode 50MB (clean): < 2 seconds\n  - Verify 100MB (fast path): < 50ms\n  - Repair 1 block (4KB): < 10ms\n  - .fec overhead at 20%: within [19%, 21%] of source size\n\nFile: benches/durability_bench.rs\n","created_at":"2026-02-13T20:46:20Z"},{"id":132,"issue_id":"bd-3w1.20","author":"Dicklesworthstone","text":"REVISION (review pass - benchmark analysis):\n\n1. CRITERION USAGE: Criterion is the right choice for statistical benchmarks. Add the dependency:\n   [dev-dependencies]\n   criterion = { version = \"0.5\", features = [\"html_reports\"] }\n\n   [[bench]]\n   name = \"durability_bench\"\n   harness = false\n\n2. PERFORMANCE BUDGET ENFORCEMENT: The hard limits (encode 50MB < 2s, verify 100MB < 50ms, etc.) are good but should be CI-enforced via a custom test, not criterion (which generates reports but doesn't fail on regression by default). Add:\n   #[test]\n   fn test_encode_performance_budget() {\n       let data = vec![0u8; 50 * 1024 * 1024];\n       let start = Instant::now();\n       codec.encode(&data).unwrap();\n       let elapsed = start.elapsed();\n       assert!(elapsed < Duration::from_secs(2),\n           \"encode 50MB took {elapsed:?}, budget is 2s\");\n   }\n\n3. REALISTIC INDEX ENCODE: The benchmark with real FSVI files is the most actionable. Include the cost breakdown:\n   - Time to read the file (mmap or read_to_vec)\n   - Time to compute xxh3_64 hash\n   - Time to encode repair symbols\n   - Time to write .fec sidecar\n   This breakdown reveals where time is actually spent and guides optimization.\n\n4. OVERHEAD_VS_PROTECTION BENCHMARK: This is unique and very informative. Output the results as a table:\n   | Overhead | FEC Size | Encode Time | Max Repairable |\n   |----------|----------|-------------|----------------|\n   | 5%       | 2.5MB   | 0.3s        | ~5%            |\n   | 10%      | 5.0MB   | 0.5s        | ~10%           |\n   | 20%      | 10.0MB  | 0.9s        | ~20%           |\n   | 30%      | 15.0MB  | 1.3s        | ~30%           |\n   | 50%      | 25.0MB  | 2.1s        | ~50%           |\n\n5. MEMORY MEASUREMENT: Add a benchmark that measures PEAK MEMORY usage during encode and decode. RaptorQ encoding can be memory-hungry if it buffers all repair symbols. Use the jemalloc allocator with stats enabled, or read /proc/self/status VmPeak.\n\n6. CONCURRENT VERIFY SCALING: The 4-thread test should use criterion's benchmark groups to compare 1/2/4/8 thread configurations systematically, not just assert \"near-linear.\"\n","created_at":"2026-02-13T21:01:39Z"},{"id":144,"issue_id":"bd-3w1.20","author":"Dicklesworthstone","text":"REVISION: Durability Benchmark Details\n\n1. Benchmark Matrix:\n   | Operation | Input Size | Target |\n   |-----------|-----------|--------|\n   | Encode (generate repair symbols) | 1MB | < 10ms |\n   | Encode | 10MB | < 50ms |\n   | Encode | 90MB (MiniLM model) | < 500ms |\n   | Decode (verify integrity) | 1MB | < 5ms |\n   | Decode | 90MB | < 200ms |\n   | Repair (single corruption) | 1MB | < 10ms |\n   | Repair (single corruption) | 90MB | < 200ms |\n   | Repair (max corruption) | 90MB | < 500ms |\n\n2. Overhead Measurement:\n   Measure the ratio: (repair_symbols_size / original_size) * 100\n   Default target: 20% overhead (configurable via TwoTierConfig)\n   Report actual overhead for each benchmark run.\n   Assert: actual overhead <= configured overhead + 1% (rounding tolerance).\n\n3. Memory Profiling:\n   Track peak RSS during encode/decode/repair operations.\n   Assert: peak memory < 2x input size (encoder should stream, not buffer).\n   Use a custom allocator wrapper or /proc/self/status on Linux.\n\n4. Benchmark Harness:\n   Use criterion for statistical rigor (warmup, multiple iterations, CI).\n   Benchmark groups: \"encode\", \"decode\", \"repair\", \"overhead\"\n   Each group parameterized by input size: [1KB, 1MB, 10MB, 90MB]\n\n5. Regression Detection:\n   Store baseline results in benches/baselines/ as JSON.\n   CI comparison: flag if any benchmark regresses by > 10%.\n   Use criterion's built-in comparison: `cargo bench -- --baseline previous`\n","created_at":"2026-02-13T21:05:05Z"}]}
{"id":"bd-3w1.21","title":"Update facade crate re-exports for storage and durability APIs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:47.199892592Z","created_by":"ubuntu","updated_at":"2026-02-13T21:05:07.124841634Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","facade","frankensqlite","raptorq"],"dependencies":[{"issue_id":"bd-3w1.21","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T20:42:34.980116503Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:47.199892592Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1.13","type":"blocks","created_at":"2026-02-13T20:47:01.752611149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1.14","type":"blocks","created_at":"2026-02-13T20:42:34.857532702Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:47:01.839319603Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":74,"issue_id":"bd-3w1.21","author":"Dicklesworthstone","text":"TASK: Update the facade crate re-exports for storage and durability APIs.\n\nExtend the frankensearch facade crate (bd-3un.30) to re-export the new storage and durability public APIs, so consumers can access everything through a single import.\n\nUPDATED RE-EXPORTS:\n\n  // frankensearch/src/lib.rs\n\n  // ... existing re-exports from bd-3un.30 ...\n\n  // Storage (feature-gated)\n  #[cfg(feature = \"storage\")]\n  pub use frankensearch_storage::{\n      Storage, StorageConfig,\n      DocumentRecord, BatchResult,\n      PersistentJobQueue, JobQueueConfig, JobQueueMetrics,\n      ContentHasher, DeduplicationDecision,\n      IndexMetadata, StalenessCheck, StalenessReason,\n      StorageBackedJobRunner, IngestResult, IngestAction,\n  };\n\n  // FTS5 (feature-gated, requires storage)\n  #[cfg(feature = \"fts5\")]\n  pub use frankensearch_storage::{\n      Fts5LexicalIndex, Fts5Config, Fts5Tokenizer, Fts5ContentMode,\n  };\n\n  // Durability (feature-gated)\n  #[cfg(feature = \"durability\")]\n  pub use frankensearch_durability::{\n      RepairCodec, RepairCodecConfig, DurabilityMetrics,\n      FileProtector, FileProtectorConfig,\n      FsviProtector, ProtectionResult, RepairResult,\n      VerifyResult, FileHealth,\n  };\n\nERGONOMIC AUTO-CONFIGURATION:\n\n  The TwoTierSearcher::auto() constructor should detect available features:\n\n  impl TwoTierSearcher {\n      pub fn auto(data_dir: &Path) -> SearchResult<Self> {\n          // If 'storage' feature enabled:\n          //   Open FrankenSQLite database at data_dir/frankensearch.db\n          //   Use persistent job queue\n          //   Use storage-backed staleness detection\n          //\n          // If 'durability' feature enabled:\n          //   Create FileProtector with default config\n          //   Verify indices on load\n          //   Protect indices after build\n          //\n          // If 'fts5' feature enabled AND 'lexical' not enabled:\n          //   Use FTS5 as the lexical engine\n          //\n          // If both 'fts5' and 'lexical' enabled:\n          //   Use Tantivy as primary, FTS5 as fallback\n\n          // ... auto-detect embedders, build config ...\n      }\n  }\n\nDOCUMENTATION ADDITIONS:\n  The facade crate's doc comments should explain:\n  1. Feature flag selection guide (which features for which use case)\n  2. Storage: what it provides and when to use it\n  3. Durability: what it protects and the overhead cost\n  4. FTS5 vs Tantivy: trade-offs and when to use each\n\nFile: frankensearch/src/lib.rs (update to bd-3un.30)\n","created_at":"2026-02-13T20:46:20Z"},{"id":133,"issue_id":"bd-3w1.21","author":"Dicklesworthstone","text":"REVISION (review pass - facade integration verification):\n\n1. AUTO-CONFIGURATION IS POWERFUL: The TwoTierSearcher::auto(data_dir) constructor that detects available features at compile time is an excellent UX decision. This means the simplest consumer code is:\n   let searcher = TwoTierSearcher::auto(\"./data\")?;\n   let results = searcher.search(\"query\", 10)?;\n   Feature detection at compile time (not runtime) means zero-cost abstraction for disabled features.\n\n2. FTS5 + TANTIVY COEXISTENCE: The comment \"If both 'fts5' and 'lexical' enabled: Use Tantivy as primary, FTS5 as fallback\" needs clarification. What does \"fallback\" mean? Suggest:\n   a) Use Tantivy by default (better tokenization, custom analyzers)\n   b) FTS5 is available via an explicit config option: config.lexical_engine = LexicalEngine::Fts5\n   c) Do NOT automatically fall back at runtime -- that adds complexity and is hard to debug\n   The user should explicitly choose their lexical engine, with Tantivy as the default.\n\n3. RE-EXPORT COMPLETENESS: The re-export list should also include:\n   - StorageBackedStaleness, StalenessReport, StalenessConfig (from bd-3w1.12)\n   - DurabilityProvider trait (from bd-3w1.9 revision -- the no-op trait)\n   - PipelineMetrics, BatchProcessResult (from bd-3w1.13)\n   Check that every public type in frankensearch-storage and frankensearch-durability is either re-exported or deliberately private.\n\n4. BACKWARD COMPATIBILITY: Consumers of the existing frankensearch API (without storage/durability features) must NOT be affected by this change. The facade additions are all feature-gated, so the default feature set produces identical behavior. Verify with: cargo check --no-default-features (should compile).\n\n5. DOCUMENTATION SECTION: Add a module-level doc comment to frankensearch/src/lib.rs that lists all features and what they enable. This appears in rustdoc and is the first thing users see:\n   //! # Feature flags\n   //!\n   //! | Feature    | Description                          | Dependencies        |\n   //! |------------|--------------------------------------|---------------------|\n   //! | hash       | FNV-1a hash embedder (default)       | none                |\n   //! | model2vec  | potion-128M fast embedder            | safetensors, ...    |\n   //! | ...        | ...                                  | ...                 |\n   //! | storage    | FrankenSQLite document store          | fsqlite             |\n   //! | durability | RaptorQ self-healing indices          | fsqlite-core        |\n   //! | fts5       | FTS5 alternative lexical engine       | storage             |\n","created_at":"2026-02-13T21:01:41Z"},{"id":145,"issue_id":"bd-3w1.21","author":"Dicklesworthstone","text":"REVISION: Facade Re-Export Design Details\n\n1. Re-Export Strategy:\n   The facade crate (frankensearch/) re-exports public APIs from all sub-crates.\n   Storage and durability APIs follow the same pattern as existing re-exports:\n\n   #[cfg(feature = \"storage\")]\n   pub mod storage {\n       pub use frankensearch_storage::{DocumentStore, IndexMetadata, PersistentJobQueue};\n   }\n\n   #[cfg(feature = \"durability\")]\n   pub mod durability {\n       pub use frankensearch_durability::{RaptorQCodec, RepairTrailer, RepairResult};\n   }\n\n   #[cfg(feature = \"fts5\")]\n   pub mod fts5 {\n       pub use frankensearch_storage::fts5::{Fts5Engine, Fts5Config};\n   }\n\n2. Auto-Detection Updates:\n   The TwoTierSearcher::auto(data_dir) method (bd-3un.30) needs updates:\n   - With \"storage\" feature: auto-open FrankenSQLite DB at {data_dir}/frankensearch.db\n   - With \"durability\" feature: auto-verify .fec sidecars on index load\n   - With \"fts5\" feature: prefer FTS5 over Tantivy if both available (configurable)\n   - Without storage features: behavior unchanged (pure in-memory)\n\n3. Error Surface:\n   New error variants needed in SearchError (bd-3un.2):\n   - StorageError(String): FrankenSQLite operation failed\n   - RepairError(String): RaptorQ repair failed\n   - RepairUnavailable: .fec sidecars missing, cannot repair\n   - SchemaVersionMismatch { expected: u32, found: u32 }\n\n4. Documentation:\n   The facade's top-level doc comment must list ALL feature flags\n   including the new storage/durability ones, with one-line descriptions.\n   Feature flag interactions (implies, conflicts) documented in a table.\n\n5. Prelude Module:\n   Consider a frankensearch::prelude module that re-exports the most\n   commonly used types regardless of feature flags:\n   - TwoTierSearcher, TwoTierConfig, SearchError, ScoredResult\n   - With \"storage\": DocumentStore\n   - With \"durability\": RepairResult\n   This gives users a single `use frankensearch::prelude::*;` import.\n","created_at":"2026-02-13T21:05:07Z"}]}
{"id":"bd-3w1.3","title":"Implement persistent embedding job queue in FrankenSQLite","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:14.063817140Z","created_by":"ubuntu","updated_at":"2026-02-13T21:06:32.221064495Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","queue","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.3","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:14.063817140Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.3","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.716973335Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.3","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:28.700934728Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":56,"issue_id":"bd-3w1.3","author":"Dicklesworthstone","text":"TASK: Implement a persistent, crash-safe embedding job queue backed by FrankenSQLite.\n\nThis replaces the in-memory Mutex-based EmbeddingQueue (bd-3un.27) with a durable queue that survives process restarts and crashes. The in-memory queue from bd-3un.27 becomes a thin wrapper that delegates to this persistent backend.\n\nSQL SCHEMA:\n\n  CREATE TABLE IF NOT EXISTS embedding_jobs (\n      job_id INTEGER PRIMARY KEY AUTOINCREMENT,\n      doc_id TEXT NOT NULL,\n      embedder_id TEXT NOT NULL,        -- Which embedder to use\n      priority INTEGER NOT NULL DEFAULT 0,  -- Higher = process first\n      submitted_at INTEGER NOT NULL,    -- Unix timestamp millis\n      started_at INTEGER,               -- When a worker picked it up\n      completed_at INTEGER,             -- When processing finished\n      status TEXT NOT NULL DEFAULT 'pending',  -- pending | processing | completed | failed | skipped\n      retry_count INTEGER NOT NULL DEFAULT 0,\n      max_retries INTEGER NOT NULL DEFAULT 3,\n      error_message TEXT,\n      content_hash BLOB,               -- SHA-256 for dedup check\n      worker_id TEXT,                   -- Which worker claimed this job (for deadlock detection)\n      UNIQUE(doc_id, embedder_id, status)  -- Prevent duplicate pending jobs\n  );\n\n  CREATE INDEX idx_jobs_pending\n      ON embedding_jobs(status, priority DESC, submitted_at ASC)\n      WHERE status = 'pending';\n  CREATE INDEX idx_jobs_processing\n      ON embedding_jobs(status, started_at)\n      WHERE status = 'processing';\n\nQUEUE API:\n\n  pub struct PersistentJobQueue {\n      storage: Arc<Storage>,\n      config: JobQueueConfig,\n      metrics: Arc<JobQueueMetrics>,\n  }\n\n  pub struct JobQueueConfig {\n      pub batch_size: usize,                  // Default: 32\n      pub visibility_timeout_ms: u64,         // Default: 30_000 (30s)\n      pub max_retries: u32,                   // Default: 3\n      pub retry_base_delay_ms: u64,           // Default: 100\n      pub stale_job_threshold_ms: u64,        // Default: 300_000 (5min)\n      pub backpressure_threshold: usize,      // Default: 10_000 pending jobs\n  }\n\n  pub struct JobQueueMetrics {\n      pub total_enqueued: AtomicU64,\n      pub total_completed: AtomicU64,\n      pub total_failed: AtomicU64,\n      pub total_skipped: AtomicU64,\n      pub total_retried: AtomicU64,\n      pub total_deduplicated: AtomicU64,\n      pub total_batches_processed: AtomicU64,\n      pub total_embed_time_us: AtomicU64,\n  }\n\n  impl PersistentJobQueue {\n      /// Enqueue a document for embedding. Deduplicates by (doc_id, embedder_id).\n      /// If a pending job exists with same content_hash, returns Ok(false) (skipped).\n      /// If a pending job exists with different content_hash, replaces it.\n      pub fn enqueue(&self, doc_id: &str, embedder_id: &str, content_hash: &[u8; 32], priority: i32) -> SearchResult<bool>;\n\n      /// Enqueue a batch atomically (single transaction)\n      pub fn enqueue_batch(&self, jobs: &[EnqueueRequest]) -> SearchResult<BatchEnqueueResult>;\n\n      /// Claim a batch of pending jobs (atomic: sets status='processing', records worker_id)\n      /// Uses SELECT ... LIMIT batch_size with immediate status update in same transaction\n      pub fn claim_batch(&self, worker_id: &str, batch_size: usize) -> SearchResult<Vec<ClaimedJob>>;\n\n      /// Mark job as completed (sets status, completed_at)\n      pub fn complete(&self, job_id: i64) -> SearchResult<()>;\n\n      /// Mark job as failed (increments retry_count, requeues if under max_retries)\n      pub fn fail(&self, job_id: i64, error: &str) -> SearchResult<FailResult>;\n\n      /// Mark job as skipped (low-signal content after canonicalization)\n      pub fn skip(&self, job_id: i64, reason: &str) -> SearchResult<()>;\n\n      /// Reclaim stale 'processing' jobs (worker died without completing)\n      /// Jobs processing for > visibility_timeout_ms get reset to 'pending'\n      pub fn reclaim_stale_jobs(&self) -> SearchResult<usize>;\n\n      /// Check backpressure (returns true if queue depth exceeds threshold)\n      pub fn is_backpressured(&self) -> SearchResult<bool>;\n\n      /// Get queue depth by status\n      pub fn queue_depth(&self) -> SearchResult<QueueDepth>;\n\n      /// Get metrics snapshot\n      pub fn metrics(&self) -> &JobQueueMetrics;\n  }\n\nVISIBILITY TIMEOUT PATTERN (from SQS/cloud queues):\nWhen a worker claims a job, it becomes invisible to other workers for visibility_timeout_ms.\nIf the worker dies without completing/failing the job, reclaim_stale_jobs() resets it to pending.\nThis prevents lost work without requiring distributed locks.\n\nclaim_batch() SQL:\n  UPDATE embedding_jobs\n  SET status = 'processing', started_at = ?now, worker_id = ?worker\n  WHERE job_id IN (\n      SELECT job_id FROM embedding_jobs\n      WHERE status = 'pending'\n      ORDER BY priority DESC, submitted_at ASC\n      LIMIT ?batch_size\n  )\n  RETURNING job_id, doc_id, embedder_id;\n\nThis is atomic in FrankenSQLite -- MVCC ensures concurrent workers get disjoint batches.\n\nEXPONENTIAL BACKOFF ON FAILURE:\n  next_retry_delay = min(retry_base_delay_ms * 2^retry_count, 30_000ms)\n  Jobs that exceed max_retries stay in 'failed' status for manual inspection.\n  From agent-mail embedding_jobs.rs: backoff shift cap at 20 (2^20 * 100ms = ~105s max).\n\nHASH-ONLY SKIP (from agent-mail):\n  When embedder_id is the hash embedder (\"fnv1a-*\"), skip the job entirely.\n  Hash embeddings are computed on-the-fly during search and don't need to be stored.\n  This prevents wasted I/O for the always-available fallback embedder.\n\nDEDUP LOGIC:\n  On enqueue, check if (doc_id, embedder_id) already has a pending/processing job:\n  - Same content_hash: skip (content hasn't changed)\n  - Different content_hash: cancel old job, create new one\n  - No existing job: create new one\n  This integrates with the content_hash dedup from bd-3w1.4.\n\nWHY PERSISTENT QUEUE OVER IN-MEMORY:\n1. Crash recovery: pending jobs survive process restart\n2. Multi-process: multiple workers can claim from same queue via MVCC\n3. Observability: queue depth, failure rates queryable via SQL\n4. Backpressure: count(*) where status='pending' is O(1) with partial index\n5. Dedup: UNIQUE constraint prevents duplicate work at the database level\n\nFile: frankensearch-storage/src/job_queue.rs\n","created_at":"2026-02-13T20:46:09Z"},{"id":77,"issue_id":"bd-3w1.3","author":"Dicklesworthstone","text":"RELATIONSHIP NOTE: Persistent vs In-Memory Job Queue\n\nThis bead (bd-3w1.3) extends the in-memory embedding job queue from bd-3un.27\nwith FrankenSQLite-backed persistence. The relationship is:\n\n- bd-3un.27: In-memory queue with crossbeam channels, backpressure, AIMD rate control\n  - Jobs lost on process crash\n  - Sufficient for real-time indexing of small batches\n\n- bd-3w1.3: Persistent queue backed by FrankenSQLite\n  - Jobs survive process restarts\n  - Supports bulk import (100K+ docs)\n  - Crash recovery: resume from last committed position\n  - Required for production deployments\n\nImplementation approach: bd-3w1.3 should implement the SAME trait interface as\nbd-3un.27 (trait EmbeddingJobQueue) but with SQLite storage. The refresh worker\n(bd-3un.28) should accept any impl EmbeddingJobQueue, making storage backend\nswappable via feature flags.\n","created_at":"2026-02-13T20:46:28Z"},{"id":115,"issue_id":"bd-3w1.3","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. NO RETURNING CLAUSE: The bead's claim_batch() SQL uses RETURNING, which may not be supported in FrankenSQLite yet (it's a SQLite 3.35+ feature, and FrankenSQLite is a clean-room reimplementation). Alternative approach:\n   -- Step 1: SELECT job_ids to claim\n   SELECT job_id FROM embedding_jobs WHERE status = 'pending'\n   ORDER BY priority DESC, submitted_at ASC LIMIT ?batch_size;\n   -- Step 2: UPDATE those specific IDs\n   UPDATE embedding_jobs SET status = 'processing', started_at = ?now, worker_id = ?worker\n   WHERE job_id IN (?, ?, ...);\n   Both within the same transaction (BEGIN CONCURRENT).\n\n2. SQL PARAMETER BINDING: All parameterized queries must use SqliteValue:\n   conn.execute_with_params(\n       \"INSERT INTO embedding_jobs (doc_id, embedder_id, priority, submitted_at, status) VALUES (?, ?, ?, ?, 'pending')\",\n       &[SqliteValue::Text(doc_id.into()), SqliteValue::Text(embedder_id.into()),\n         SqliteValue::Integer(priority as i64), SqliteValue::Integer(now_ms)]\n   )?;\n\n3. RELATIONSHIP TO IN-MEMORY QUEUE (bd-3un.27): The in-memory EmbeddingQueue from bd-3un.27 should become a THIN WRAPPER around PersistentJobQueue. When the 'storage' feature is disabled, bd-3un.27's in-memory implementation is used directly. When 'storage' is enabled, bd-3un.27 delegates to PersistentJobQueue. This is a compile-time feature switch, not runtime.\n\n   #[cfg(feature = \"storage\")]\n   pub type JobQueue = PersistentJobQueue;\n   #[cfg(not(feature = \"storage\"))]\n   pub type JobQueue = InMemoryJobQueue;\n\n4. BEGIN CONCURRENT for claim_batch(): Use \"BEGIN CONCURRENT\" instead of plain \"BEGIN\" to leverage FrankenSQLite's MVCC. This allows multiple workers to claim batches simultaneously without serializing on the transaction lock. Each worker's claim_batch() runs in a concurrent transaction with page-level locking.\n","created_at":"2026-02-13T20:58:07Z"},{"id":159,"issue_id":"bd-3w1.3","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MODERATE REVISION (persistent job queue):\n\nbd-3w1.3 implements a persistent job queue in FrankenSQLite that replaces bd-3un.27's in-memory queue. The asupersync migration applies to both:\n\nBEFORE:\n  - References \"crossbeam channels\" from bd-3un.27\n  - Visibility timeout pattern with manual timer\n  - MVCC concurrent workers\n\nAFTER:\n  - Worker coordination via asupersync::channel::mpsc (for signaling between queue and workers)\n  - Visibility timeout via asupersync::combinator::timeout (cancel-correct)\n  - Worker pool via asupersync::sync::Pool or cx.region with multiple scope.spawn()\n  - MVCC concurrent access is FrankenSQLite's domain (not asupersync's)\n\n  pub struct PersistentJobQueue {\n      db: FrankenSqliteConnection,\n      notify: asupersync::sync::Notify,  // Signal when new jobs arrive\n  }\n\n  impl PersistentJobQueue {\n      pub async fn dequeue(&self, cx: &Cx) -> asupersync::Result<Option<EmbeddingJob>> {\n          loop {\n              // Check for available jobs\n              if let Some(job) = self.try_claim_job(cx).await? {\n                  return Ok(Some(job));\n              }\n              // Wait for notification (cancel-aware)\n              self.notify.notified(cx).await?;\n          }\n      }\n\n      pub async fn enqueue(&self, cx: &Cx, job: EmbeddingJob) -> asupersync::Result<()> {\n          self.db.execute(cx, INSERT_SQL, &job).await?;\n          self.notify.notify_one();  // Wake waiting worker\n          Ok(())\n      }\n  }","created_at":"2026-02-13T21:06:32Z"}]}
{"id":"bd-3w1.4","title":"Implement content-hash deduplication layer","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:15.412628486Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:07.979859967Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["dedup","frankensqlite","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.4","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T20:46:27.305310381Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.4","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:15.412628486Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.4","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.835698874Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.4","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:28.773830125Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":57,"issue_id":"bd-3w1.4","author":"Dicklesworthstone","text":"TASK: Implement content-hash based deduplication for embedding pipeline.\n\nDocuments are identified by (doc_id, content_hash) where content_hash = SHA-256 of the canonicalized text. This layer prevents re-embedding when content hasn't changed, and detects when content has changed and needs re-embedding.\n\nDESIGN:\n\n  pub struct ContentHasher {\n      // Stateless -- just wraps SHA-256\n  }\n\n  impl ContentHasher {\n      /// Compute SHA-256 of canonicalized text\n      pub fn hash(canonical_text: &str) -> [u8; 32];\n\n      /// Compare two hashes\n      pub fn matches(a: &[u8; 32], b: &[u8; 32]) -> bool;\n  }\n\n  pub enum DeduplicationDecision {\n      /// Content unchanged, skip embedding entirely\n      Skip { doc_id: String, reason: &'static str },\n      /// New document, needs embedding\n      New { doc_id: String },\n      /// Content changed, needs re-embedding (old embedding invalidated)\n      Changed { doc_id: String, old_hash: [u8; 32], new_hash: [u8; 32] },\n  }\n\n  impl Storage {\n      /// Check if a document needs (re-)embedding by comparing content hashes\n      pub fn check_dedup(&self, doc_id: &str, new_hash: &[u8; 32], embedder_id: &str) -> SearchResult<DeduplicationDecision>;\n\n      /// Batch check for dedup (single query, much faster than N individual checks)\n      pub fn check_dedup_batch(&self, items: &[(String, [u8; 32])], embedder_id: &str) -> SearchResult<Vec<DeduplicationDecision>>;\n  }\n\nINTEGRATION WITH CANONICALIZATION (bd-3un.42):\n  The dedup pipeline is:\n  1. Canonicalize raw text (NFC, markdown strip, code collapse, whitespace normalize)\n  2. If canonical text is empty -> skip (low-signal content)\n  3. Compute SHA-256 of canonical text\n  4. Check dedup against storage: skip / new / changed\n  5. Only if new/changed: actually embed the text\n\n  This means content_hash is computed AFTER canonicalization but BEFORE embedding.\n  The hash represents the exact text that was embedded, so identical canonical forms\n  always produce the same hash regardless of original formatting.\n\nBATCH DEDUP SQL:\n  -- Check which doc_ids need embedding\n  SELECT d.doc_id, d.content_hash, es.status\n  FROM documents d\n  LEFT JOIN embedding_status es ON d.doc_id = es.doc_id AND es.embedder_id = ?embedder\n  WHERE d.doc_id IN (?, ?, ?, ...)\n  -- Then compare content_hash in application code\n\nPERFORMANCE:\n  - SHA-256: ~400MB/s on modern CPUs (sha2 crate with hardware acceleration)\n  - For 10K documents with 2KB average canonical text: ~50ms total hash time\n  - Batch SQL query: single round-trip regardless of batch size\n  - Net savings: skip re-embedding unchanged docs (128ms per MiniLM embed)\n\nWHY SHA-256 (not xxhash or FNV):\n  - Collision resistance matters here: two different texts producing the same hash\n    means we'd skip embedding, silently serving stale results\n  - SHA-256 has 128-bit collision resistance (birthday bound), vs 32-bit for CRC32\n  - Performance is not the bottleneck: hashing is 1000x faster than embedding\n  - Consistency with FrankenSQLite: ECS objects use BLAKE3 for content addressing,\n    SHA-256 is similarly cryptographic-grade\n\nFile: frankensearch-storage/src/content_hash.rs\n","created_at":"2026-02-13T20:46:09Z"},{"id":76,"issue_id":"bd-3w1.4","author":"Dicklesworthstone","text":"DEPENDENCY NOTE: Content Hash and Canonicalization\n\nbd-3w1.4 (content-hash deduplication) should use the same SHA-256 content hashing\nas bd-3un.42 (text canonicalization pipeline). The canonicalization must happen\nBEFORE hashing for dedup to work correctly:\n\n  canonical_text = canonicalize(raw_text)\n  content_hash = sha256(canonical_text)\n\nWithout canonicalization first, trivially different versions of the same content\n(different whitespace, markdown formatting, etc.) would produce different hashes\nand bypass dedup.\n\nThe dependency bd-3w1.4 -> bd-3un.42 ensures the canonicalization pipeline is\navailable when the dedup layer is implemented.\n","created_at":"2026-02-13T20:46:28Z"},{"id":116,"issue_id":"bd-3w1.4","author":"Dicklesworthstone","text":"REVISION (review pass - cross-codebase verification):\n\n1. CANONICALIZATION-BEFORE-HASHING REQUIREMENT: The content hash MUST be computed on the canonicalized text, not the raw input. The pipeline is:\n   raw_text -> canonicalize() -> canonical_text -> SHA-256 -> content_hash\n   This ensures that formatting-only changes (extra whitespace, markdown formatting differences) don't trigger unnecessary re-embedding. Two documents with different raw text but identical canonical forms should produce the same hash.\n\n2. HASH CRATE: Use sha2::Sha256 (the sha2 crate), which has hardware acceleration on x86_64 (SHA-NI extension) and ARM (SHA2 extension). Performance: ~2GB/s on modern CPUs with hardware support, ~400MB/s without. This is more than sufficient since hashing time is dominated by the much slower embedding step.\n\n3. EMPTY CANONICAL TEXT: If canonicalization produces an empty string (low-signal content like \"OK\", \"Done\", etc.), the DeduplicationDecision should be Skip, and no content_hash should be computed or stored. Empty text is not a valid state for the content_hash field.\n","created_at":"2026-02-13T20:58:07Z"}]}
{"id":"bd-3w1.5","title":"Add frankensearch-durability crate for RaptorQ integration","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:16.305906544Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:09.355727460Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","scaffold","tier2"],"dependencies":[{"issue_id":"bd-3w1.5","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T20:46:30.065403534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.5","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:46:30.156928715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.5","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:16.305906544Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":58,"issue_id":"bd-3w1.5","author":"Dicklesworthstone","text":"TASK: Create the frankensearch-durability sub-crate.\n\nThis crate provides the RaptorQ integration layer for self-healing search indices. It wraps FrankenSQLite's SymbolCodec trait and provides high-level APIs for adding repair symbols to arbitrary binary files (FSVI, Tantivy segments, etc.).\n\nCRATE STRUCTURE:\n  crates/frankensearch-durability/\n    Cargo.toml\n    src/\n      lib.rs              -- Public API re-exports\n      codec.rs            -- RaptorQ codec wrapper (wraps fsqlite-core SymbolCodec)\n      repair_trailer.rs   -- Binary trailer format for repair symbols appended to files\n      file_protector.rs   -- High-level API: protect/verify/repair a file\n      tantivy_wrapper.rs  -- Tantivy-specific segment protection\n      metrics.rs          -- Durability metrics (encode/decode counts, repair events)\n      config.rs           -- Durability configuration\n\nCARGO.TOML DEPENDENCIES:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite-core = { path = \"/dp/frankensqlite/crates/fsqlite-core\" }  # For SymbolCodec, RaptorQMetrics\n  xxhash-rust = { version = \"0.8\", features = [\"xxh3\"] }  # Fast checksums for corruption detection\n  crc32fast = \"1.4\"                     # CRC-32 for header validation\n  tracing = \"0.1\"\n  serde = { version = \"1\", features = [\"derive\"] }\n\nFEATURE FLAG:\n  The durability crate is feature-gated:\n  [features]\n  durability = [\"dep:frankensearch-durability\"]\n\n  Consumers who don't want erasure coding overhead can omit this feature.\n  When disabled, all durability operations become no-ops (trait provides defaults).\n\nDESIGN RATIONALE:\n  We reuse FrankenSQLite's SymbolCodec trait rather than implementing our own RaptorQ.\n  This gives us:\n  1. Battle-tested encode/decode implementation\n  2. Deterministic repair symbols (same seed = same symbols)\n  3. Configurable overhead (DEFAULT_OVERHEAD_PERCENT = 20%)\n  4. Metrics integration (RaptorQMetrics with atomic counters)\n  5. Proper error handling (DecodeFailureReason enum)\n\n  The durability crate adds a layer ON TOP of SymbolCodec:\n  - RepairTrailer: binary format for appending repair symbols to existing files\n  - FileProtector: high-level protect/verify/repair workflow\n  - TantivyWrapper: hooks into Tantivy's segment lifecycle\n\nKEY INSIGHT -- WHY THIS IS DIFFERENT FROM TRADITIONAL CHECKSUMS:\n  CRC32/SHA-256 can DETECT corruption but cannot REPAIR it.\n  RaptorQ repair symbols can both detect AND repair:\n  - With R=2 repair symbols and K source symbols, any 2 corrupted source symbols can be recovered\n  - With 20% overhead (R = ceil(K * 0.2)), up to 20% of the file can be corrupted and recovered\n  - Recovery is information-theoretically optimal (approaches Shannon limit)\n  - No coordination needed: repair symbols are deterministic from the source data\n\n  For a 73MB vector index (100K docs x 384d x f16), 20% overhead = 14.6MB of repair symbols.\n  This is stored in a sidecar file (.fsvi.fec) to avoid modifying the main index format.\n\nFile: crates/frankensearch-durability/src/lib.rs\n","created_at":"2026-02-13T20:46:10Z"},{"id":117,"issue_id":"bd-3w1.5","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. DEPENDENCY ON asupersync: The actual RaptorQ encoder/decoder lives in /dp/asupersync (a sibling project), not directly in fsqlite-core. fsqlite-core provides the SymbolCodec TRAIT, but the concrete implementation wrapping the actual fountain code math is in asupersync. The durability crate has two options:\n   a) Depend on fsqlite-core for the SymbolCodec trait + asupersync for the impl (2 deps)\n   b) Depend only on fsqlite-core which re-exports asupersync's codec (1 dep, if re-exported)\n   Check: does fsqlite-core re-export the concrete codec? If not, we need asupersync directly.\n\n   Updated Cargo.toml:\n   [dependencies]\n   fsqlite-core = { path = \"/dp/frankensqlite/crates/fsqlite-core\" }\n   asupersync = { path = \"/dp/asupersync\", optional = true }  # For concrete SymbolCodec impl\n\n2. PageSymbolSink AND PageSymbolSource TRAITS: In addition to SymbolCodec, fsqlite-core also exposes PageSymbolSink and PageSymbolSource traits for streaming symbol I/O. For our use case (protecting files), we likely want to:\n   - Implement PageSymbolSink for writing repair symbols to the .fec sidecar\n   - Implement PageSymbolSource for reading repair symbols during decode\n   This is more efficient than loading all symbols into memory at once.\n\n3. RepairConfig IS CONST-CONSTRUCTIBLE: All creation methods are const fn. Use this:\n   const DEFAULT_REPAIR_CONFIG: RepairCodecConfig = RepairCodecConfig {\n       symbol_size: 4096,\n       overhead_percent: 20,\n       max_repair_symbols: 250_000,\n       slack_decode: 2,\n   };\n   This enables compile-time validation and zero-cost construction.\n\n4. RaptorQMetrics SINGLETON: FrankenSQLite maintains a global GLOBAL_RAPTORQ_METRICS atomic counter set. Our DurabilityMetrics should be SEPARATE (not sharing the global), because we want to track frankensearch-specific repair events independently of any FrankenSQLite-internal operations. The global metrics are always active and can't be disabled.\n","created_at":"2026-02-13T20:58:09Z"}]}
{"id":"bd-3w1.6","title":"Implement RaptorQ repair symbol codec wrapper","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:22.195438810Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:10.550661778Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["codec","durability","raptorq","tier2"],"dependencies":[{"issue_id":"bd-3w1.6","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:22.195438810Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.6","depends_on_id":"bd-3w1.5","type":"blocks","created_at":"2026-02-13T20:42:27.509866322Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":59,"issue_id":"bd-3w1.6","author":"Dicklesworthstone","text":"TASK: Implement the RaptorQ repair symbol codec wrapper.\n\nThis wraps FrankenSQLite's SymbolCodec trait with a frankensearch-specific API that's optimized for our use case: protecting binary files (vector indices, segment files) rather than database pages.\n\nCODEC WRAPPER API:\n\n  pub struct RepairCodecConfig {\n      pub symbol_size: u32,            // Default: 4096 (4KB, matching typical page size)\n      pub overhead_percent: u32,       // Default: 20 (20% extra repair symbols)\n      pub max_repair_symbols: u32,     // Default: 250_000 (anti-footgun guardrail)\n      pub slack_decode: u32,           // Default: 2 (RFC 6330 Annex B: K+2 for negligible failure)\n  }\n\n  pub struct RepairCodec {\n      inner: Box<dyn SymbolCodec>,     // FrankenSQLite's codec implementation\n      config: RepairCodecConfig,\n      metrics: Arc<DurabilityMetrics>,\n  }\n\n  pub struct DurabilityMetrics {\n      pub files_protected: AtomicU64,\n      pub files_verified: AtomicU64,\n      pub files_repaired: AtomicU64,\n      pub repair_failures: AtomicU64,\n      pub total_source_bytes: AtomicU64,\n      pub total_repair_bytes: AtomicU64,\n      pub total_encode_time_us: AtomicU64,\n      pub total_decode_time_us: AtomicU64,\n      pub corruption_events_detected: AtomicU64,\n  }\n\n  impl RepairCodec {\n      pub fn new(config: RepairCodecConfig) -> SearchResult<Self>;\n\n      /// Encode source data into source + repair symbols\n      pub fn encode(&self, source_data: &[u8]) -> SearchResult<EncodedData>;\n\n      /// Verify source data integrity using repair symbols\n      /// Returns Ok(true) if intact, Ok(false) if corrupted but repairable\n      pub fn verify(&self, source_data: &[u8], repair_data: &RepairData) -> SearchResult<VerifyResult>;\n\n      /// Attempt to repair corrupted source data using repair symbols\n      pub fn repair(&self, corrupted_data: &[u8], repair_data: &RepairData) -> SearchResult<Vec<u8>>;\n\n      /// Compute deterministic repair symbols for a given source\n      /// Determinism: same source_data + same config = identical repair symbols every time\n      /// Seed derivation: xxh3_64(source_data) (matching FrankenSQLite's approach)\n      pub fn compute_repair_symbols(&self, source_data: &[u8]) -> SearchResult<RepairData>;\n  }\n\n  pub struct EncodedData {\n      pub source_symbols: Vec<(u32, Vec<u8>)>,  // (ESI, data)\n      pub repair_symbols: Vec<(u32, Vec<u8>)>,  // (ESI, data)\n      pub k_source: u32,\n      pub symbol_size: u32,\n  }\n\n  pub struct RepairData {\n      pub repair_symbols: Vec<(u32, Vec<u8>)>,\n      pub k_source: u32,\n      pub symbol_size: u32,\n      pub source_hash: [u8; 8],         // xxh3_64 of original source (for verification)\n  }\n\n  pub enum VerifyResult {\n      Intact,\n      Corrupted { corrupted_symbols: usize, repairable: bool },\n  }\n\nREPAIR BUDGET FORMULA (from FrankenSQLite repair_symbols.rs):\n  slack_decode = 2\n  R_formula = max(slack_decode, ceil(K_source * overhead_percent / 100))\n  R = min(max_repair_symbols, R_formula)\n\n  For a 73MB FSVI index with 4KB symbols:\n  K_source = 73MB / 4KB = 18,688 source symbols\n  R = max(2, ceil(18688 * 0.20)) = 3,738 repair symbols\n  Repair data size = 3,738 * 4KB = ~14.6MB\n\nDETERMINISTIC REPAIR (from FrankenSQLite):\n  The repair symbols are deterministic: given the same source data and config, the same\n  repair symbols are always generated. This is achieved by deriving the RaptorQ random seed\n  from xxh3_64(source_data). Benefits:\n  1. Verification without original: compare generated repair symbols against stored ones\n  2. Incremental repair: regenerate specific missing symbols on demand\n  3. Idempotent writes: writing the same repair symbols twice is harmless\n  4. Cross-replica consistency: any node generates identical symbols\n\nCORRUPTION DETECTION:\n  Before invoking the full decode pipeline (expensive), do a quick integrity check:\n  1. xxh3_64 of source data vs stored hash: if match, file is intact (fast path)\n  2. Per-symbol CRC32: identify which symbols are corrupted (medium path)\n  3. Full RaptorQ decode: reconstruct from surviving + repair symbols (slow path)\n\n  The fast path (xxh3_64 comparison) takes < 1ms for a 73MB file.\n  Full decode only happens when corruption is detected.\n\nFile: frankensearch-durability/src/codec.rs\n","created_at":"2026-02-13T20:46:10Z"},{"id":118,"issue_id":"bd-3w1.6","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. SymbolCodec LOCATION CONFIRMED: The trait is in fsqlite_core::raptorq_integration (line 334). The beads correctly reference this. The encode() and decode() signatures match exactly:\n   fn encode(&self, source_data: &[u8], symbol_size: u32, repair_overhead: f64) -> Result<CodecEncodeResult>;\n   fn decode(&self, symbols: &[(u32, Vec<u8>)], k_source: u32, symbol_size: u32) -> Result<CodecDecodeResult>;\n\n2. CodecDecodeResult IS AN ENUM, NOT STRUCT: The decode result has two variants:\n   CodecDecodeResult::Success { data, symbols_used, peeled_count, inactivated_count }\n   CodecDecodeResult::Failure { reason: DecodeFailureReason, symbols_received, k_required }\n\n   The bead's repair() method should match on this enum, not assume success. Update:\n   match self.inner.decode(symbols, k_source, symbol_size)? {\n       CodecDecodeResult::Success { data, .. } => Ok(data),\n       CodecDecodeResult::Failure { reason, .. } => Err(SearchError::RepairFailed { reason: format!(\"{reason:?}\") }),\n   }\n\n3. REPAIR SEED DERIVATION: The bead says \"seed = xxh3_64(source_data)\". The actual FrankenSQLite API uses:\n   pub fn derive_repair_seed(object_id: &ObjectId) -> u64\n   This derives from ObjectId (16-byte BLAKE3 truncation), NOT from raw source data. For frankensearch, since we don't have ECS ObjectIds, we should compute:\n   seed = xxh3_64(source_data_bytes)\n   This is consistent with the bead's approach but uses a DIFFERENT derivation than FrankenSQLite's. Document this difference.\n\n4. REPAIR BUDGET CALCULATION: Use the existing API:\n   pub fn select_repair_count(k_source: u32, overhead_percent: u32) -> u32\n   This is simpler than reimplementing the formula. The RepairConfig and RepairBudget types are also available for more complex scenarios (object-class-specific budgets).\n\n5. PageSymbolSink/PageSymbolSource: For streaming I/O (large indices), implement these traits instead of loading all symbols into memory. The Sink writes symbols to the .fec file incrementally; the Source reads them back on demand. This keeps memory usage bounded regardless of index size.\n\n   pub struct FecFileSink {\n       writer: BufWriter<File>,\n       written: u32,\n   }\n   impl PageSymbolSink for FecFileSink {\n       fn write_symbol(&mut self, esi: u32, data: &[u8]) -> Result<()> { ... }\n       fn flush(&mut self) -> Result<()> { ... }\n       fn written_count(&self) -> u32 { self.written }\n   }\n","created_at":"2026-02-13T20:58:10Z"}]}
{"id":"bd-3w1.7","title":"Add RaptorQ repair trailer to FSVI vector index format","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:23.247430442Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:11.340885101Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","tier2","vector-index"],"dependencies":[{"issue_id":"bd-3w1.7","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T20:42:27.748405998Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.7","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:23.247430442Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.7","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:27.630468374Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":60,"issue_id":"bd-3w1.7","author":"Dicklesworthstone","text":"TASK: Add RaptorQ repair trailer format to FSVI vector index files.\n\nThis extends the FSVI format (bd-3un.13) with an optional sidecar file containing RaptorQ repair symbols. The main FSVI file is unchanged (backwards compatible), and the repair data lives in a parallel .fsvi.fec file.\n\nSIDECAR FILE FORMAT (.fsvi.fec):\n\n  Header (32 bytes, little-endian):\n    Offset  Size  Field\n    0       4     magic: \"FECR\" (FrankenSearch Erasure Code Repair)\n    4       2     version: u16 (start at 1)\n    6       2     reserved: u16\n    8       4     symbol_size: u32 (bytes per symbol, e.g., 4096)\n    12      4     k_source: u32 (number of source symbols)\n    16      4     r_repair: u32 (number of repair symbols)\n    20      4     overhead_percent: u32 (for documentation/verification)\n    24      8     source_hash: u64 (xxh3_64 of the protected file)\n\n  Repair Symbol Table (r_repair entries, each symbol_size bytes):\n    [symbol_0: symbol_size bytes]\n    [symbol_1: symbol_size bytes]\n    ...\n    [symbol_{r_repair-1}: symbol_size bytes]\n\n  Footer (8 bytes):\n    0       4     trailer_crc32: u32 (CRC32 of entire file excluding this footer)\n    4       4     magic_end: \"RCEF\" (reverse magic, end-of-file marker)\n\nFILE NAMING CONVENTION:\n  vector.fast.fsvi     -> vector.fast.fsvi.fec\n  vector.quality.fsvi  -> vector.quality.fsvi.fec\n\nPROTECTION WORKFLOW:\n\n  pub struct FsviProtector {\n      codec: RepairCodec,\n  }\n\n  impl FsviProtector {\n      /// Generate repair symbols for an FSVI file and write the .fec sidecar\n      pub fn protect(&self, fsvi_path: &Path) -> SearchResult<ProtectionResult>;\n\n      /// Verify FSVI file integrity using the .fec sidecar\n      /// Returns VerifyResult::Intact if file matches source_hash\n      /// Returns VerifyResult::Corrupted with repair info if damaged\n      pub fn verify(&self, fsvi_path: &Path) -> SearchResult<VerifyResult>;\n\n      /// Attempt to repair a corrupted FSVI file using the .fec sidecar\n      /// On success, overwrites the corrupted file with repaired data\n      /// On failure, returns error (corruption exceeds repair capacity)\n      pub fn repair(&self, fsvi_path: &Path) -> SearchResult<RepairResult>;\n\n      /// Atomic protect: write .fec to temp file, then rename (crash-safe)\n      pub fn protect_atomic(&self, fsvi_path: &Path) -> SearchResult<ProtectionResult>;\n  }\n\n  pub struct ProtectionResult {\n      pub source_size: u64,\n      pub repair_size: u64,\n      pub overhead_ratio: f32,\n      pub k_source: u32,\n      pub r_repair: u32,\n      pub encode_time: Duration,\n  }\n\n  pub struct RepairResult {\n      pub bytes_corrupted: usize,\n      pub symbols_repaired: usize,\n      pub decode_time: Duration,\n      pub source_hash_before: u64,  // xxh3_64 of corrupted data\n      pub source_hash_after: u64,   // xxh3_64 of repaired data (should match original)\n  }\n\nINTEGRATION WITH VECTOR INDEX WRITER (bd-3un.13):\n  VectorIndexWriter::finish() should optionally call FsviProtector::protect_atomic()\n  when the 'durability' feature is enabled. The protection happens AFTER fsync of the\n  main index file, ensuring the repair symbols cover the durable version.\n\n  Updated finish() flow:\n  1. Write all records to FSVI file\n  2. fsync the FSVI file\n  3. fsync the parent directory\n  4. If durability feature enabled:\n     a. Compute repair symbols from the fsynced file\n     b. Write .fec sidecar (atomic: temp + rename)\n     c. fsync the .fec file and parent directory\n     d. Log protection result\n\nAUTOMATIC REPAIR ON LOAD:\n  VectorIndex::open() should optionally verify integrity and attempt repair:\n\n  impl VectorIndex {\n      pub fn open(path: &Path) -> SearchResult<Self> {\n          // ... existing load logic ...\n\n          #[cfg(feature = \"durability\")]\n          if let Some(protector) = FsviProtector::try_new() {\n              match protector.verify(path)? {\n                  VerifyResult::Intact => { /* fast path, < 1ms */ }\n                  VerifyResult::Corrupted { repairable: true, .. } => {\n                      tracing::warn!(\"vector index corrupted, attempting repair\");\n                      protector.repair(path)?;\n                      tracing::info!(\"vector index repaired successfully\");\n                      // Re-open from repaired file\n                  }\n                  VerifyResult::Corrupted { repairable: false, .. } => {\n                      tracing::error!(\"vector index corrupted beyond repair capacity\");\n                      return Err(SearchError::IndexCorrupted { path: path.to_owned() });\n                  }\n              }\n          }\n      }\n  }\n\nEXAMPLE SIZES:\n  | Index Size | Symbols (4KB) | Repair (20%) | .fec Size | Overhead |\n  |-----------|---------------|--------------|-----------|----------|\n  | 7.3MB     | 1,869         | 374          | 1.5MB     | 20.5%    |\n  | 73MB      | 18,688        | 3,738        | 14.6MB    | 20.0%    |\n  | 730MB     | 186,880       | 37,376       | 146MB     | 20.0%    |\n\nFile: frankensearch-durability/src/repair_trailer.rs + integration in frankensearch-index\n","created_at":"2026-02-13T20:46:10Z"},{"id":119,"issue_id":"bd-3w1.7","author":"Dicklesworthstone","text":"REVISION (review pass - FrankenSQLite API verification):\n\n1. AUTH TAG HANDLING: FrankenSQLite's SymbolRecord always includes a 16-byte auth_tag field (zeroed when auth is disabled). Our .fec sidecar format does NOT need auth tags since we're protecting local files, not transmitting over networks. The sidecar format is simpler than SymbolRecord: just raw repair symbol data without the ECS envelope. This is correct as designed.\n\n2. SYMBOL SIZE ALIGNMENT: The symbol_size must be compatible with RaptorQ requirements. From the PipelineConfig in raptorq_integration.rs, symbol_size should be between 512 and 65536 bytes and ideally a power of 2. Our default of 4096 is correct and matches typical filesystem page size.\n\n3. STREAMING ENCODE FOR LARGE FILES: For indices > 100MB, loading the entire file into memory for encoding is wasteful. Use memory-mapped I/O:\n   - mmap the FSVI file (read-only)\n   - Pass mmap slice to SymbolCodec::encode()\n   - Write repair symbols to .fec via PageSymbolSink\n   This keeps memory usage at O(symbol_size * R) rather than O(file_size + R * symbol_size).\n\n4. VERIFICATION FAST PATH: The xxh3_64 hash in the .fec header enables a O(1) integrity check without decoding any repair symbols. On a 73MB file, xxh3_64 computation takes ~10ms (limited by memory bandwidth, not CPU). This fast path should be the default verification mode, with full symbol-level verification only when corruption is suspected.\n\n5. FSVI FORMAT BACKWARD COMPATIBILITY: The .fec sidecar is a SEPARATE file that doesn't modify the FSVI format. Consumers without the 'durability' feature simply won't have .fec files. The FSVI format remains unchanged. This is a key design principle: durability is additive, not intrusive.\n","created_at":"2026-02-13T20:58:11Z"}]}
{"id":"bd-3w1.8","title":"Implement self-healing Tantivy segment wrapper with RaptorQ","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:23.916608544Z","created_by":"ubuntu","updated_at":"2026-02-13T20:58:12.449454900Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","tantivy","tier2"],"dependencies":[{"issue_id":"bd-3w1.8","depends_on_id":"bd-3un.17","type":"blocks","created_at":"2026-02-13T20:42:27.986993183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.8","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:23.916608544Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.8","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:27.866126756Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":61,"issue_id":"bd-3w1.8","author":"Dicklesworthstone","text":"TASK: Implement self-healing Tantivy segment wrapper with RaptorQ repair symbols.\n\nTantivy stores its index as multiple segment files. Each segment is a set of files (postings, positions, terms, store, fast fields). This task wraps Tantivy's segment lifecycle to add RaptorQ protection.\n\nCHALLENGE:\n  Tantivy manages its own files via the Directory trait. We can't simply append repair\n  symbols to Tantivy's files because Tantivy's merge process creates/deletes segments.\n  Instead, we hook into the segment lifecycle and protect completed segments.\n\nAPPROACH -- SEGMENT LIFECYCLE HOOKS:\n\n  pub struct DurableTantivyIndex {\n      inner: tantivy::Index,\n      protector: Arc<FileProtector>,\n      data_dir: PathBuf,\n  }\n\n  impl DurableTantivyIndex {\n      /// Open a Tantivy index with durability protection\n      pub fn open(data_dir: &Path, schema: Schema, config: RepairCodecConfig) -> SearchResult<Self>;\n\n      /// After a commit, protect any new/changed segment files\n      /// Called automatically after writer.commit() via a post-commit hook\n      pub fn protect_segments(&self) -> SearchResult<SegmentProtectionReport>;\n\n      /// Verify all segment files, attempt repair if corrupted\n      /// Called automatically on open()\n      pub fn verify_and_repair(&self) -> SearchResult<SegmentHealthReport>;\n\n      /// Get the inner Tantivy index for search operations\n      pub fn index(&self) -> &tantivy::Index;\n  }\n\n  pub struct SegmentProtectionReport {\n      pub segments_protected: usize,\n      pub segments_already_protected: usize,\n      pub total_source_bytes: u64,\n      pub total_repair_bytes: u64,\n      pub encode_time: Duration,\n  }\n\n  pub struct SegmentHealthReport {\n      pub segments_checked: usize,\n      pub segments_intact: usize,\n      pub segments_repaired: usize,\n      pub segments_unrecoverable: usize,\n      pub verify_time: Duration,\n      pub repair_time: Duration,\n  }\n\nTANTIVY SEGMENT FILE PROTECTION:\n  Each Tantivy segment has an ID (UUID) and multiple component files:\n    - {segment_id}.pos   (positions)\n    - {segment_id}.idx   (postings)\n    - {segment_id}.term  (term dictionary)\n    - {segment_id}.store (document store)\n    - {segment_id}.fast  (fast fields)\n    - {segment_id}.fieldnorm (field norms)\n\n  For each segment, we create a single .seg.fec sidecar that contains repair symbols\n  for ALL component files concatenated. The sidecar header maps component files to\n  their byte ranges within the concatenated source:\n\n  .seg.fec Header Extension:\n    component_count: u32\n    For each component:\n      filename_len: u16\n      filename: bytes\n      offset: u64 (within concatenated source)\n      size: u64\n\n  This means one .fec file per segment, not one per component file.\n\nLIFECYCLE INTEGRATION:\n  1. ON COMMIT: After writer.commit(), enumerate new segments and protect them\n  2. ON MERGE: After merge completes, protect the merged segment, remove .fec for merged-away segments\n  3. ON OPEN: Verify all segments, repair any corrupted ones before search\n  4. ON DELETE: When segments are garbage-collected, remove their .fec sidecars too\n\n  The protect step is asynchronous (doesn't block the commit path) but must complete\n  before the segment is considered durable.\n\nCORRUPTED INDEX RECOVERY (from cass tantivy.rs lines 110-121):\n  If a segment is corrupted beyond repair (RaptorQ fails), fall back to:\n  1. Log WARN with corruption details\n  2. Attempt Tantivy's built-in recovery (open with FORCE flag)\n  3. If that fails, trigger a full index rebuild from the document store (FrankenSQLite)\n  4. This is the fallback of last resort -- the document store IS the source of truth\n\n  This integrates with the schema versioning from bd-3un.17: if schema_hash mismatches,\n  the index is rebuilt anyway, so corruption during a schema migration is handled.\n\nPERFORMANCE CONSIDERATIONS:\n  - Tantivy indices are typically 10-100MB (much larger with store enabled)\n  - Encoding a 50MB segment at 20% overhead: ~200ms (4KB symbols, parallelizable)\n  - Verification (xxh3_64 fast path): ~10ms for 50MB\n  - This is acceptable because it happens at COMMIT time, not QUERY time\n\nFile: frankensearch-durability/src/tantivy_wrapper.rs\n","created_at":"2026-02-13T20:46:10Z"},{"id":120,"issue_id":"bd-3w1.8","author":"Dicklesworthstone","text":"REVISION (review pass - cross-codebase verification):\n\n1. TANTIVY DIRECTORY TRAIT: Tantivy uses a Directory trait for file I/O. We CANNOT hook into Tantivy's internal segment lifecycle via the Directory trait because that would require forking Tantivy or using its extension points. Instead, the approach should be:\n   a) After writer.commit() returns, enumerate segment files on the filesystem\n   b) Compare against known-protected segments (tracked in a HashMap<SegmentId, PathBuf>)\n   c) Protect any new/changed segments\n   This is EXTERNAL to Tantivy (filesystem-level), not internal (Directory-level).\n\n2. SEGMENT FILE ENUMERATION: Tantivy stores segments with UUIDs. After commit:\n   let segment_metas = index.searchable_segment_metas()?;\n   for meta in &segment_metas {\n       let seg_id = meta.id();\n       // Check if this segment is already protected\n       // If not, enumerate its component files and protect them\n   }\n   Use Tantivy's public API (Index::searchable_segment_metas()) for this, not filesystem scanning.\n\n3. MERGE HANDLING: When Tantivy merges segments, old segments are eventually garbage-collected. We need to:\n   a) Protect the new merged segment\n   b) Remove .seg.fec files for garbage-collected segments\n   But Tantivy's GC timing is non-deterministic (it depends on the MergePolicy). We should lazily clean up: when we encounter a .seg.fec file whose corresponding segment no longer exists in the index, delete the .fec file.\n\n4. SINGLE .FEC PER SEGMENT: The design of one .fec file containing protection for ALL component files (postings, positions, terms, store, fast fields, fieldnorms) is correct. The alternative (one .fec per component) would create too many small files. The concatenated approach with a component map in the header is the right design.\n\n5. RECOVERY ORDER: When verifying on open, check segments in creation order (oldest first). If an older segment is corrupted, it's more likely that newer segments are too (bit rot propagates). But for repair, do the opposite: repair newest first since they contain the most recent data.\n","created_at":"2026-02-13T20:58:12Z"}]}
{"id":"bd-3w1.9","title":"Implement corruption detection and automatic repair pipeline","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:24.721964896Z","created_by":"ubuntu","updated_at":"2026-02-13T21:59:12.271737045Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","repair","tier2"],"dependencies":[{"issue_id":"bd-3w1.9","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:53:48.458883788Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.9","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:24.721964896Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.9","depends_on_id":"bd-3w1.7","type":"blocks","created_at":"2026-02-13T20:42:28.111622734Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.9","depends_on_id":"bd-3w1.8","type":"blocks","created_at":"2026-02-13T20:42:28.231363504Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":62,"issue_id":"bd-3w1.9","author":"Dicklesworthstone","text":"TASK: Implement the corruption detection and automatic repair pipeline.\n\nThis is the high-level orchestrator that ties together the codec (bd-3w1.6), FSVI protection (bd-3w1.7), and Tantivy protection (bd-3w1.8) into a single coherent repair pipeline.\n\nFILE PROTECTOR API:\n\n  pub struct FileProtector {\n      codec: RepairCodec,\n      config: FileProtectorConfig,\n      metrics: Arc<DurabilityMetrics>,\n  }\n\n  pub struct FileProtectorConfig {\n      pub verify_on_open: bool,           // Default: true (verify indices on load)\n      pub protect_on_write: bool,         // Default: true (generate .fec after index write)\n      pub auto_repair: bool,              // Default: true (attempt repair when corruption detected)\n      pub repair_log_dir: Option<PathBuf>, // Optional directory for repair event logs\n      pub verify_interval_secs: Option<u64>, // Optional periodic verification (e.g., every 3600s)\n  }\n\n  impl FileProtector {\n      /// Protect a file: compute repair symbols and write .fec sidecar\n      pub fn protect(&self, path: &Path) -> SearchResult<ProtectionResult>;\n\n      /// Verify a file's integrity using its .fec sidecar\n      pub fn verify(&self, path: &Path) -> SearchResult<VerifyResult>;\n\n      /// Attempt to repair a corrupted file\n      pub fn repair(&self, path: &Path) -> SearchResult<RepairResult>;\n\n      /// Full pipeline: verify, repair if needed, report\n      pub fn verify_and_repair(&self, path: &Path) -> SearchResult<HealthCheckResult>;\n\n      /// Protect all index files in a directory (FSVI + Tantivy segments)\n      pub fn protect_directory(&self, dir: &Path) -> SearchResult<DirectoryProtectionReport>;\n\n      /// Verify all protected files in a directory\n      pub fn verify_directory(&self, dir: &Path) -> SearchResult<DirectoryHealthReport>;\n  }\n\n  pub struct HealthCheckResult {\n      pub path: PathBuf,\n      pub status: FileHealth,\n      pub details: String,\n  }\n\n  pub enum FileHealth {\n      Intact,\n      Repaired { corrupted_bytes: usize, repair_time: Duration },\n      Unrecoverable { reason: String },\n      Unprotected,  // No .fec sidecar found\n  }\n\nAUTOMATIC REPAIR PIPELINE:\n\n  The repair pipeline is invoked:\n  1. On index open (if verify_on_open = true)\n  2. Periodically (if verify_interval_secs is set)\n  3. When a search returns unexpected errors (SearchError::IndexCorrupted)\n\n  Pipeline steps:\n  a. FAST CHECK: xxh3_64(file) vs stored source_hash (~1ms per 100MB)\n     - If match: file is intact, return immediately\n  b. SYMBOL-LEVEL CHECK: Compare individual source symbols against stored checksums\n     - Identifies which specific 4KB blocks are corrupted\n  c. REPAIR ATTEMPT: Feed surviving + repair symbols into RaptorQ decoder\n     - If decode succeeds: overwrite corrupted file, verify again, log event\n     - If decode fails: log error, return Unrecoverable\n\n  All repair events are logged with full context:\n    tracing::warn!(\n        path = %path.display(),\n        corrupted_bytes = corrupted,\n        repair_symbols_used = used,\n        decode_time_ms = time.as_millis(),\n        \"index file repaired after corruption detected\"\n    );\n\nREPAIR EVENT LOG:\n  When repair_log_dir is set, each repair event is appended as a JSONL record:\n  {\n      \"timestamp\": \"2026-02-13T20:30:00Z\",\n      \"path\": \"/data/search/vector.fast.fsvi\",\n      \"corrupted_symbols\": 3,\n      \"total_symbols\": 18688,\n      \"repair_succeeded\": true,\n      \"decode_time_ms\": 450,\n      \"source_hash_before\": \"0x1234abcd\",\n      \"source_hash_after\": \"0x5678ef01\"\n  }\n\n  This provides an audit trail for understanding corruption patterns\n  (hardware issues, filesystem bugs, etc.).\n\nGRACEFUL DEGRADATION:\n  If the durability feature is disabled at compile time, the FileProtector becomes a no-op:\n  - protect() returns Ok immediately\n  - verify() returns VerifyResult::Unprotected\n  - repair() returns error \"durability feature not enabled\"\n\n  This is implemented via a trait with a default no-op implementation and a\n  #[cfg(feature = \"durability\")] real implementation.\n\nINTEGRATION WITH SEARCH PIPELINE:\n  The TwoTierSearcher (bd-3un.24) should:\n  1. Hold an optional Arc<FileProtector>\n  2. On SearchPhase::RefinementFailed, check if failure was due to index corruption\n  3. If so, attempt repair and retry the search once\n  4. Log the entire sequence (corruption detected -> repair -> retry -> success/failure)\n\nFile: frankensearch-durability/src/file_protector.rs\n","created_at":"2026-02-13T20:46:11Z"},{"id":121,"issue_id":"bd-3w1.9","author":"Dicklesworthstone","text":"REVISION (review pass - cross-codebase verification):\n\n1. GRACEFUL DEGRADATION TRAIT: The FileProtector should implement a trait with default no-op methods:\n\n   pub trait DurabilityProvider: Send + Sync {\n       fn protect(&self, path: &Path) -> SearchResult<ProtectionResult> {\n           Ok(ProtectionResult::noop())\n       }\n       fn verify(&self, path: &Path) -> SearchResult<VerifyResult> {\n           Ok(VerifyResult::Unprotected)\n       }\n       fn repair(&self, path: &Path) -> SearchResult<RepairResult> {\n           Err(SearchError::DurabilityDisabled)\n       }\n   }\n\n   #[cfg(feature = \"durability\")]\n   impl DurabilityProvider for FileProtector { /* real implementation */ }\n\n   #[cfg(not(feature = \"durability\"))]\n   pub struct NoopDurability;\n   impl DurabilityProvider for NoopDurability {}\n\n   This enables compile-time elimination of all durability overhead when the feature is disabled.\n\n2. ERROR VARIANT: Add SearchError::IndexCorrupted and SearchError::RepairFailed and SearchError::DurabilityDisabled to the error types (bd-3un.2). These should be added as a revision to bd-3un.2.\n\n3. REPAIR LOG ROTATION: The JSONL repair log can grow unbounded if corruption events are frequent (e.g., on failing hardware). Add a max_repair_log_entries config (default: 1000). When exceeded, rotate: rename to .1 and start fresh. This prevents disk exhaustion from repair event logging.\n\n4. PERIODIC VERIFICATION THREAD: When verify_interval_secs is set, spawn a background thread:\n   std::thread::Builder::new()\n       .name(\"frankensearch-durability-verify\".into())\n       .spawn(move || {\n           loop {\n               std::thread::sleep(Duration::from_secs(interval));\n               if shutdown.load(Ordering::Relaxed) { break; }\n               if let Err(e) = protector.verify_directory(&data_dir) {\n                   tracing::error!(error = %e, \"periodic durability verification failed\");\n               }\n           }\n       })?;\n   This thread should be owned by the top-level TwoTierSearcher or a DurabilityManager.\n","created_at":"2026-02-13T20:58:13Z"},{"id":257,"issue_id":"bd-3w1.9","author":"Dicklesworthstone","text":"REVISION (review pass 7 - backup before destructive repair):\n\nIMPORTANT: FileProtector::repair() overwrites the corrupted file with repaired data. This is destructive with no backup. If the repair produces incorrect data (codec bug, corrupted .fec sidecar), the original file is permanently lost.\n\nREQUIRED SAFETY MECHANISM:\n\nBefore overwriting, rename the corrupted file as a backup:\n  let backup_path = path.with_extension(format!(\"corrupt.{}\", timestamp_secs()));\n  std::fs::rename(&path, &backup_path)?;\n  // ... write repaired data to original path ...\n  // Verify repaired file passes integrity check\n  if verify_integrity(&path)? {\n      std::fs::remove_file(&backup_path)?;  // Cleanup backup\n  } else {\n      std::fs::rename(&backup_path, &path)?;  // Restore original\n      return Err(SearchError::RepairFailed { reason: \"repaired file failed verification\" });\n  }\n\nConfiguration: keep_corrupt_backups: bool (default: true in debug, false in release).\n\nALSO: Add protect_all_existing() method to FileProtector that scans for unprotected indices and generates .fec sidecars. This handles the migration case where durability is enabled on a system with pre-existing unprotected indices. Without this, all existing indices emit warnings on every open.\n","created_at":"2026-02-13T21:54:45Z"},{"id":261,"issue_id":"bd-3w1.9","author":"Dicklesworthstone","text":"REVISION (review pass 7 - error type dependency):\n\nADDED bd-3un.2 (core error types) as a blocking dependency. The repair pipeline introduces new error variants:\n- SearchError::IndexCorrupted { path, expected_crc, actual_crc }\n- SearchError::RepairFailed { reason }\n- SearchError::DurabilityDisabled\n\nThese must be defined in the shared error enum (bd-3un.2) so that consumers can pattern-match on them uniformly. The repair pipeline returns these errors through the standard SearchResult type.\n\nNOTE: bd-3un.2's revision already includes feature-gated error variants for storage/durability. The dependency ensures the error types are implemented before the repair pipeline that uses them.\n","created_at":"2026-02-13T21:55:12Z"},{"id":279,"issue_id":"bd-3w1.9","author":"Dicklesworthstone","text":"REVISION (review pass 7 - asupersync for periodic verification):\n\nSTALE PATTERN: An earlier comment (point 4) suggests std::thread::Builder::new() for periodic durability verification. This is PROHIBITED — asupersync is the only runtime.\n\nCORRECTED PATTERN:\n  // In DurabilityManager or TwoTierSearcher setup\n  scope.spawn(async move |cx| {\n      loop {\n          cx.sleep(Duration::from_secs(interval)).await;\n          if let Err(e) = protector.verify_directory(&data_dir) {\n              tracing::error!(error = %e, \"periodic durability verification failed\");\n          }\n      }\n  });\n\nThe periodic verification task is a child of the asupersync region that owns the search pipeline. It cancels automatically when the pipeline shuts down (structured concurrency). No manual shutdown flag needed.\n","created_at":"2026-02-13T21:59:12Z"}]}
{"id":"bd-6sj","title":"Implement off-policy evaluation for safe ranking changes","description":"Implement offline ranking evaluation infrastructure using Inverse Propensity Scoring (IPS) and Doubly Robust (DR) estimators. This enables estimating the impact of ranking algorithm changes (new K, new blend_factor, new reranker model) using historical search logs BEFORE deploying online.\n\nGraveyard entry: §12.12 Off-Policy Evaluation (IPS/DR)\nEV score: 6.0 (Impact=3, Confidence=3, Reuse=4, Effort=3, Friction=2)\nPriority tier: B (requires evidence ledger from bd-3un.39 first)\n\nArchitecture:\npub struct OffPolicyEvaluator {\n    logging_policy: Box<dyn RankingPolicy>,   // The policy that generated the logs\n    target_policy: Box<dyn RankingPolicy>,    // The proposed new policy\n    clipping_threshold: f64,                   // Max IPS weight (default 100.0)\n}\n\npub trait RankingPolicy: Send + Sync {\n    fn score(&self, query: &str, doc_id: &str) -> f64;\n    fn propensity(&self, query: &str, doc_id: &str, rank: usize) -> f64;\n}\n\nEstimators:\n1. IPS (Inverse Propensity Scoring):\n   estimated_reward = (1/N) * sum(reward_i * target_propensity_i / logging_propensity_i)\n   - Unbiased but high variance\n   - Clipping: cap importance weight at threshold to reduce variance\n\n2. DR (Doubly Robust):\n   estimated_reward = IPS_term + control_variate_from_reward_model\n   - Variance <= IPS variance (provable)\n   - Requires reward model (can use NDCG@10 from golden corpus)\n\n3. Effective Sample Size (ESS):\n   ESS = (sum(w_i))^2 / sum(w_i^2)\n   - Reject estimate if ESS < 100 (insufficient overlap)\n\nIntegration:\n- Consumes evidence ledger (bd-3un.39) as log source\n- Consumes test fixture corpus (bd-3un.38) for reward labels\n- Produces: estimated NDCG@10 delta, confidence interval, ESS\n\nBudgeted mode: Offline only (no runtime cost). Memory proportional to log size.\n\nFallback: Don't use OPE estimates; rely on golden-check corpus only.\n\nFile: frankensearch-fusion/src/ope.rs (offline evaluation module)\n\nReference: Dudik et al. (2011) \"Doubly Robust Policy Evaluation\", Swaminathan & Joachims (2015) \"Batch Learning from Logged Bandit Feedback\"\nBaseline comparator: Golden-check-only validation (current bd-3un.38)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:45:43.570858954Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:40.721019226Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["evaluation","graveyard","offline","phase11"],"dependencies":[{"issue_id":"bd-6sj","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.437480493Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6sj","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:22:22.893984838Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6sj","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:46:12.983731002Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6sj","depends_on_id":"bd-3un.39","type":"blocks","created_at":"2026-02-13T20:46:12.896196972Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6sj","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T21:22:22.635147528Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":84,"issue_id":"bd-6sj","author":"Dicklesworthstone","text":"REFINEMENT PASS 1 — Architecture clarifications and integration notes:\n\n1. EVIDENCE LEDGER CLARIFICATION: This bead depends on bd-3un.39 (structured tracing) as the source of historical search logs. The evidence ledger is the structured tracing output (JSON spans) from the search pipeline. OPE reads these logs offline to evaluate proposed ranking changes.\n\n2. REWARD MODEL: The reward model for DR estimation uses NDCG@10 computed against the golden corpus (bd-3un.38). For queries not in the golden corpus, use click-through rate as a proxy reward (if click data is available in the evidence ledger).\n\n3. OFFLINE-ONLY DESIGN: This is an analysis tool, not a runtime component. It lives in a separate binary (examples/ope_eval.rs or tools/ope_eval.rs) that reads serialized evidence logs and outputs a report. No impact on search latency or correctness.\n\n4. PRACTICAL USAGE: An engineer would run this before deploying a ranking change:\n   cargo run --example ope_eval -- --log-dir ./evidence/ --new-k 80 --new-blend 0.6\n   Output: Estimated NDCG@10 delta: +0.03 (95% CI: [-0.01, +0.07]), ESS: 234, Recommendation: DEPLOY\n\n5. TEST REQUIREMENTS (covered by bd-3un.31/32): Add tests for:\n   - IPS with uniform logging policy reduces to simple average\n   - DR variance <= IPS variance on same dataset\n   - ESS computation correctness\n   - Clipping reduces variance (verify on synthetic high-variance scenario)\n   - Reject when ESS < threshold","created_at":"2026-02-13T20:51:37Z"},{"id":214,"issue_id":"bd-6sj","author":"Dicklesworthstone","text":"REVISION (review pass 5 - dependency corrections):\n\n1. ADDED bd-3un.5 and bd-3un.2 DEPENDENCIES: The OPE evaluator works with ranked lists of ScoredResult and computes NDCG over FusedHit-style outputs (from bd-3un.5). Error handling for the offline evaluation binary needs SearchError (from bd-3un.2).\n\n2. EVIDENCE LEDGER CLARIFICATION: The \"evidence ledger\" consumed by this bead IS the structured tracing output (JSON spans) from bd-3un.39 -- NOT a separate data structure. The bd-3un.39 tracing spans contain per-query records with query_hash, query_class, blend_factor_used, latency_ms, etc. The OPE evaluator parses these spans offline.\n\n3. ASUPERSYNC NOTE: This bead is offline-only (batch evaluation binary). No async runtime needed. All computation is synchronous file I/O + math.\n","created_at":"2026-02-13T21:23:14Z"},{"id":246,"issue_id":"bd-6sj","author":"Dicklesworthstone","text":"REVIEW FIX — Formula corrections and propensity clarification:\n\n1. PROPENSITY CLARIFICATION: The RankingPolicy trait's propensity() method conflates two distinct concepts:\n   - Policy propensity: P(doc placed at rank | query, policy) — probability the ranking policy places this document at this rank\n   - Position bias: P(user examines rank) — probability the user looks at position `rank`\n\n   For IPS in ranking evaluation, we need POLICY propensity (not position bias). Clarify:\n\n   pub trait RankingPolicy {\n       /// Probability that this policy places doc_id at the given rank for this query.\n       /// For deterministic policies: 1.0 if doc is at that rank, 0.0 otherwise.\n       /// For stochastic policies: the probability under the policy's randomization.\n       fn rank_probability(&self, query: &str, doc_id: &str, rank: usize) -> f64;\n   }\n\n2. DOUBLY ROBUST FORMULA (complete):\n   DR = (1/N) * Σᵢ [ r_hat(xᵢ) + wᵢ * (rᵢ - r_hat(xᵢ)) ]\n   where:\n     rᵢ = observed reward (e.g., NDCG@10 for query i)\n     r_hat(xᵢ) = reward model prediction (e.g., predicted NDCG based on score features)\n     wᵢ = π_target(aᵢ|xᵢ) / π_logging(aᵢ|xᵢ) = importance weight\n     xᵢ = query features, aᵢ = ranking action (the full ranked list)\n\n   Properties:\n   - If reward model is perfect (r_hat = r): DR = E[r] regardless of weights (unbiased)\n   - If weights are correct: DR is unbiased regardless of reward model\n   - Variance(DR) <= Variance(IPS) (always, by construction)\n\n3. CLIPPING DEFAULT: Change from 100.0 to 10.0. At w=100, a single observation gets 100x the influence of a normal observation — this is rarely desirable. With w=10, max influence ratio is 10:1, which provides meaningful variance reduction while preserving most of the bias correction.\n\n4. ESS CONTEXT: Add note that the ESS < 100 threshold assumes at least 500 total queries in the log. For smaller logs, scale proportionally: ESS_threshold = max(20, total_queries / 5).\n\n5. TEST REQUIREMENT ADDITIONS:\n   - Known-answer test: create synthetic data where true policy value is computable, verify OPE estimate within CI\n   - Propensity ratio: for identical policies (target = logging), all weights = 1.0, IPS = simple mean\n   - Clipping effect: with clipping=10, max weight in output is 10.0","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-i37","title":"Embedding Batch Coalescing with Deadline Scheduling","description":"Implement batch coalescing for embedding requests with deadline-aware scheduling. When multiple concurrent callers request embeddings (e.g., during index building), coalesce their requests into optimal batches rather than processing one-at-a-time.\n\n## Design\n\n```rust\npub struct BatchCoalescer {\n    pending: Mutex<Vec<PendingRequest>>,\n    config: CoalescerConfig,\n    notify: Condvar,\n}\n\npub struct CoalescerConfig {\n    pub max_batch_size: usize,       // Maximum texts per batch (default: 32)\n    pub max_wait_ms: u64,            // Maximum time to wait for batch fill (default: 10ms)\n    pub min_batch_size: usize,       // Minimum batch before early dispatch (default: 4)\n    pub priority_lanes: usize,       // Number of priority levels (default: 2: interactive vs background)\n}\n\npub struct PendingRequest {\n    pub text: String,\n    pub deadline: Instant,           // When this request MUST be processed\n    pub priority: Priority,          // Interactive (high) vs Background (low)\n    pub result_tx: oneshot::Sender<Vec<f32>>,\n}\n\npub enum Priority {\n    Interactive,  // Search query — tight deadline (~15ms budget)\n    Background,   // Index building — can wait for full batch\n}\n```\n\n## Scheduling Algorithm\n\n1. Requests arrive with deadlines and priorities\n2. Interactive requests: dispatch immediately if batch has >= 1 interactive request and wait time > max_wait_ms / 2\n3. Background requests: accumulate until max_batch_size or max_wait_ms\n4. Mixed batches: interactive requests set the deadline for the whole batch\n5. Batch dispatched to Embedder::embed_batch() (if available, else sequential)\n\n## Key Optimization\n\nONNX model inference (FastEmbed, Model2Vec) has near-fixed overhead per batch. Processing 1 text vs 32 texts takes similar GPU/CPU setup time. Batching amortizes this overhead.\n\n## Performance Model\n\n- FastEmbed (MiniLM-L6-v2): 128ms for 1 text, ~140ms for 32 texts = 4.4ms/text batched vs 128ms/text unbatched = 29x throughput improvement\n- Model2Vec (potion): 0.57ms for 1 text, ~2ms for 32 texts = 0.06ms/text batched\n\n## Why This Matters\n\nDuring index building, frankensearch processes thousands of documents. Without batching, each document is embedded individually, wasting 95%+ of the model inference overhead. Batch coalescing is the single highest-impact optimization for index build time.\n\nThe deadline scheduling ensures that interactive search queries are never delayed by background batch accumulation — a critical UX property.\n\n## Testing\n\n- Unit: single request dispatched within max_wait_ms\n- Unit: full batch dispatched immediately\n- Unit: interactive priority triggers early dispatch\n- Unit: deadline enforcement (no request waits past deadline)\n- Unit: mixed priority batch scheduling\n- Integration: concurrent callers receive correct results\n- Integration: throughput improvement vs sequential\n- Benchmark: batch coalescing overhead, throughput scaling with batch size","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T22:01:38.836118166Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:39.996748073Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-i37","depends_on_id":"bd-3un.27","type":"blocks","created_at":"2026-02-13T22:02:39.890403842Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-i37","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T22:02:39.996705303Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-l7v","title":"Implement S3-FIFO lock-free cache eviction","description":"Implement S3-FIFO (Yang et al. 2023) three-queue cache eviction for embedding vectors, search results, and index segments. S3-FIFO uses Small/Main/Ghost FIFO queues with no per-access list manipulation (unlike LRU), achieving higher hit rates and lock-free operation.\n\nGraveyard entry: §15.1 S3-FIFO Cache Eviction\nEV score: 50 (Impact=4, Confidence=5, Reuse=5, Effort=2, Friction=1)\nPriority tier: A\n\nArchitecture:\npub struct S3FifoCache<K, V> {\n    small: FifoQueue<K, V>,    // New entries land here (10% capacity)\n    main: FifoQueue<K, V>,     // Promoted entries (90% capacity)\n    ghost: GhostQueue<K>,      // Evicted keys (metadata only, 2x main capacity)\n    max_bytes: usize,          // Configurable memory budget (default 256MB)\n    freq_threshold: u8,        // Promotion threshold (default 1)\n}\n\nEviction policy:\n1. New items enter Small queue\n2. On Small eviction: if accessed >= freq_threshold times, promote to Main; else evict\n3. On Main eviction: evict (FIFO order)\n4. Ghost tracks recently evicted keys (metadata only); re-access of ghost key → direct Main insert\n5. All queues are FIFO (no list manipulation per access)\n\nCache targets in frankensearch:\n- Embedding vectors: Key=(doc_id, embedder_id), Value=Vec<f32> (256-768 bytes)\n- Quality model inference: Key=content_hash, Value=Vec<f32> (avoids re-embedding)\n- Tantivy search results: Key=(query_hash, k), Value=Vec<ScoredResult>\n\nTrait interface:\npub trait CachePolicy<K: Hash + Eq, V>: Send + Sync {\n    fn get(&self, key: &K) -> Option<&V>;\n    fn insert(&self, key: K, value: V, size_bytes: usize);\n    fn hit_rate(&self) -> f64;\n    fn memory_used(&self) -> usize;\n}\n\nImplementations: S3Fifo (default), Unbounded (HashMap, for tests), NoCache (passthrough).\n\nBudgeted mode: max_bytes configurable via FRANKENSEARCH_CACHE_MB env var. On exhaustion: evict Small first. Hit rate < 30% for 100 queries → bypass cache + log WARN.\n\nFallback: CachePolicy::noop() returns None for all gets (zero-cost passthrough).\n\nIsomorphism proof: Cache is transparent — same query produces identical rankings with/without cache. Verify via golden-check corpus (bd-3un.38).\n\nFile: frankensearch-core/src/cache.rs (trait + S3Fifo impl)\n\nReference: Yang et al. \"FIFO Queues are All You Need for Cache Eviction\" (SOSP 2023)\nBaseline comparator: OnceLock (never evicts), HashMap (unbounded growth), LRU (lock per access)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:45:31.505125880Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:40.987879133Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cache","graveyard","phase7"],"dependencies":[{"issue_id":"bd-l7v","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.178896495Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-l7v","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:45:55.877271569Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-l7v","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:46:00.462669693Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":81,"issue_id":"bd-l7v","author":"Dicklesworthstone","text":"REFINEMENT PASS 1 — Architecture clarifications and integration notes:\n\n1. SCOPE CLARIFICATION: This bead (S3-FIFO cache) is for EMBEDDING-LEVEL caching (vectors, model inference results, search result sets). It is ORTHOGONAL to bd-3un.41 (IndexCache for index-level caching with OnceLock). They operate at different layers:\n   - bd-l7v: Per-query/per-document embedding cache (hot path, high churn)\n   - bd-3un.41: Per-index file cache (cold path, loaded once, invalidated on staleness)\n   Both should use CachePolicy trait from this bead, but bd-3un.41's OnceLock pattern is appropriate for index segments (loaded once, held for lifetime).\n\n2. FILE PLACEMENT CORRECTION: The CachePolicy trait should live in frankensearch-core/src/cache.rs (as stated). The S3Fifo implementation should ALSO live there since it's a zero-dep data structure. The NoCache and Unbounded impls go alongside. No external crate dependency needed — S3-FIFO is ~150 lines of safe Rust.\n\n3. THREAD SAFETY: S3FifoCache must be Send + Sync. Use DashMap for concurrent access or wrap with RwLock. For the FIFO queues, use crossbeam-queue::ArrayQueue (bounded, lock-free) or a simple VecDeque behind Mutex (simpler, still fast since queue operations are O(1)).\n\n4. INTEGRATION HOOK: The TwoTierSearcher (bd-3un.24) should accept an optional Arc<dyn CachePolicy> parameter. When present, check cache before embedding and store results after embedding. This means the cache is injected, not hardcoded.\n\n5. TEST REQUIREMENTS (covered by bd-3un.31): Add tests for:\n   - Cache hit/miss counting accuracy\n   - Small→Main promotion on freq_threshold\n   - Ghost queue re-admission to Main\n   - Memory budget enforcement (insert beyond max_bytes triggers eviction)\n   - Concurrent get/insert from multiple threads\n   - Isomorphism: same query produces identical results with/without cache","created_at":"2026-02-13T20:51:33Z"},{"id":88,"issue_id":"bd-l7v","author":"Dicklesworthstone","text":"REFINEMENT PASS 3 — Composition with bd-qtx (quantization ladder):\n\nThe S3-FIFO cache stores embedding vectors keyed by (doc_id, embedder_id). When bd-qtx is implemented, the cache key should also include the quantization level to avoid serving f16-quantized vectors when f32 was requested (or vice versa). Updated key: (doc_id, embedder_id, quant_level).\n\nThis is a soft composition concern — no dependency needed since the cache stores whatever Vec<f32> the embedder produces (always f32 at query time, quantization only affects storage). The cache correctly caches the COMPUTED embeddings, not the STORED ones. No action needed — just documenting that the cache is quantization-transparent.","created_at":"2026-02-13T20:52:49Z"},{"id":155,"issue_id":"bd-l7v","author":"Dicklesworthstone","text":"ASUPERSYNC MIGRATION — MODERATE REVISION (sync primitives for cache):\n\nReplace std sync primitives with asupersync equivalents for the S3-FIFO cache:\n\nBEFORE:\n  - DashMap or RwLock for concurrent cache access\n  - crossbeam-queue::ArrayQueue for FIFO queues\n\nAFTER:\n  - asupersync::sync::RwLock for concurrent cache access (cancel-aware)\n  - VecDeque behind asupersync::sync::Mutex for FIFO queues (simpler, cancel-aware)\n  - OR: keep lock-free ArrayQueue from crossbeam if benchmarks show it's needed (crossbeam-queue is a small, focused crate without tokio deps — acceptable to keep)\n\nREVISED CachePolicy trait:\n  pub trait CachePolicy<K: Hash + Eq, V>: Send + Sync {\n      async fn get(&self, cx: &Cx, key: &K) -> Option<V>;\n      async fn insert(&self, cx: &Cx, key: K, value: V, size_bytes: usize);\n      fn hit_rate(&self) -> f64;\n      fn memory_used(&self) -> usize;\n  }\n\nNote: get() and insert() are now async because they acquire cancel-aware locks. The Cx parameter enables:\n1. Cancel-aware lock acquisition (don't deadlock if task is cancelled while waiting for lock)\n2. Budget enforcement (cache operations respect the task's deadline)\n3. Tracing integration (cache hit/miss events in structured trace)\n\nTESTING: Use LabRuntime for deterministic cache behavior testing:\n  - Deterministic scheduling means cache hit/miss patterns are reproducible\n  - ContendedMutex variant for measuring lock contention under load","created_at":"2026-02-13T21:06:23Z"},{"id":248,"issue_id":"bd-l7v","author":"Dicklesworthstone","text":"REVIEW FIX — Title accuracy and API reconciliation:\n\n1. TITLE CLARIFICATION: \"lock-free\" in the title is misleading. The S3-FIFO algorithm itself avoids per-access linked list manipulation (unlike LRU), but the Rust implementation will use asupersync::sync::Mutex or RwLock for thread safety — which are locks. The \"lock-free\" property refers to the eviction algorithm's internal simplicity, not the concurrency implementation.\n\n   SUGGESTED TITLE: \"Implement S3-FIFO cache eviction\" (drop \"lock-free\")\n\n2. CACHE API RECONCILIATION: The body defines sync methods:\n     fn get(&self, key: &K) -> Option<&V>;\n     fn insert(&self, key: K, value: V, size_bytes: usize);\n\n   The ASUPERSYNC comment makes them async:\n     async fn get(&self, cx: &Cx, key: &K) -> Option<V>;\n     async fn insert(&self, cx: &Cx, key: K, value: V, size_bytes: usize);\n\n   RESOLUTION: Provide BOTH:\n   - CachePolicy trait with async methods (for use in async contexts)\n   - A sync wrapper for synchronous contexts:\n     pub fn get_blocking(&self, key: &K) -> Option<V>  // For use in rayon data parallelism\n\n3. GHOST QUEUE SIZING: \"Ghost queue: 2x main capacity\" — this means the ghost queue stores metadata for 2x as many entries as main. Since ghost stores only keys (not values), the memory is small. For a 256MB cache with 10/90 split:\n   - Small: 25.6MB of values (~1000 entries at 256 bytes each → 1000 keys in ghost ≈ negligible)\n   - Main: 230.4MB of values\n   - Ghost: metadata for 2000 entries ≈ 2000 * 64 bytes = ~128KB\n   The sizing is fine. Add this calculation to the body for clarity.\n\n4. HIT RATE WINDOW: \"Hit rate < 30% for 100 queries → bypass\" — 100 queries is a small window. Use a larger window with exponential moving average:\n   hit_rate_ema = alpha * is_hit + (1 - alpha) * hit_rate_ema  (alpha = 0.01)\n   This smooths over transient workload changes and avoids cache thrashing on small sample sizes.\n\n5. CACHE VALUE OWNERSHIP: Use Arc<V> to avoid cloning embedding vectors (which are Vec<f32>, 1.5KB each):\n   pub trait CachePolicy<K: Hash + Eq, V>: Send + Sync {\n       async fn get(&self, cx: &Cx, key: &K) -> Option<Arc<V>>;\n       async fn insert(&self, cx: &Cx, key: K, value: Arc<V>, size_bytes: usize);\n   }","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-qtx","title":"Implement quantization ladder (f32/f16/int8/int4) with formal quality bounds","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:31:23.285837206Z","created_by":"ubuntu","updated_at":"2026-02-13T21:50:54.497866338Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["optimization","phase4","quantization","vector-index"],"dependencies":[{"issue_id":"bd-qtx","depends_on_id":"bd-3un","type":"blocks","created_at":"2026-02-13T20:31:46.435141937Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T20:31:39.251284448Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T20:31:39.331697702Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T21:50:54.497821894Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T21:50:54.281927301Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T21:50:54.391320269Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":37,"issue_id":"bd-qtx","author":"Dicklesworthstone","text":"Implement a quantization ladder (f32/f16/int8/int4) with formal quality-memory tradeoff characterization for vector indices. Enables extreme memory savings for large-scale deployments.\n\nOPTIMIZATION RATIONALE:\n\nCurrent FSVI format supports f32 and f16. For large indices (100K+ docs), memory is the bottleneck:\n- 100K docs x 384 dims x f32 = 147 MB\n- 100K docs x 384 dims x f16 = 73 MB   (current default)\n- 100K docs x 384 dims x int8 = 37 MB   (THIS BEAD)\n- 100K docs x 384 dims x int4 = 18 MB   (THIS BEAD)\n\nOpportunity Matrix:\n| Quantization | Memory | Quality Loss | Effort | Score (Impact x Conf / Effort) |\n|-------------|--------|-------------|--------|-------------------------------|\n| int8 scalar | 4x vs f32 | ~1-2%  | 2      | 10.0                          |\n| int4 packed | 8x vs f32 | ~3-5%  | 4      | 5.0                           |\n\nIMPLEMENTATION:\n\n1. Scalar Quantization (int8):\n   - Per-dimension min/max computed during indexing\n   - Quantize: q = round((x - min) / (max - min) * 255)\n   - Dequantize: x' = q / 255 * (max - min) + min\n   - Store quantization parameters in FSVI header (2 floats per dimension)\n   - SIMD dot product on int8: use i16 multiply-accumulate via wide crate\n\npub struct ScalarQuantizer {\n    mins: Vec<f32>,     // Per-dimension minimum\n    scales: Vec<f32>,   // Per-dimension (max - min) / 255\n}\n\nimpl ScalarQuantizer {\n    pub fn fit(vectors: &[Vec<f32>]) -> Self;\n    pub fn quantize(&self, vector: &[f32]) -> Vec<u8>;\n    pub fn dot_product_quantized(&self, stored: &[u8], query: &[f32]) -> f32;\n}\n\n2. Product Quantization (optional, for int4-equivalent compression):\n   - Split 384-dim vector into 48 sub-vectors of 8 dims each\n   - K-means cluster each sub-space into 256 centroids\n   - Store 1 byte per sub-vector (centroid index)\n   - Asymmetric distance computation (ADC): query in full precision, database quantized\n   - Compression: 384 dims -> 48 bytes (8x vs f32, 4x vs f16)\n\n3. FSVI Format Extension:\n   - quantization field in header: 0=f32, 1=f16, 2=int8, 3=int4/PQ\n   - For int8: append quantization parameters after header (mins + scales)\n   - For PQ: append codebook after header\n\n4. Quality Characterization (Alien-Artifact):\n   - For each quantization level, compute NDCG@10 on the test fixture corpus\n   - Provide formal bounds on maximum quality loss:\n     For int8: |cos_sim(q, x) - cos_sim(q, x')| <= epsilon\n     where epsilon = max_dim(scale_i / 255) * sqrt(d) / ||q|| / ||x||\n   - This is a PROVABLE bound on the worst-case quality degradation\n\n5. Automatic Selection:\n   pub fn recommended_quantization(index_size: usize, available_memory: usize) -> Quantization {\n       let f16_size = index_size * dimension * 2;\n       let int8_size = index_size * dimension;\n       if f16_size <= available_memory { Quantization::F16 }\n       else if int8_size <= available_memory { Quantization::Int8 }\n       else { Quantization::PQ }\n   }\n\nFile: frankensearch-index/src/quantization.rs\nDependencies: bd-3un.13 (FSVI format), bd-3un.14 (SIMD dot product)\n\nIsomorphism: Rankings may change slightly due to quantization error. Provide formal epsilon bounds and NDCG regression test: NDCG@10(int8) >= 0.95 * NDCG@10(f16) on test corpus.\n","created_at":"2026-02-13T20:31:32Z"},{"id":244,"issue_id":"bd-qtx","author":"Dicklesworthstone","text":"REVIEW FIX — Quality bound formula correction, naming, and missing deps:\n\n1. QUALITY BOUND FORMULA CORRECTION: The body states:\n   |cos_sim(q, x) - cos_sim(q, x')| <= epsilon\n   where epsilon = max_dim(scale_i / 255) * sqrt(d) / ||q|| / ||x||\n\n   This is INCORRECT. The correct bound:\n   - Quantization error per dimension: |x_i - x'_i| <= scale_i / 255\n   - Vector error: ||x - x'|| <= sqrt(sum((scale_i/255)²))\n   - Dot product error: |dot(q, x) - dot(q, x')| <= ||q|| * ||x - x'||\n   - Cosine similarity error:\n     |cos_sim(q, x) - cos_sim(q, x')| <= ||x - x'|| / ||x|| ≈ sqrt(sum((scale_i/255)²)) / ||x||\n\n   For unit-normalized vectors (||x|| = 1, typical for embeddings):\n     epsilon ≈ sqrt(sum((scale_i/255)²))\n\n   For uniform scale across dimensions (scale = max - min):\n     epsilon ≈ (scale / 255) * sqrt(d)\n\n   For 384-dim unit vectors with typical scale ~2: epsilon ≈ (2/255) * sqrt(384) ≈ 0.154\n   This means int8 quantization introduces at most ~15% cosine similarity error — verifiable empirically.\n\n2. NAMING FIX: \"int4-equivalent\" for Product Quantization is MISLEADING. PQ stores 1 byte per sub-vector (centroid index), not 4-bit integers. Rename:\n\n   pub enum QuantizationLevel {\n       F32,                        // Full precision (4 bytes/dim)\n       F16,                        // Half precision (2 bytes/dim) — current default\n       Int8 { scale: f32, zero: f32 },  // Scalar quantization (1 byte/dim)\n       ProductQuantization {       // PQ: sub-vector centroid indices\n           n_subvectors: usize,    // Default: 48 (for 384-dim)\n           n_centroids: usize,     // Default: 256 (1 byte per index)\n       },\n   }\n\n3. WIDE CRATE INT SIMD: The `wide` crate supports f32x8 and i32x8 but NOT i16 or i8 multiply-accumulate. For int8 dot product with accumulation into i32:\n   - Option A: Use i32x8 from `wide` (widen i8 to i32, then multiply-accumulate) — safe, portable\n   - Option B: Use Rust nightly std::simd for native i8 operations — nightly-only\n   - DECISION: Use Option A (i32x8 widening) for V1. It's 4x slower than native i8 SIMD but safe and portable. Profile before optimizing.\n\n4. MISSING DEPENDENCIES — Add:\n   - bd-3un.38 (test corpus for NDCG evaluation)\n   - bd-3un.5 (VectorHit result types)\n   - bd-3un.2 (SearchError for error handling)\n\n5. DEPENDENCY TYPE FIX: bd-3un should be parent-child, not blocks.\n\n6. TEST REQUIREMENTS:\n   - Int8 round-trip: quantize f32 → int8 → dequantize, max error within bound\n   - NDCG regression: NDCG@10(int8) >= 0.95 * NDCG@10(f16) on test corpus\n   - PQ round-trip: train centroids, quantize, dequantize, max error within expected range\n   - FSVI format: write/read int8 quantized index (format version check)\n   - Auto-selection: given memory budget, correct quantization level chosen\n   - Edge cases: zero vectors, constant vectors, single-dimension vectors\n   - Int8 dot product: matches f32 reference within quantization tolerance\n   - Memory savings: verify 4x reduction for int8 vs f32, 2x vs f16","created_at":"2026-02-13T21:50:40Z"}]}
{"id":"bd-sot","title":"Soft-Delete Tombstones in FSVI","description":"Implement soft-delete tombstones in the FSVI vector index format. Currently, deleting a document requires a full index rebuild. Tombstones allow O(1) logical deletion by flipping a bit in the record's flags field.\n\n## Background\n\nThe FSVI format stores document embeddings as fixed-size records with a header containing metadata fields. Each record already has a `flags: u16` field that is currently reserved (always zero). This bead repurposes bit 0 of the flags field as a tombstone marker, enabling O(1) logical deletion without rewriting the index file.\n\nDocument deletion is a basic operation that currently requires an expensive full rebuild. For mcp_agent_mail_rust (message archival with TTL), messages expire continuously, and rebuilding the entire index for each expiration is impractical. For xf, users may want to remove specific tweets from search results without re-indexing.\n\n## Design\n\n### Tombstone Flag\n\n```rust\nconst TOMBSTONE_FLAG: u16 = 0x0001;  // Bit 0 of the flags field\n```\n\nThe tombstone is set by writing a single u16 to the record's flags offset in the memory-mapped file. This is an atomic write on all supported platforms (u16 is naturally aligned in the FSVI record layout).\n\n### API\n\n```rust\nimpl VectorIndex {\n    pub fn soft_delete(&mut self, doc_id: &str) -> Result<bool, SearchError>;\n    pub fn soft_delete_batch(&mut self, doc_ids: &[&str]) -> Result<usize, SearchError>;\n    pub fn is_deleted(&self, record_index: usize) -> bool;\n    pub fn tombstone_count(&self) -> usize;\n    pub fn tombstone_ratio(&self) -> f64;  // tombstones / total records\n    pub fn needs_vacuum(&self) -> bool;    // tombstone_ratio > threshold (default: 0.2)\n    pub fn vacuum(&mut self) -> Result<VacuumStats, SearchError>;\n}\n\npub struct VacuumStats {\n    pub records_before: usize,\n    pub records_after: usize,\n    pub tombstones_removed: usize,\n    pub bytes_reclaimed: usize,\n    pub duration: Duration,\n}\n```\n\n### Search Integration\n\nDuring top-k scan, skip records with the tombstone flag set:\n```rust\nif self.is_deleted(record_index) {\n    continue;  // Skip tombstoned record\n}\n```\n\nThis adds a single bit-test per record — negligible overhead (< 1ns per record). The BinaryHeap-based top-k selection naturally handles the reduced effective record count.\n\n### Vacuum\n\nWhen tombstone_ratio exceeds the threshold (default: 0.2, meaning 20% of records are deleted), vacuum rewrites the index without tombstoned records:\n1. Scan all records, collecting non-tombstoned ones\n2. Write to `.fsvi.new` file\n3. Atomic `rename()` over the old file (same pattern as compaction)\n\nVacuum is semantically identical to compaction — both rewrite the index file. When WAL support is implemented (Idea 1), compaction and vacuum can be unified: compaction merges main + WAL and removes tombstones in a single pass.\n\n### Interaction with WAL (Idea 1)\n\n- WAL entries use the same flags field and can be tombstoned with the same mechanism\n- Compaction naturally removes tombstoned entries from both main index and WAL\n- A document appended via WAL and then soft-deleted will be tombstoned in the WAL, and the tombstone will be dropped during compaction\n\n## Justification\n\nDocument deletion is a basic operation that currently requires an expensive full rebuild. Specific consumer needs:\n- **mcp_agent_mail_rust**: message archival with TTL — messages expire continuously, requiring efficient deletion\n- **xf**: users may want to remove specific tweets from search results (e.g., deleted tweets, blocked accounts)\n- **cass**: old sessions may be archived/deleted to keep the search index focused on recent conversations\n\nSoft-delete with tombstones provides O(1) deletion at the cost of slightly degraded search performance (scanning tombstoned records). Vacuum reclaims this overhead when tombstones accumulate.\n\n## Considerations\n\n- Tombstone persistence: the flags write is immediately durable (mmap'd file, fsync on next batch operation). Individual tombstone writes without fsync are acceptable because tombstones are idempotent — re-deleting is harmless.\n- doc_id lookup: soft_delete needs to find the record by doc_id. The current FSVI format stores doc_id hashes in the record. A linear scan of hashes is O(n) but sufficient for individual deletes. For batch deletes, build a hash set first.\n- Concurrent access: tombstone writes (single u16) are atomic on all platforms. Readers may see stale (non-tombstoned) records briefly, which is acceptable (eventual consistency for deletes).\n- Vacuum threshold: 0.2 (20%) is a reasonable default. Lower values vacuum more often (less scan overhead but more I/O). Higher values tolerate more dead records.\n\n## Testing\n\n- [ ] Unit: soft_delete marks record, is_deleted returns true\n- [ ] Unit: soft_delete returns false for non-existent doc_id\n- [ ] Unit: soft-deleted records excluded from search results\n- [ ] Unit: tombstone_count and tombstone_ratio calculations are correct\n- [ ] Unit: needs_vacuum triggers at correct threshold\n- [ ] Unit: vacuum removes tombstoned records, search results unchanged for non-deleted docs\n- [ ] Unit: vacuum stats (records_before, records_after, bytes_reclaimed) are accurate\n- [ ] Unit: batch delete correctly deletes all specified doc_ids\n- [ ] Integration: delete + search + vacuum full lifecycle\n- [ ] Integration: concurrent delete + search (no corruption)\n- [ ] Benchmark: search overhead with various tombstone ratios (0%, 10%, 20%, 50%)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T22:01:07.444757669Z","created_by":"ubuntu","updated_at":"2026-02-13T22:05:22.548874017Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-sot","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T22:02:23.509595895Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-yrq","title":"Embedding-Stable Document Fingerprinting","description":"Implement content-aware document fingerprinting that detects when a document has changed enough to warrant re-embedding. This bridges the content-hash deduplication layer (bd-3w1.4) and the staleness detector (bd-3w1.12) with embedding-aware change detection.\n\n## Key Insight\n\nNot all document changes affect embedding quality equally. A typo fix doesn't change semantic meaning, but adding a new paragraph does. We need a fingerprint that is stable under minor edits but sensitive to semantic changes.\n\n## Design\n\n```rust\npub struct DocumentFingerprint {\n    pub content_hash: u64,           // Full content FNV-1a hash (exact change detection)\n    pub semantic_hash: u64,          // SimHash of content (approximate semantic similarity)\n    pub char_count: u32,             // Character count (length change detection)\n    pub token_estimate: u32,         // Estimated token count (whitespace split)\n}\n\nimpl DocumentFingerprint {\n    pub fn compute(text: &str) -> Self;\n    pub fn needs_reembedding(&self, other: &Self, threshold: f64) -> bool;\n}\n```\n\n## SimHash Algorithm (Charikar 2002)\n\n1. Tokenize text into shingles (3-word windows)\n2. Hash each shingle with FNV-1a\n3. For each bit position, sum +1 for 1-bits, -1 for 0-bits\n4. Final hash: bit i = 1 if sum_i > 0, else 0\n5. Hamming distance between SimHashes ≈ 1 - cosine similarity of shingle sets\n\n## Re-Embedding Decision Logic\n\n- If content_hash differs AND hamming_distance(semantic_hash_a, semantic_hash_b) > threshold (default: 8/64 bits = 12.5%): re-embed\n- If only content_hash differs but semantic_hash is close: skip re-embedding (minor edit)\n- If char_count change > 20%: always re-embed (significant length change)\n\n## Integration with Staleness Detector (bd-3w1.12)\n\n- Store DocumentFingerprint in FrankenSQLite alongside document metadata\n- Staleness detector compares stored fingerprints with current content\n- Report: \"N documents need re-embedding (M changed significantly, K minor edits skipped)\"\n\n## Why This Matters\n\nRe-embedding is the most expensive operation in frankensearch (128ms per document for quality tier). For xf with 50K tweets, a full re-index takes ~107 minutes. Smart fingerprinting can skip 60-80% of documents that had only minor edits (e.g., metadata updates, formatting changes), reducing re-index time to ~20-40 minutes.\n\n## Testing\n\n- Unit: identical text → identical fingerprint\n- Unit: minor typo → same semantic_hash, different content_hash\n- Unit: significant change → different semantic_hash\n- Unit: needs_reembedding threshold logic\n- Unit: char_count/token_estimate accuracy\n- Integration: fingerprint round-trip through FrankenSQLite storage\n- Benchmark: fingerprint computation throughput (should be >100K docs/sec)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T22:00:14.096481882Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:23.995940997Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-yrq","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T22:02:23.773738815Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-yrq","depends_on_id":"bd-3w1.12","type":"blocks","created_at":"2026-02-13T22:02:23.995892867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-yrq","depends_on_id":"bd-3w1.4","type":"blocks","created_at":"2026-02-13T22:02:23.884886689Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-z3j","title":"MMR Diversified Ranking","description":"Implement Maximum Marginal Relevance (MMR) as an optional post-processing step after RRF fusion. MMR re-ranks results to balance relevance with diversity, preventing near-duplicate results from dominating the top-k.\n\n## Background\n\nHybrid search often surfaces near-duplicate content because both lexical and semantic signals agree that similar documents are relevant. For example, in xf (tweet search), a cluster of retweets or quote-tweets on the same topic may all score highly, pushing diverse but equally-relevant results out of the top-k. MMR (Carbonell & Goldberg, 1998) is the standard approach to mitigating this in information retrieval.\n\n## Algorithm\n\n```\nMMR(d) = lambda * Sim(d, q) - (1-lambda) * max_{d' in S} Sim(d, d')\n```\n\nWhere:\n- lambda in [0,1] controls relevance vs diversity tradeoff (default: 0.7)\n- Sim(d, q) = existing RRF/blend score (relevance to query)\n- Sim(d, d') = cosine similarity between document embeddings (inter-document similarity)\n- S = already-selected documents\n\n### Greedy Selection\n\nMMR uses a greedy iterative selection process:\n1. Select the highest-scoring document first (pure relevance)\n2. For each subsequent selection, compute MMR score for all remaining candidates\n3. Select the candidate with the highest MMR score\n4. Repeat until k documents are selected\n\nComplexity: O(k * n) where k = desired results, n = candidate pool size. For typical k=10, n=30, this is <1ms.\n\n## Implementation\n\n```rust\npub struct MmrConfig {\n    pub lambda: f64,          // Default: 0.7 (relevance-heavy)\n    pub enabled: bool,        // Default: false\n    pub candidate_pool: usize, // Consider top N candidates for re-ranking (default: 3x limit)\n}\n\npub fn mmr_rerank(\n    candidates: &[FusedHit],\n    embeddings: &HashMap<String, Vec<f32>>,  // doc_id -> embedding\n    lambda: f64,\n    limit: usize,\n) -> Vec<FusedHit>;\n```\n\nThe candidate_pool parameter controls how many pre-ranked results to consider for MMR re-ranking. Using 3x the desired limit (e.g., 30 candidates for top-10) provides sufficient diversity without excessive computation. The embeddings map provides the vector representations needed for inter-document similarity computation.\n\n## Justification\n\nWithout MMR, top-10 results may contain 3-4 near-identical items, which is a poor user experience. This is especially valuable for:\n- **cass** (session search): conversation sessions often have similar content across turns\n- **xf** (tweet search): retweets, quote-tweets, and thread replies cluster heavily\n- **mcp_agent_mail_rust**: email threads contain highly redundant content\n\nMMR ensures the top-k covers diverse aspects of the query while still prioritizing relevance (lambda=0.7 means relevance is weighted 2.3x more than diversity).\n\n## Considerations\n\n- Embedding access: MMR needs document embeddings for inter-document similarity. These are already available in the FSVI index. The embeddings parameter should be populated from the same index used for search.\n- Score normalization: RRF scores and cosine similarities are on different scales. Normalize both to [0,1] before combining in the MMR formula.\n- Lambda tuning: provide lambda as a config parameter so consumers can tune it. lambda=1.0 disables diversity entirely (pure relevance), lambda=0.0 is maximum diversity.\n- Interaction with explanations (Idea 2): MMR re-ranking should be reflected in HitExplanation.rank_movement.\n\n## Testing\n\n- [ ] Unit: lambda=1.0 produces pure relevance ordering (same order as input)\n- [ ] Unit: lambda=0.0 produces maximum diversity ordering (most different items first)\n- [ ] Unit: candidate pool exhaustion (fewer candidates than limit returns all candidates)\n- [ ] Unit: single candidate is trivially returned\n- [ ] Unit: two identical documents — second one is penalized\n- [ ] Integration: verify diversity improvement on clustered test corpus (measure pairwise similarity of top-k before/after MMR)\n- [ ] Benchmark: MMR overhead for various candidate pool sizes (30, 100, 300)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T21:59:01.587521455Z","created_by":"ubuntu","updated_at":"2026-02-13T22:02:06.406811774Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-z3j","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T22:02:06.406766600Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z3j","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T22:02:06.296138129Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z3j","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T22:02:06.147745308Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
