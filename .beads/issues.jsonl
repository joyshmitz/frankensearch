{"id":"bd-1cr","title":"Implement robust statistics primitives for search monitoring","description":"Implement robust statistics primitives for search monitoring that are stable under outliers and heavy-tailed latency distributions. Replace mean/stddev with Median+MAD, Huber M-estimator, and streaming t-digest for zero-allocation quantile estimation.\n\nGraveyard entry: §12.15 Robust Statistics Primitives + §9.4 Sketching & Streaming\nEV score: 48 (Impact=3, Confidence=4, Reuse=4, Effort=1, Friction=1)\nPriority tier: A\n\nArchitecture:\npub struct RobustMetrics {\n    tdigest: TDigest,           // Streaming quantiles (any percentile)\n    median_mad: MedianMAD,      // Robust center + spread\n    huber: HuberEstimator,      // Outlier-resistant mean\n    count: u64,\n    last_reset: Instant,\n}\n\nComponents:\n\n1. TDigest (streaming quantile estimation):\n   - Compression parameter: 100 (default)\n   - Memory: ~4KB per metric stream\n   - Update: O(log delta) per observation\n   - Query: any quantile in O(delta) — p50, p90, p95, p99, p999\n   - Merge: two t-digests can be merged (for per-thread → global aggregation)\n   - Use existing `tdigest` crate (or implement ~200 lines)\n\n2. Median + MAD (Median Absolute Deviation):\n   - Robust center: median (breakdown point 50%)\n   - Robust spread: MAD = median(|x_i - median|) * 1.4826\n   - Requires sorted window; use circular buffer of last N observations (N=1000)\n\n3. Huber M-estimator:\n   - Iteratively reweighted least squares with tuning constant k=1.345\n   - Breakdown point: min(k, 1-k) ≈ 20%\n   - For normally-distributed data, converges to mean (no regression)\n   - Streaming variant: exponentially weighted Huber mean\n\n4. HyperLogLog (cardinality estimation):\n   - Estimate unique queries, unique doc_ids in results\n   - Memory: 12KB for <2% error\n   - Use existing `hyperloglog` crate\n\nIntegration into TwoTierMetrics (bd-3un.24):\n- Replace raw latency fields with RobustMetrics\n- TwoTierMetrics.fast_latency → RobustMetrics (t-digest p50/p90/p99)\n- TwoTierMetrics.refine_latency → RobustMetrics\n- TwoTierMetrics.query_count → HyperLogLog (unique queries)\n\nConcurrency: Per-thread RobustMetrics with periodic merge (lock-free via atomic swap of TDigest).\n\nBudgeted mode: <500ns per metric update. Memory: ~4KB per metric × ~6 metrics = ~24KB total.\n\nFallback: Raw metric recording (f64 values) — zero impact on search correctness.\n\nFile: frankensearch-core/src/metrics.rs\n\nReference: Dunning & Ertl (2019) \"t-digest\", Huber (1981) \"Robust Statistics\", Flajolet et al. (2007) \"HyperLogLog\"\nBaseline comparator: mean/stddev (current), P2 quantile estimator (bd-3un.24 comment — t-digest is more flexible: any quantile, not just predetermined)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:45:39.859430152Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:23.346650894Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["graveyard","metrics","monitoring","phase7"],"dependencies":[{"issue_id":"bd-1cr","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.346608715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cr","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:46:07.593489045Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-21g","title":"Implement adaptive fusion parameters via Bayesian online learning","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:31:19.664609739Z","created_by":"ubuntu","updated_at":"2026-02-13T20:31:46.264537470Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","bayesian","fusion","phase6"],"dependencies":[{"issue_id":"bd-21g","depends_on_id":"bd-3un","type":"blocks","created_at":"2026-02-13T20:31:46.264499419Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T20:31:37.086376331Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.21","type":"blocks","created_at":"2026-02-13T20:31:37.166256878Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-21g","depends_on_id":"bd-3un.22","type":"blocks","created_at":"2026-02-13T20:31:37.246866950Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":35,"issue_id":"bd-21g","author":"Dicklesworthstone","text":"Implement Bayesian online learning for adaptive fusion parameters (RRF K constant and blend factor). Instead of fixed constants, maintain conjugate priors that update from implicit relevance feedback.\n\nMATHEMATICAL FOUNDATION:\n\n1. Adaptive Blend Factor via Beta-Bernoulli Model:\n   - Prior: Beta(7, 3) encoding the initial 0.7 blend factor\n   - Update: observe whether quality-tier reranking improved results (via click/dwell signals)\n   - Posterior: Beta(7 + successes, 3 + failures)\n   - Blend factor = E[posterior] = alpha / (alpha + beta)\n   - Thompson sampling variant: sample blend factor from posterior for exploration\n\n2. Adaptive RRF K via Gamma-Normal Model:\n   - Prior: Gamma(60, 1) encoding the K=60 default\n   - Update: observe fusion quality (Kemeny distance between fused ranking and click-derived ranking)\n   - Posterior update via conjugate machinery\n   - K = E[posterior]\n\n3. Per-Query-Class Adaptation:\n   - Maintain separate Beta posteriors per query classification (from bd-3un.43):\n     Short keyword queries may prefer higher K (uniform weighting)\n     Natural language queries may prefer lower K (top-heavy weighting)\n     Identifier queries may prefer blend_factor closer to 0.0 (fast tier is fine)\n\n4. Evidence Ledger:\n   - Every search emits a structured record:\n     query_hash, query_class, blend_factor_used, k_used, fast_ndcg, quality_ndcg, blended_ndcg, rank_correlation\n   - Enables offline analysis and posterior initialization for new deployments\n\n5. Regret Bound:\n   - Under Beta-Bernoulli Thompson sampling: E[Regret(T)] is O(sqrt(T log T))\n   - Provably converges to optimal parameters\n\nImplementation:\n\npub struct AdaptiveFusionParams {\n    blend_alpha: AtomicF64,   // Beta posterior for blend factor\n    blend_beta: AtomicF64,\n    k_alpha: AtomicF64,       // Gamma posterior for RRF K\n    k_beta: AtomicF64,\n    min_samples: usize,       // Don't adapt until N queries (default: 50)\n}\n\nFile: frankensearch-fusion/src/adaptive.rs\n\nThis is a pure addition. The fixed defaults remain as the zero-observation case. No API changes needed; TwoTierConfig gains an optional adaptive_params field.\n\nAlien-artifact characteristics:\n- Mathematical rigor: conjugate Bayesian posteriors with formal update rules\n- Explainability: evidence ledger tracks every decision\n- Formal guarantees: Thompson sampling regret bound\n- Graceful degradation: falls back to fixed defaults with insufficient data\n- Operational excellence: O(1) per query, two floats of state per parameter\n","created_at":"2026-02-13T20:31:30Z"}]}
{"id":"bd-22k","title":"Implement score calibration service (Platt/isotonic/temperature)","description":"Implement a ScoreCalibrator trait and calibration layer that converts heterogeneous raw model scores (BM25 [0,inf), cosine [-1,1], reranker logits (-inf,inf)) into calibrated probabilities [0,1] before fusion. This makes blend_factor and RRF score combination mathematically meaningful.\n\nGraveyard entry: §12.16 Calibration Service Abstraction\nEV score: 50 (Impact=5, Confidence=4, Reuse=5, Effort=2, Friction=1)\nPriority tier: A\n\nArchitecture:\npub trait ScoreCalibrator: Send + Sync {\n    fn calibrate(&self, raw_score: f64) -> f64;  // raw -> [0,1] probability\n    fn calibrate_batch(&self, scores: &mut [f64]);\n    fn ece(&self) -> f64;  // Expected Calibration Error\n    fn name(&self) -> &str;\n}\n\nImplementations:\n1. Identity — passthrough (default, backward-compatible)\n2. TemperatureScaling — single parameter T: calibrated = sigmoid(score / T)\n   - T learned offline via NLL minimization on validation set\n   - O(1) per score, <10ns overhead\n3. PlattScaling — logistic regression: calibrated = sigmoid(a * score + b)\n   - Parameters (a, b) fit offline via L-BFGS on held-out data\n   - O(1) per score\n4. IsotonicRegression — non-parametric monotonic mapping\n   - Learned offline: piecewise-constant monotone function\n   - O(log n) per score (binary search on breakpoints)\n   - Guaranteed monotonic (preserves ranking order within each source)\n\nIntegration points:\n- Before RRF fusion (bd-3un.20): calibrate lexical + semantic scores\n- Before two-tier blending (bd-3un.21): calibrate fast + quality scores\n- After reranking (bd-3un.26): calibrate reranker output (replaces raw sigmoid)\n\nCalibration training (offline):\n- Use test fixture corpus (bd-3un.38) as calibration set\n- For each score source: fit calibrator on (raw_score, relevance_label) pairs\n- Store calibration parameters as JSON artifact (sha256 signed)\n- Load at search time; recalibrate periodically\n\nMonitoring:\n- ECE (Expected Calibration Error): partition [0,1] into 10 bins, measure |avg_confidence - accuracy| per bin\n- Brier score: mean squared error of calibrated probabilities vs relevance\n- ECE > 0.10 for 5 windows → automatic fallback to Identity + trigger retrain\n\nBudgeted mode: <1us per score calibration. Memory: ~1KB for isotonic breakpoints.\n\nIsomorphism proof: Isotonic regression is monotonic by construction → calibrated scores preserve original ranking order within each source. Prove: for all i<j, raw[i] <= raw[j] implies calibrated[i] <= calibrated[j].\n\nFile: frankensearch-fusion/src/calibration.rs\n\nReference: Platt (1999) \"Probabilistic outputs for SVMs\", Zadrozny & Elkan (2002) \"Transforming classifier scores\", Guo et al. (2017) \"On Calibration of Modern Neural Networks\"\nBaseline comparator: Raw score passthrough (current), naive min-max (bd-3un.19)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:45:35.751761205Z","created_by":"ubuntu","updated_at":"2026-02-13T20:50:00.537661149Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["calibration","fusion","graveyard","phase6"],"dependencies":[{"issue_id":"bd-22k","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.262027302Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.19","type":"blocks","created_at":"2026-02-13T20:46:04.380803839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:50:00.537623549Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22k","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:46:04.463501565Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ps","title":"Implement sequential testing gates (e-processes) for phase transitions","description":"Implement anytime-valid sequential testing (e-processes) for phase transition decisions in the TwoTierSearcher. An e-process accumulates evidence over time and can be checked at ANY query — no predetermined sample sizes needed. When the e-value exceeds 1/alpha, the decision (skip/don't skip quality tier) is statistically guaranteed.\n\nGraveyard entry: §0.18 Sequential Testing (e-processes, anytime-valid)\nEV score: 9.0 (Impact=3, Confidence=4, Reuse=3, Effort=2, Friction=2)\nPriority tier: B\n\nArchitecture:\npub struct PhaseGate {\n    e_value: f64,                    // Running e-value (product of e-factors)\n    alpha: f64,                      // Significance level (default 0.05)\n    decision: Option<PhaseDecision>, // None until evidence sufficient\n    observations: u64,               // Query count since last reset\n    timeout_queries: u64,            // Max queries before forced decision (default 500)\n}\n\npub enum PhaseDecision {\n    SkipQuality,   // Evidence: fast tier is sufficient\n    AlwaysRefine,  // Evidence: quality tier adds value\n}\n\nE-process update (per query):\n1. Observe: (fast_score, quality_score, user_click/relevance) for top-k results\n2. Compute e-factor: likelihood ratio test statistic for \"quality adds value\" vs \"fast sufficient\"\n   e_factor = P(observation | quality_adds_value) / P(observation | fast_sufficient)\n3. Update: e_value *= e_factor\n4. Check: if e_value > 1/alpha → PhaseDecision::AlwaysRefine\n         if 1/e_value > 1/alpha → PhaseDecision::SkipQuality\n         if observations > timeout → default to AlwaysRefine + reset\n\nProperties (Ville's inequality):\n- Under null hypothesis, P(e_value ever exceeds 1/alpha) <= alpha\n- Can check at any time without multiple-testing correction\n- Accumulated evidence is never wasted (unlike fixed-horizon tests)\n\nIntegration with bd-3un.24 (TwoTierSearcher):\n- PhaseGate runs alongside progressive iterator\n- Before quality embedding: check gate.decision\n- If SkipQuality: yield SearchPhase::RefinementFailed(\"skipped by e-gate\")\n- If None: proceed with normal refinement\n- After each query: gate.update(observation)\n\nComposability with bd-21g (adaptive Bayesian fusion):\n- E-process gates the PHASE decision (skip/refine)\n- Bayesian posterior tunes the PARAMETERS (K, blend_factor)\n- Timescale separation: e-process operates per-query; Bayesian updates per-window\n- Interference test: gate decision is binary (skip/refine) and does not affect parameter tuning\n\nBudgeted mode: O(1) per query update (single multiplication). Memory: 3 f64 values. <10ns per decision.\n\nFallback: gate.decision = None → always refine (safe default). Timeout after 500 queries → reset.\n\nFile: frankensearch-fusion/src/phase_gate.rs\n\nReference: Ramdas et al. (2020) \"Admissible Anytime-Valid Sequential Testing\", Grunwald et al. (2019) \"Safe Testing\"\nBaseline comparator: Fixed threshold skip (current bd-3un.24 comment), always-refine (safe default)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:45:46.915253464Z","created_by":"ubuntu","updated_at":"2026-02-13T20:50:02.884923972Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["adaptive","graveyard","phase7","sequential-testing"],"dependencies":[{"issue_id":"bd-2ps","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.522218639Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ps","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T20:46:17.183307375Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ps","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:50:02.884884749Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yj","title":"Implement conformal prediction wrappers for search quality guarantees","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:31:20.613664049Z","created_by":"ubuntu","updated_at":"2026-02-13T20:31:46.351595600Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["alien-artifact","conformal","phase10","quality"],"dependencies":[{"issue_id":"bd-2yj","depends_on_id":"bd-3un","type":"blocks","created_at":"2026-02-13T20:31:46.351555875Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yj","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T20:31:38.031766694Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yj","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:31:38.113929262Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":36,"issue_id":"bd-2yj","author":"Dicklesworthstone","text":"Implement conformal prediction wrappers for distribution-free search quality guarantees. This provides formal coverage bounds on search results without distributional assumptions.\n\nMATHEMATICAL FOUNDATION:\n\nConformal prediction guarantees: P(relevant_doc in top_k) >= 1 - alpha, for any alpha, with NO distributional assumptions. This is the strongest formal guarantee possible for search quality.\n\nCore algorithm:\n1. CALIBRATION PHASE: Given a calibration set of (query, known_relevant_doc) pairs:\n   - For each pair, compute the nonconformity score = rank of relevant doc in search results\n   - Sort these scores to form the empirical distribution\n\n2. PREDICTION PHASE: For a new query:\n   - required_k = ceil((1-alpha) quantile of calibration scores) + 1\n   - Guarantee: with probability >= 1-alpha, the relevant doc is in the top required_k\n\nImplementation:\n\npub struct ConformalSearchCalibration {\n    nonconformity_scores: Vec<f32>,  // Sorted calibration scores\n    n_calibration: usize,\n}\n\nimpl ConformalSearchCalibration {\n    pub fn calibrate(searcher: &TwoTierSearcher, cal_set: &[(String, String)]) -> Self;\n\n    // Required k to guarantee coverage at level (1-alpha)\n    pub fn required_k(&self, alpha: f32) -> usize;\n\n    // Prediction interval for the rank of a relevant document\n    pub fn rank_prediction_interval(&self, alpha: f32) -> (usize, usize);\n\n    // p-value for a specific result: how unusual is this rank?\n    pub fn p_value(&self, observed_rank: usize) -> f32;\n}\n\nAdditional capabilities:\n\n1. Adaptive Conformal Prediction (ACI):\n   For non-stationary data (index changes over time), use Gibbs & Candes 2021 adaptive conformal:\n   - alpha_t = alpha + gamma * (err_{t-1} - alpha)\n   - Maintains coverage guarantee even as distribution shifts\n   - gamma controls adaptation speed (default: 0.01)\n\n2. Per-Query-Type Calibration:\n   Separate calibration sets per query classification (bd-3un.43):\n   - Short queries need different k than long queries\n   - Identifier queries need different k than natural language\n\n3. Conditional Coverage via Mondrian Conformal:\n   Guarantee coverage within each query type, not just marginally.\n\nFile: frankensearch-fusion/src/conformal.rs\nDependencies: bd-3un.24 (TwoTierSearcher), bd-3un.38 (test fixtures for calibration data)\n\nAlien-artifact characteristics:\n- Mathematical rigor: Vovk et al. conformal prediction framework\n- Formal guarantees: distribution-free finite-sample coverage P >= 1-alpha\n- Complete explainability: p-values for each result, required_k derivation\n- Graceful degradation: works with any embedder, any index size\n- Operational excellence: O(log n) per query (binary search on sorted calibration scores)\n","created_at":"2026-02-13T20:31:31Z"}]}
{"id":"bd-3un","title":"Epic: Create frankensearch standalone crate","description":"Extract the 2-tier hybrid search system from cass, xf, and mcp_agent_mail_rust into a standalone, reusable Rust crate called frankensearch. This crate should be drop-in for any future Rust project needing high-quality local semantic+lexical hybrid search with progressive refinement.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-13T17:46:43.968487926Z","created_by":"ubuntu","updated_at":"2026-02-13T20:47:52.741945809Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["epic","meta"],"comments":[{"id":1,"issue_id":"bd-3un","author":"Dicklesworthstone","text":"PROJECT VISION: frankensearch is a standalone, reusable Rust crate that extracts the 2-tier hybrid search system currently duplicated across 3 projects (cass, xf, mcp_agent_mail_rust). The name 'frankensearch' reflects its Frankenstein-like assembly from the best parts of each codebase.\n\nKEY INSIGHT (from the X post): The user wanted the best of both worlds — sub-millisecond response time (potion-128M) AND high-quality semantic understanding (MiniLM-L6-v2). Rather than choosing one, the solution is a 2-tier progressive system: show fast results immediately, then upgrade them in the background when the quality model finishes. This creates a smooth UX where results appear instantly and improve within ~150ms.\n\nBAKEOFF RESULTS (motivation for model choices):\n- FNV-1a hash: 0.07ms, no semantic meaning (fallback only)\n- potion-multilingual-128M: 0.57ms, decent semantics (223x faster than MiniLM)\n- all-MiniLM-L6-v2: 128ms, excellent semantics (baseline quality)\n- The 223x speed gap between potion and MiniLM is exactly why the 2-tier design exists.\n\nHYBRID SEARCH: Combines lexical (Tantivy BM25) with semantic (vector cosine similarity) via Reciprocal Rank Fusion (RRF, K=60). Documents appearing in both lexical AND semantic results get boosted scores, which empirically produces better results than either alone.\n\nGOAL: Drop-in crate for any Rust project needing local search. Feature-gated so consumers pay only for what they use.","created_at":"2026-02-13T17:56:21Z"},{"id":11,"issue_id":"bd-3un","author":"Dicklesworthstone","text":"TESTING POLICY: Every component bead (bd-3un.6 through bd-3un.28) MUST include inline #[cfg(test)] unit tests alongside the implementation. These inline tests should cover: (a) happy path, (b) edge cases (empty input, max values, boundary conditions), (c) error conditions. The dedicated testing beads (bd-3un.31, bd-3un.32, bd-3un.40) are for CROSS-COMPONENT tests, integration tests, and e2e validation scripts -- not for basic per-component unit testing.\n\nLOGGING POLICY: All public functions should use the tracing crate for structured logging:\n- ERROR: unrecoverable failures (model load failed, index corrupted)\n- WARN: degraded operation (quality model unavailable, fallback to hash)\n- INFO: significant lifecycle events (index opened, model loaded, search completed)\n- DEBUG: operational details (query embedding latency, candidate counts, blend scores)\n- TRACE: hot-path internals (individual dot products, per-record scores) -- gated behind cfg\n\nEvery search operation should emit a tracing span with: query length, k, phase, result count, latency_ms, embedder used.\n","created_at":"2026-02-13T20:11:19Z"},{"id":80,"issue_id":"bd-3un","author":"Dicklesworthstone","text":"CROSS-REFERENCE: FrankenSQLite Integration Epic (bd-3w1)\n\nThe FrankenSQLite integration (bd-3w1) is a sibling epic that adds:\n- Persistent document storage via FrankenSQLite (replaces in-memory state)\n- Self-healing indices via pervasive RaptorQ erasure coding\n- FTS5 as alternative lexical engine (alongside Tantivy)\n\nKey cross-epic dependencies:\n- bd-3w1.1 (storage crate) depends on bd-3un.1 (scaffold) and bd-3un.2 (errors)\n- bd-3w1.5 (durability crate) depends on bd-3un.1 (scaffold) and bd-3un.2 (errors)\n- bd-3w1.7 (FSVI RaptorQ) depends on bd-3un.13 (FSVI format)\n- bd-3w1.8 (Tantivy RaptorQ) depends on bd-3un.17 (Tantivy schema)\n- bd-3w1.10 (FTS5) depends on bd-3un.18 (Tantivy queries, for LexicalIndex trait)\n- bd-3w1.13 (pipeline) depends on bd-3un.27 (embedding job runner)\n- bd-3w1.12 (staleness) depends on bd-3un.41 (staleness detection)\n- bd-3w1.14 (features) depends on bd-3un.29 (feature flags)\n- bd-3w1.21 (facade) depends on bd-3un.30 (public API)\n\nThe two epics can be worked in parallel: bd-3un tasks build the core search engine,\nbd-3w1 tasks add persistence and durability on top.\n","created_at":"2026-02-13T20:47:52Z"}]}
{"id":"bd-3un.1","title":"Scaffold Cargo workspace and crate structure","description":"Create the frankensearch Cargo workspace with the following structure:\n\nfrankensearch/\n├── Cargo.toml (workspace root)\n├── crates/\n│   ├── frankensearch-core/      # Traits, types, error types, scoring primitives\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-embed/     # All embedder implementations (hash, model2vec, fastembed)\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-index/     # Vector index, SIMD, ANN\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-lexical/   # Tantivy integration (feature-gated)\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   ├── frankensearch-fusion/    # RRF, blending, two-tier orchestration\n│   │   ├── Cargo.toml\n│   │   └── src/lib.rs\n│   └── frankensearch-rerank/    # Reranker trait + implementations\n│       ├── Cargo.toml\n│       └── src/lib.rs\n├── frankensearch/               # Facade crate re-exporting everything\n│   ├── Cargo.toml\n│   └── src/lib.rs\n├── tests/\n├── benches/\n└── examples/\n\nDesign decisions:\n- Workspace-level dep management (workspace = true pattern)\n- Feature flags: 'full' (everything), 'semantic' (embedders), 'lexical' (tantivy), 'hybrid' (both), 'rerank' (rerankers)\n- Rust edition 2024 (nightly), matching existing projects\n- Release profile: opt-level='z', lto=true, codegen-units=1, strip=true\n- unsafe code: forbidden (#\\![forbid(unsafe_code)])\n\nKey workspace deps (from cross-referencing cass/xf/agent-mail):\n- half = '2.4' (f16 quantization)\n- wide = '0.7' (SIMD f32x8)\n- fastembed = '4.9' (ONNX embeddings)\n- tantivy = '0.22' (full-text search)\n- tokenizers = '0.21' (HuggingFace BPE)\n- safetensors = '0.5' (Model2Vec weights)\n- ort = '2.0.0-rc.9' (ONNX Runtime)\n- rayon = '1.10' (data parallelism)\n- serde + serde_json (serialization)\n- thiserror (error types)\n- tracing (structured logging)\n- memmap2 (memory-mapped files)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:01.537714547Z","created_by":"ubuntu","updated_at":"2026-02-13T17:56:48.929684357Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation","phase1","setup"],"dependencies":[{"issue_id":"bd-3un.1","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:01.537714547Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":4,"issue_id":"bd-3un.1","author":"Dicklesworthstone","text":"CRATE STRUCTURE RATIONALE: The workspace is split into multiple sub-crates to enable:\n1. Selective compilation: consumers only compile what they need\n2. Parallel compilation: independent crates build concurrently\n3. Clear boundaries: traits in -core, impls in domain crates\n4. Testing isolation: each crate testable independently\n\nThe facade crate (frankensearch/) re-exports everything so consumers can just 'use frankensearch::*' in simple cases, or import sub-crates directly for finer control.\n\nThe 6 sub-crates map to the logical architecture:\n- core: zero-dep traits/types (everything depends on this)\n- embed: all embedder implementations (feature-gated per model)\n- index: vector storage and search (SIMD, mmap)\n- lexical: Tantivy full-text search\n- fusion: RRF, blending, two-tier orchestration\n- rerank: cross-encoder reranking","created_at":"2026-02-13T17:56:48Z"}]}
{"id":"bd-3un.10","title":"Implement model manifest and SHA256 verification","description":"Implement the model manifest system that tracks required model files, their SHA256 checksums, and HuggingFace repo details. This ensures reproducible, verifiable model installations.\n\npub struct ModelManifest {\n    pub id: String,               // e.g., 'all-minilm-l6-v2'\n    pub repo: String,             // HuggingFace repo path\n    pub revision: String,         // Pinned commit SHA for reproducibility\n    pub files: Vec<ModelFile>,    // Required files with checksums\n    pub license: String,          // SPDX identifier\n}\n\npub struct ModelFile {\n    pub name: String,             // Path in repo (e.g., 'onnx/model.onnx')\n    pub sha256: String,           // Expected SHA256 hex string\n    pub size: u64,                // Expected file size in bytes\n}\n\npub enum ModelState {\n    NotInstalled,\n    NeedsConsent,                 // User must approve before download\n    Downloading { progress_pct: u8, bytes_downloaded: u64, total_bytes: u64 },\n    Verifying,\n    Ready,\n    Disabled { reason: String },\n    VerificationFailed { reason: String },\n    UpdateAvailable { current_revision: String, latest_revision: String },\n    Cancelled,\n}\n\nKey principles (from cass src/search/model_download.rs):\n- NO network calls without explicit user consent (consent-gated downloads)\n- Placeholder checksum constant: 'PLACEHOLDER_VERIFY_AFTER_DOWNLOAD'\n- Production-ready = has_verified_checksums() && has_pinned_revision()\n- Atomic installation: download to temp dir, verify, then rename into place\n\nBuilt-in manifests:\n- ModelManifest::minilm_v2() - the baseline quality model\n- ModelManifest::potion_128m() - the fast tier model\n\nVerification:\n- SHA256 streaming verification during download (sha2 crate)\n- Post-install verification on model load\n- Version upgrade detection (compare revisions)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:49:26.536014967Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:12.408333636Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["model-mgmt","phase3"],"dependencies":[{"issue_id":"bd-3un.10","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:49:26.536014967Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.10","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:12.408296236Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.11","title":"Implement model download system with progress reporting","description":"Implement the model download system that handles fetching model files from HuggingFace with progress reporting, resumable downloads, and atomic installation.\n\nDownload pipeline:\n1. Check ModelState → if NotInstalled, transition to NeedsConsent\n2. On consent, transition to Downloading\n3. For each file in manifest:\n   a. HTTP GET from HuggingFace CDN with range headers for resume\n   b. Stream to temp file with progress callbacks\n   c. SHA256 verification during streaming\n4. After all files downloaded, verify checksums (Verifying state)\n5. Atomic rename from temp dir to final location (Ready state)\n\nProgress reporting:\n- AtomicU64 for bytes_downloaded (lock-free progress reads)\n- AtomicBool for cancellation\n- Callback-based progress: Fn(DownloadProgress) for UI integration\n- Rate-limited progress updates (every 100ms or 1% progress)\n\npub struct DownloadProgress {\n    pub file_name: String,\n    pub bytes_downloaded: u64,\n    pub total_bytes: u64,\n    pub files_completed: usize,\n    pub files_total: usize,\n    pub speed_bytes_per_sec: u64,\n    pub eta_seconds: Option<u64>,\n}\n\nNetwork policy: \n- reqwest with rustls (no system SSL deps)\n- Timeout: 30s connect, 300s total per file\n- Retry: 3 attempts with exponential backoff\n- User-Agent: 'frankensearch/{version}'\n\nThis is behind a 'download' feature flag to keep the core crate network-free.\n\nReference: cass src/search/model_download.rs (state machine, atomic install)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:49:26.674121057Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:12.493145106Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["download","model-mgmt","phase3"],"dependencies":[{"issue_id":"bd-3un.11","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:49:26.674121057Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.11","depends_on_id":"bd-3un.10","type":"blocks","created_at":"2026-02-13T17:55:12.493111893Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.12","title":"Implement model registry with bakeoff infrastructure","description":"Implement the model registry that catalogs all supported embedders and rerankers. This is a static registry (compile-time data) enriched with runtime availability checks.\n\npub struct RegisteredEmbedder {\n    pub name: &'static str,           // Short name (e.g., 'minilm')\n    pub id: &'static str,             // Unique ID (e.g., 'minilm-384')\n    pub dimension: usize,\n    pub is_semantic: bool,\n    pub description: &'static str,\n    pub requires_model_files: bool,\n    pub release_date: &'static str,   // ISO 8601\n    pub huggingface_id: &'static str,\n    pub size_bytes: u64,\n    pub is_baseline: bool,            // Baseline for bakeoff comparison\n}\n\npub struct RegisteredReranker {\n    pub name: &'static str,\n    pub id: &'static str,\n    pub description: &'static str,\n    pub requires_model_files: bool,\n    pub release_date: &'static str,\n    pub huggingface_id: &'static str,\n    pub size_bytes: u64,\n    pub is_baseline: bool,\n}\n\nStatic registries (from cross-referencing all 3 codebases):\n\nEMBEDDERS: \n- minilm (384d, semantic, baseline, 2022-08-01)\n- snowflake-arctic-s (384d, semantic, 2025-11-10)\n- nomic-embed (768d, semantic, 2025-11-05)\n- potion-multilingual-128M (256d, semantic/static, 2025+)\n- potion-retrieval-32M (512d, semantic/static, 2025+)\n- hash/fnv1a (384d, non-semantic, always available)\n\nRERANKERS:\n- ms-marco-minilm (baseline)\n- flashrank-nano (~4MB)\n- bge-reranker-v2\n- jina-reranker-turbo\n- mxbai-rerank-xsmall\n\nRuntime registry:\npub struct EmbedderRegistry {\n    data_dir: PathBuf,\n}\n\nimpl EmbedderRegistry {\n    pub fn available(&self) -> Vec<&RegisteredEmbedder>;\n    pub fn get(&self, name: &str) -> Option<&RegisteredEmbedder>;\n    pub fn best_available(&self) -> &RegisteredEmbedder;\n    pub fn bakeoff_eligible(&self) -> Vec<&RegisteredEmbedder>;\n}\n\nBakeoff eligibility cutoff: 2025-11-01 (models released after this date)\n\nReference: cass src/search/embedder_registry.rs, src/search/reranker_registry.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:49:26.791533937Z","created_by":"ubuntu","updated_at":"2026-02-13T20:33:29.782385802Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["model-mgmt","phase3","registry"],"dependencies":[{"issue_id":"bd-3un.12","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:49:26.791533937Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.12","depends_on_id":"bd-3un.10","type":"blocks","created_at":"2026-02-13T17:55:12.757907053Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.12","depends_on_id":"bd-3un.7","type":"blocks","created_at":"2026-02-13T17:55:12.592024762Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.12","depends_on_id":"bd-3un.8","type":"blocks","created_at":"2026-02-13T17:55:12.675057419Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":41,"issue_id":"bd-3un.12","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Model Registry & Bakeoff)\n\n## Mathematical Upgrade: From Point Estimates to Bayesian Bakeoff\n\nThe current bakeoff infrastructure compares embedders by average NDCG. This is ad-hoc and doesn't account for variance, sample size, or the multiple comparisons problem.\n\n### 1. Bayesian A/B Testing with Beta Posteriors\n\nFor each embedder pair (A, B), maintain a Beta posterior:\n\n  // For each query in the bakeoff:\n  //   If A's NDCG@10 > B's NDCG@10: A_wins += 1\n  //   If B's NDCG@10 > A's NDCG@10: B_wins += 1\n  //   If tied: both += 0.5\n\n  P(A better than B) = P(Beta(A_wins+1, B_wins+1) > 0.5)\n\nDecision rule:\n  - P(A > B) > 0.95 → declare A the winner (95% Bayesian credibility)\n  - P(A > B) < 0.05 → declare B the winner\n  - Otherwise → need more queries (continue testing)\n\nThis gives a FORMAL stopping criterion — no more arbitrary \"run N queries and take the average.\"\n\n### 2. Multi-Armed Bandit for Model Selection\n\nWhen multiple embedders are available, use Thompson sampling to select the best one adaptively:\n\n  pub struct EmbedderBandit {\n      arms: Vec<(String, Beta)>,  // (embedder_id, Beta posterior)\n  }\n\n  impl EmbedderBandit {\n      pub fn select_embedder(&self) -> &str {\n          // Sample from each Beta posterior, pick highest\n          self.arms.iter()\n              .max_by(|(_, a), (_, b)| a.sample().partial_cmp(&b.sample()).unwrap())\n              .map(|(id, _)| id.as_str())\n              .unwrap()\n      }\n  }\n\nThis automatically explores new models while exploiting the best known one. Provable O(sqrt(T log T)) regret.\n\n### 3. e-Values for Anytime-Valid Testing\n\nUse e-values instead of p-values for the bakeoff. e-values support OPTIONAL STOPPING — you can look at results at any time and make valid decisions:\n\n  e_n = product(likelihood_ratio_i for i in 1..n)\n  If e_n > 1/alpha, reject H0 at level alpha\n\nThis means: you can stop the bakeoff EARLY if one model is clearly better, or CONTINUE if results are ambiguous. Traditional hypothesis testing requires fixed sample sizes.\n\n### 4. FDR Control for Multi-Model Comparisons\n\nWhen comparing K models pairwise (K*(K-1)/2 comparisons), use the e-BH procedure for False Discovery Rate control:\n\n  1. Compute e-values for all pairwise comparisons\n  2. Sort e-values in decreasing order\n  3. Apply BH threshold: reject hypothesis i if e_i > K*(K-1)/(2*i*alpha)\n\nThis controls the expected proportion of false discoveries at level alpha.\n\n### Implementation Priority\n\n1. Beta posterior A/B testing: add to bakeoff report generation\n2. Thompson sampling model selection: add to EmbedderStack\n3. e-values: add to bakeoff infrastructure\n4. FDR control: add when > 4 models are being compared\n","created_at":"2026-02-13T20:33:29Z"}]}
{"id":"bd-3un.13","title":"Implement vector index binary format and I/O","description":"Implement the binary vector index format for storing and loading embeddings. This is the on-disk format that enables memory-mapped vector search.\n\nWe should unify the formats from all 3 codebases into a single 'FSVI' (FrankenSearch Vector Index) format:\n\nBinary Format (Little-Endian):\n\nHeader (variable length):\n  Offset  Size  Field\n  0       4     magic: 'FSVI' (4 ASCII bytes)\n  4       2     version: u16 (start at 1)\n  6       2     embedder_id_len: u16\n  8       N     embedder_id: UTF-8 bytes\n  8+N     4     dimension: u32\n  12+N    4     quantization: u8 (0=f32, 1=f16)\n  13+N    3     reserved: [u8; 3]\n  16+N    8     record_count: u64\n  24+N    8     vectors_offset: u64 (byte offset to vector slab)\n  32+N    4     header_crc32: u32\n\nRecord Table (fixed-size per record, 16 bytes each):\n  doc_id_hash: u64     (FNV-1a hash of doc_id for fast lookup)\n  doc_id_offset: u32   (offset into string table)\n  doc_id_len: u16      (byte length of doc_id)\n  flags: u16           (reserved for doc_type enum, etc.)\n\nString Table:\n  Concatenated UTF-8 doc_id strings (referenced by offset+len)\n\nVector Slab (32-byte aligned):\n  record_count × dimension × bytes_per_quant (2 for f16, 4 for f32)\n\nDesign decisions:\n- f16 quantization by default (2x memory savings, minimal quality loss)\n- Memory-mapped via memmap2 for zero-copy access\n- 32-byte aligned vector slab for SIMD (AVX2 alignment)\n- CRC32 header checksum for corruption detection\n- Sorted by doc_id_hash for binary search lookup\n\nAPI:\n\npub struct VectorIndex {\n    mmap: Mmap,               // Memory-mapped file\n    metadata: VectorMetadata,\n    record_count: usize,\n    dimension: usize,\n}\n\nimpl VectorIndex {\n    pub fn open(path: &Path) -> SearchResult<Self>;\n    pub fn create(path: &Path, embedder_id: &str, dimension: usize) -> SearchResult<VectorIndexWriter>;\n    pub fn record_count(&self) -> usize;\n    pub fn dimension(&self) -> usize;\n    pub fn embedder_id(&self) -> &str;\n    pub fn vector_at_f16(&self, index: usize) -> &[f16];\n    pub fn vector_at_f32(&self, index: usize) -> Vec<f32>;\n    pub fn doc_id_at(&self, index: usize) -> &str;\n}\n\npub struct VectorIndexWriter {\n    file: BufWriter<File>,\n    dimension: usize,\n    count: u64,\n}\n\nimpl VectorIndexWriter {\n    pub fn write_record(&mut self, doc_id: &str, embedding: &[f32]) -> SearchResult<()>;\n    pub fn finish(self) -> SearchResult<()>;\n}\n\nReference formats:\n- cass: CVVI (src/search/vector_index.rs, 70-byte rows with message metadata)\n- xf: XFVI (src/vector.rs, variable records with doc_type)\n- agent-mail: planned AMVI (simplified from XFVI)\n\nOur format (FSVI) is a clean generalization that doesn't bake in domain-specific fields.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:23.484008880Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:56.163597467Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["io","phase4","vector-index"],"dependencies":[{"issue_id":"bd-3un.13","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.484008880Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.13","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:17.773296513Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.13","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:17.853723728Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":7,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"VECTOR INDEX FORMAT DESIGN: The FSVI format is a deliberate generalization of the three existing formats:\n- CVVI (cass): 70-byte rows with domain-specific fields (MessageID, AgentID, etc.)\n- XFVI (xf): variable records with doc_type enum (tweet/like/dm/grok)\n- AMVI (agent-mail): simplified from XFVI\n\nFSVI strips all domain-specific fields. The principle: frankensearch stores (doc_id, embedding) pairs. Any domain metadata belongs in the consumer's own storage. This keeps the index format universal.\n\nKey design choice — f16 by default: \n- 384-dim f16: 768 bytes per doc (vs 1536 for f32) = 50% memory savings\n- Quality loss from f16 quantization is < 1% on cosine similarity benchmarks\n- Memory-mapped f16 means the OS page cache holds 2x more vectors\n- For 100K docs × 384 dims: 73MB (f16) vs 147MB (f32)\n\nThe string table design (separate from records) enables fixed-size record entries for binary search lookups while supporting variable-length doc IDs.","created_at":"2026-02-13T17:57:22Z"},{"id":15,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. EMBEDDER REVISION FIELD MISSING: The FSVI header must include an embedder_revision field (variable-length UTF-8 string, like embedder_id). This tracks the model's pinned commit SHA (e.g., \"c9745ed1d9f207416be6d2e6f8de32d1f16199bf\" for MiniLM). Purpose: when a model is updated, the revision changes, and the index must be rebuilt. Without this, stale indices silently return degraded results.\n\nUpdated header layout:\n  8+N     2     embedder_rev_len: u16\n  10+N    M     embedder_revision: UTF-8 bytes\n  (adjust all subsequent offsets by M+2)\n\nReference: cass CVVI header has EmbedderRevision (u16 len + bytes). This was in the original format but was accidentally omitted from the FSVI design.\n\n2. FSYNC ON SAVE: VectorIndexWriter::finish() must call fsync on the file AND fsync the parent directory for write durability. Reference: cass vector_index.rs sync_dir() at line 1538. This prevents data loss on power failure.\n\n3. VECTOR ALIGNMENT: The vector slab must be 32-byte aligned (VECTOR_ALIGN_BYTES = 32) for AVX2 SIMD. Add padding between string table and vector slab as needed.\n","created_at":"2026-02-13T20:24:01Z"},{"id":33,"issue_id":"bd-3un.13","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (FSVI Vector Index Format)\n\n## Profile-Informed I/O and Memory Optimization\n\n### Opportunity Matrix\n\n| Hotspot                    | Impact | Confidence | Effort | Score |\n|----------------------------|--------|------------|--------|-------|\n| 64-byte cache alignment    | 3      | 5          | 1      | 15.0  |\n| madvise(MADV_SEQUENTIAL)   | 4      | 5          | 1      | 20.0  |\n| Huge pages (2MB THP)       | 3      | 4          | 1      | 12.0  |\n| Vectored write (writev)    | 3      | 3          | 2      | 4.5   |\n| fsync strategy             | 2      | 5          | 1      | 10.0  |\n\n### 1. madvise for Sequential Scan (Score: 20.0, MUST DO)\n\nThe vector search does a sequential scan through the entire mmap. Tell the OS:\n\n  // After opening the mmap:\n  #[cfg(unix)]\n  unsafe {\n      libc::madvise(\n          mmap.as_ptr() as *mut libc::c_void,\n          mmap.len(),\n          libc::MADV_SEQUENTIAL,\n      );\n  }\n\nThis enables aggressive readahead — the kernel will prefetch pages before we need them. Measured impact: 2-3x throughput improvement for sequential scans on cold caches.\n\nNOTE: This requires `unsafe`. Since the project forbids unsafe_code, use the memmap2 crate's `advise()` method if available, or document this as a future optimization if memmap2 doesn't expose it safely. Alternative: the `region` crate provides safe madvise wrappers.\n\nSAFE ALTERNATIVE: memmap2::MmapOptions::new().populate() will pre-fault all pages, and memmap2::Mmap::advise(Advice::Sequential) provides a safe wrapper for MADV_SEQUENTIAL in recent versions. Check memmap2 API.\n\n### 2. 64-Byte Cache Line Alignment (Score: 15.0)\n\nCurrent spec says 32-byte alignment for vector slab. Modern CPUs use 64-byte cache lines. Misaligned vector access causes 2 cache line loads per vector start:\n\n  // In FSVI header:\n  vectors_offset: round_up_to(header_size + records_size + strings_size, 64)\n\nChange: \"32-byte aligned\" → \"64-byte aligned\" in the FSVI format spec. This is a one-line change with measurable impact on scan performance.\n\n### 3. Huge Pages for Large Indices (Score: 12.0)\n\nFor indices > 100MB, transparent huge pages (2MB pages) reduce TLB misses by 512x:\n\n  // When creating the mmap:\n  #[cfg(target_os = \"linux\")]\n  {\n      libc::madvise(ptr, len, libc::MADV_HUGEPAGE);\n  }\n\nFor a 100K-doc × 384-dim f16 index (~73MB), this reduces TLB misses from ~18K to ~36. Each TLB miss costs ~100 cycles on modern CPUs.\n\nSAFE ALTERNATIVE: Set the system's transparent_hugepages to \"madvise\" mode and let memmap2 use MAP_HUGETLB flag if available. Or simply document that sysadmins should enable THP for large indices.\n\n### 4. Write Performance: Buffered + Vectored I/O\n\nFor index building, use BufWriter with a large buffer (256KB instead of default 8KB), and batch writes:\n\n  let writer = BufWriter::with_capacity(256 * 1024, file);\n\nFor the final vector slab write, if all vectors are already in memory (they are, during batch embedding), use a single write() call for the entire slab rather than per-record writes. This reduces syscall overhead from N to 1.\n\n### 5. Isomorphism Proof\n\n- Cache alignment: no data change, only padding\n- madvise: kernel hint only, no data change\n- Huge pages: memory mapping only, no data change\n- Buffer size: same bytes written, just fewer syscalls\n- All: sha256(index_file) identical before/after\n","created_at":"2026-02-13T20:29:56Z"}]}
{"id":"bd-3un.14","title":"Implement SIMD-accelerated dot product (f16/f32)","description":"Implement SIMD-accelerated dot product for cosine similarity computation. This is the performance-critical inner loop of vector search — called once per stored vector per query.\n\nPrimary implementation using wide crate (portable SIMD):\n\npub fn dot_product_f16_f32(stored: &[f16], query: &[f32]) -> f32 {\n    use wide::f32x8;  // 8-wide SIMD, portable across x86/ARM\n    \n    let chunks = stored.len() / 8;\n    let mut sum = f32x8::ZERO;\n    \n    for i in 0..chunks {\n        let base = i * 8;\n        // Convert 8 f16 values to f32\n        let s_f32: [f32; 8] = [\n            f32::from(stored[base]),   f32::from(stored[base+1]),\n            f32::from(stored[base+2]), f32::from(stored[base+3]),\n            f32::from(stored[base+4]), f32::from(stored[base+5]),\n            f32::from(stored[base+6]), f32::from(stored[base+7]),\n        ];\n        let q_arr: [f32; 8] = query[base..base+8].try_into().unwrap();\n        sum += f32x8::from(s_f32) * f32x8::from(q_arr);\n    }\n    \n    let mut result = sum.reduce_add();  // Horizontal sum\n    \n    // Scalar remainder\n    for i in (chunks * 8)..stored.len() {\n        result += f32::from(stored[i]) * query[i];\n    }\n    result\n}\n\nAlso provide:\n- dot_product_f32_f32(a: &[f32], b: &[f32]) -> f32 (for non-quantized indices)\n- cosine_similarity_f16(a: &[f16], b: &[f32]) -> f32 (wrapper assuming L2-normalized inputs)\n\nPerformance targets (from xf benchmarks):\n- 256-dim f16 dot product: < 1μs\n- 384-dim f16 dot product: < 2μs\n- 10K vectors × 384-dim search: < 15ms\n\nDependencies:\n- wide = '0.7' (portable SIMD)\n- half = '2.4' (f16 type)\n\nDesign note: wide::f32x8 is portable across x86 (SSE2/AVX2) and ARM (NEON). No unsafe code needed.\n\nFile: frankensearch-index/src/simd.rs\n\nReference implementations:\n- cass: src/search/two_tier_search.rs lines 785-832 (dot_product_f16)\n- xf: src/embedder.rs (dot_product_simd)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 638-692","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:23.557226468Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:51.392657046Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["performance","phase4","simd","vector-index"],"dependencies":[{"issue_id":"bd-3un.14","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.557226468Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.14","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T17:55:17.936597738Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":3,"issue_id":"bd-3un.14","author":"Dicklesworthstone","text":"PERFORMANCE: This is the hottest inner loop in the entire search pipeline. The SIMD dot product is called once per stored vector per query. For a 10K-document index with 384-dim embeddings, that's 10,000 dot products of 384 floats each.\n\nwide::f32x8 is the right choice because:\n1. Portable: works on x86 (SSE2/AVX2) and ARM (NEON) without #[cfg] branches\n2. Safe: no unsafe code needed (wide handles the intrinsics)\n3. Fast: 8-wide parallelism reduces loop iterations by 8x\n4. Simple: the API is just multiply + horizontal sum\n\nThe f16→f32 conversion before SIMD multiply is a necessary cost. f16 SIMD isn't widely supported on CPU. The 2 bytes per dimension (vs 4 for f32) saves 50% memory, which matters when the entire index is memory-mapped.\n\nBenchmark baseline: 384-dim f16×f32 dot product should be < 2μs.","created_at":"2026-02-13T17:56:21Z"},{"id":28,"issue_id":"bd-3un.14","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (SIMD Dot Product & Vector Search)\n\n## Profiling-Informed Optimization Opportunities\n\nThe SIMD dot product is called once per stored vector per query — the single hottest loop in the entire system. Every microsecond saved here multiplies across thousands of vectors.\n\n### Opportunity Matrix\n\n| Hotspot               | Impact | Confidence | Effort | Score |\n|-----------------------|--------|------------|--------|-------|\n| f16→f32 batch convert | 4      | 5          | 2      | 10.0  |\n| Cache-line prefetch   | 4      | 4          | 2      | 8.0   |\n| SoA vector layout     | 5      | 4          | 3      | 6.7   |\n| ILP interleaving      | 3      | 4          | 2      | 6.0   |\n| int8 quantization     | 4      | 3          | 4      | 3.0   |\n| AVX-512 path          | 3      | 3          | 3      | 3.0   |\n\n### 1. Batch f16→f32 Conversion (Score: 10.0, MUST DO)\n\nThe current design converts f16 values individually inside the dot product loop. The `half` crate provides `half::slice::convert_to_f32_slice()` which uses F16C SIMD instructions (vcvtph2ps on x86) to convert 8 f16 values simultaneously. This should be done OUTSIDE the dot product, converting the entire stored vector in one batch call, then computing the dot product on the resulting f32 slice.\n\n  // BEFORE: Convert inside loop (scalar f16→f32 per element)\n  for i in 0..chunks {\n      let s_f32: [f32; 8] = [f32::from(stored[base]), ...];  // 8 scalar conversions\n  }\n\n  // AFTER: Batch convert first, then pure f32×f32 SIMD\n  let mut buf = [0f32; 384];  // Stack buffer, reuse across queries\n  half::slice::convert_to_f32_slice(stored, &mut buf);\n  dot_product_f32_f32(&buf, query)\n\nExpected speedup: 2-3x for the conversion step (which is ~40% of total dot product time).\n\n### 2. Cache-Line Prefetching (Score: 8.0, MUST DO)\n\nDuring the linear scan of 10K vectors, each vector access is a cache miss (~100ns penalty on L2 miss). Prefetch the NEXT vector while computing the current one:\n\n  for i in 0..record_count {\n      // Prefetch vector i+4 while computing vector i\n      if i + 4 < record_count {\n          std::arch::x86_64::_mm_prefetch(\n              index.vector_ptr(i + 4) as *const i8,\n              std::arch::x86_64::_MM_HINT_T0,\n          );\n      }\n      let score = dot_product(index.vector_at(i), query);\n      ...\n  }\n\nNOTE: This requires `unsafe` for the raw prefetch intrinsic. Since the project forbids unsafe_code, an alternative is to use the `prefetch` crate which provides safe wrappers, or restructure the scan to use iterator patterns that encourage hardware prefetching (sequential access patterns with no branching).\n\nSAFE ALTERNATIVE: Ensure vectors are stored contiguously in memory (they already are in the mmap) and access them strictly sequentially. Modern CPUs have hardware stream prefetchers that detect sequential access patterns and prefetch automatically. The key is to NEVER skip vectors or access them out of order — even filtered vectors should be loaded and immediately discarded rather than skipped via random access.\n\n### 3. Structure-of-Arrays Vector Layout (Score: 6.7)\n\nThe current FSVI format stores vectors as Array-of-Structures (each record is [doc_id_hash, offset, len, flags, vector_data]). For pure scanning workloads, a Structure-of-Arrays layout is superior:\n\n  // SoA layout in FSVI v2:\n  [All doc_id_hashes]  // Compact for binary search\n  [All vectors]        // Contiguous for linear scan + SIMD\n  [All doc_id strings] // Only touched for top-k results\n\nThis gives perfect cache locality during the scan phase — the CPU prefetcher sees a contiguous f16 slab with no interleaved metadata.\n\nHOWEVER: The current FSVI format already separates the vector slab from records. Verify the vector slab is truly contiguous (no padding between vectors) and 64-byte aligned (cache line boundary). The format spec says 32-byte aligned — consider upgrading to 64-byte.\n\n### 4. ILP: Compute 4 Dot Products Simultaneously (Score: 6.0)\n\nModern CPUs have multiple execution ports. Instead of computing one dot product at a time, process 4 vectors simultaneously to maximize instruction-level parallelism:\n\n  // Process 4 vectors per iteration\n  for chunk in vectors.chunks(4) {\n      let mut sums = [f32x8::ZERO; 4];\n      for dim_block in 0..dim/8 {\n          let q = f32x8::from(&query[dim_block*8..]);\n          sums[0] += f32x8::from(&chunk[0][dim_block*8..]) * q;\n          sums[1] += f32x8::from(&chunk[1][dim_block*8..]) * q;\n          sums[2] += f32x8::from(&chunk[2][dim_block*8..]) * q;\n          sums[3] += f32x8::from(&chunk[3][dim_block*8..]) * q;\n      }\n      // Update top-k heap with all 4 scores\n  }\n\nThis keeps the SIMD execution units fully saturated. Expected improvement: 1.5-2x on modern CPUs with multiple FMA ports.\n\n### 5. Buffer Reuse Pattern (CRITICAL)\n\nAllocate the f32 conversion buffer ONCE and reuse across all dot products in a search:\n\n  pub fn search_top_k(index: &VectorIndex, query: &[f32], k: usize) -> Vec<VectorHit> {\n      let mut buf = vec![0f32; index.dimension()];  // Allocate ONCE\n      for i in 0..index.record_count() {\n          half::slice::convert_to_f32_slice(index.vector_at_f16(i), &mut buf);\n          let score = dot_product_f32_f32(&buf, query);\n          // ... heap update\n      }\n  }\n\nThis eliminates record_count × dimension allocations. For 10K × 384: saves 15M f32 writes to fresh memory.\n\n### Isomorphism Proof Template\n\nAll optimizations preserve:\n- Ordering: Same scores → same rankings (f32 arithmetic identical)\n- Tie-breaking: doc_id ordering preserved for equal scores\n- Floating-point: f16→f32 conversion path identical (same precision)\n- Golden outputs: sha256 of top-k results unchanged\n","created_at":"2026-02-13T20:29:51Z"}]}
{"id":"bd-3un.15","title":"Implement brute-force top-k vector search","description":"Implement brute-force exact nearest neighbor search over the vector index. Uses SIMD dot product + binary heap for efficient top-k retrieval.\n\npub fn search_top_k(\n    index: &VectorIndex,\n    query: &[f32],     // L2-normalized query embedding\n    k: usize,\n    filter: Option<&dyn Fn(usize) -> bool>,  // Optional per-record filter\n) -> Vec<VectorHit> {\n    // Use BinaryHeap<Reverse<VectorHit>> for min-heap (tracks worst of top-k)\n    // For each record in index:\n    //   1. Optional filter check (skip if filtered)\n    //   2. Compute dot_product_f16_f32(stored, query)\n    //   3. If score > heap.peek() or heap.len() < k: push to heap\n    // Return sorted Vec<VectorHit> (descending by score)\n}\n\nOptimizations:\n- Two-phase approach (from xf src/vector.rs): Phase 1 stores only indices+scores (no String allocs), Phase 2 extracts doc_ids for top-k only\n- Rayon parallelism for large indices (threshold: 10,000 records)\n  - Split into chunks of 1024, each chunk produces local top-k\n  - Merge chunk results into global top-k\n- Early termination impossible for cosine similarity (no bounds), so we rely on SIMD speed\n\nConstants:\n- PARALLEL_THRESHOLD: 10_000\n- PARALLEL_CHUNK_SIZE: 1_024\n- These match cass's constants from src/search/vector_index.rs\n\nPerformance targets:\n- 1K vectors: < 1ms\n- 10K vectors: < 15ms\n- 100K vectors: < 150ms (with rayon parallelism)\n\nFile: frankensearch-index/src/search.rs\n\nReference: cass src/search/vector_index.rs, xf src/vector.rs (search_top_k)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:23.629532738Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:54.129034993Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase4","search","vector-index"],"dependencies":[{"issue_id":"bd-3un.15","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.629532738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.15","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T17:55:18.017639224Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.15","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T17:55:18.118248211Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":20,"issue_id":"bd-3un.15","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. NaN-SAFE ORDERING: The BinaryHeap-based top-k search MUST use total_cmp() for float comparison, not partial_cmp(). From cass vector_index.rs (ScoredEntry, lines 459-479):\n\nstruct ScoredEntry { score: f32, index: usize }\nimpl Ord for ScoredEntry {\n    fn cmp(&self, other: &Self) -> Ordering {\n        self.score.total_cmp(&other.score)\n            .then(self.index.cmp(&other.index))  // index as tiebreaker\n    }\n}\n\nWithout total_cmp(), NaN values cause panics in BinaryHeap. Using total_cmp + index tiebreaker ensures deterministic results.\n\n2. TWO-PHASE SEARCH (from xf vector.rs): The search should use a two-phase approach:\n   Phase 1: Collect (index, score) pairs only -- NO String allocations\n   Phase 2: Look up doc_ids only for the final top-k winners\n   This avoids N string allocations for N vectors, only doing K allocations for the K results.\n\n3. PARALLEL SEARCH ENV VAR: Add FRANKENSEARCH_PARALLEL_SEARCH env var (default: true) to let users disable parallel search for debugging. Reference: cass CASS_PARALLEL_SEARCH.\n","created_at":"2026-02-13T20:25:17Z"},{"id":31,"issue_id":"bd-3un.15","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (Top-K Vector Search)\n\n## Profile-Informed Optimization for search_top_k\n\n### Opportunity Matrix\n\n| Hotspot                  | Impact | Confidence | Effort | Score |\n|--------------------------|--------|------------|--------|-------|\n| Two-phase alloc strategy | 5      | 5          | 1      | 25.0  |\n| Heap branch elimination  | 3      | 4          | 2      | 6.0   |\n| Rayon chunk merge opt    | 4      | 4          | 3      | 5.3   |\n| Early abandonment        | 3      | 3          | 2      | 4.5   |\n| Bloom pre-filter         | 3      | 3          | 3      | 3.0   |\n\n### 1. Two-Phase Allocation Strategy (Score: 25.0, CRITICAL)\n\nThe current design is described correctly in bd-3un.15: \"Phase 1 stores only indices+scores (no String allocs), Phase 2 extracts doc_ids for top-k only.\" This is the single most important optimization. ENSURE it's implemented:\n\n  // Phase 1: Score-only scan (ZERO allocations in hot loop)\n  struct ScoreHit { index: u32, score: f32 }  // 8 bytes, fits in register\n  let mut heap: BinaryHeap<Reverse<ScoreHit>> = BinaryHeap::with_capacity(k + 1);\n\n  for i in 0..record_count {\n      let score = dot_product(...);\n      if heap.len() < k || score > heap.peek().unwrap().0.score {\n          heap.push(Reverse(ScoreHit { index: i as u32, score }));\n          if heap.len() > k { heap.pop(); }\n      }\n  }\n\n  // Phase 2: String alloc only for top-k (typically 10-20 items)\n  heap.into_sorted_vec().iter().map(|h| VectorHit {\n      index: h.0.index as usize,\n      score: h.0.score,\n      doc_id: index.doc_id_at(h.0.index as usize).to_string(),\n  }).collect()\n\nThis avoids 10,000 String allocations (the doc_id lookup), saving ~200μs for 10K vectors.\n\n### 2. Heap Guard Pattern (Score: 6.0)\n\nAfter the heap is full (len == k), most candidates will be worse than the worst in the heap. Add a \"guard\" score to skip the heap comparison entirely:\n\n  let mut min_score = f32::NEG_INFINITY;\n  for i in 0..record_count {\n      let score = dot_product(...);\n      if score > min_score {  // Branch predicted as NOT TAKEN (99%+ of the time)\n          heap.push(Reverse(ScoreHit { index: i as u32, score }));\n          if heap.len() > k {\n              heap.pop();\n              min_score = heap.peek().unwrap().0.score;\n          }\n      }\n  }\n\nThe key insight: after ~2k vectors, the guard score stabilizes and ~99% of candidates are rejected by a single float comparison (< 1ns). The heap push/pop (~50ns) is almost never reached.\n\n### 3. Rayon Parallel Merge Optimization (Score: 5.3)\n\nFor > 10K vectors with rayon parallelism, the merge of per-chunk heaps matters:\n\n  // Each chunk produces a local top-k heap\n  // Merge: pour all heaps into one, re-heapify to global top-k\n\n  // BAD: Merge one-by-one\n  for chunk_heap in chunk_heaps { global.extend(chunk_heap); }\n\n  // GOOD: Tournament merge (log₂(num_chunks) rounds)\n  while heaps.len() > 1 {\n      heaps = heaps.chunks(2).map(|pair| merge_heaps(pair[0], pair.get(1))).collect();\n  }\n\nAlso: set PARALLEL_CHUNK_SIZE to match L2 cache size / vector_bytes:\n  - 384-dim f16 = 768 bytes per vector\n  - 8MB L2 cache → ~10K vectors per chunk (matches current default)\n  - Adjust at runtime: chunk_size = l2_cache_bytes / (dimension * 2)\n\n### 4. Isomorphism Proof\n\n- Ordering: top-k by descending score, ties broken by index (deterministic)\n- Two-phase: Phase 2 produces identical doc_ids as single-phase\n- Parallel: global top-k identical to sequential (heap is deterministic for same inputs)\n- Guard pattern: only changes branch prediction, not comparison result\n","created_at":"2026-02-13T20:29:54Z"}]}
{"id":"bd-3un.16","title":"Implement optional HNSW approximate nearest neighbor index","description":"Implement optional HNSW (Hierarchical Navigable Small World) approximate nearest neighbor index for large vector collections (>50K documents). This is an optimization for scale — brute-force is preferred for smaller collections.\n\nBased on cass src/search/ann_index.rs:\n\nBinary format (CHSW magic, persisted to disk):\n- M (max connections per node): 16\n- ef_construction (build-time accuracy): 200\n- ef_search (query-time accuracy): 100 (tunable)\n- MAX_LAYER: 16\n- Distance metric: DistDot (dot product for cosine)\n\npub struct HnswIndex {\n    hnsw: Hnsw<f32, DistDot>,\n    doc_ids: Vec<String>,\n    dimension: usize,\n}\n\nimpl HnswIndex {\n    pub fn build_from_vector_index(vi: &VectorIndex, config: HnswConfig) -> Self;\n    pub fn load(path: &Path) -> SearchResult<Self>;\n    pub fn save(&self, path: &Path) -> SearchResult<()>;\n    pub fn knn_search(&self, query: &[f32], k: usize, ef: usize) -> Vec<VectorHit>;\n}\n\npub struct AnnSearchStats {\n    pub index_size: usize,\n    pub dimension: usize,\n    pub ef_search: usize,\n    pub k_requested: usize,\n    pub k_returned: usize,\n    pub search_time_us: u64,\n    pub is_approximate: bool,\n    pub estimated_recall: f64,  // min(1.0, 0.9 + 0.1 * log2(ef / k))\n}\n\nDependencies: hnsw_rs crate (behind 'ann' feature flag)\n\nPriority P3 because brute-force + SIMD is sufficient for typical use cases (<50K docs). HNSW adds complexity (build time, graph persistence, recall estimation) and is only needed at scale.\n\nReference: cass src/search/ann_index.rs (200+ lines)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T17:50:23.697487665Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:18.279846515Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["ann","optional","phase4","vector-index"],"dependencies":[{"issue_id":"bd-3un.16","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:23.697487665Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.16","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T17:55:18.199925117Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.16","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T17:55:18.279814044Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.17","title":"Implement Tantivy schema and document indexing","description":"Implement the Tantivy full-text search schema and document indexing in frankensearch-lexical. This provides BM25 keyword matching that complements semantic search in the hybrid pipeline.\n\nSchema design (generalized from all 3 codebases):\n\npub fn build_schema() -> Schema {\n    let mut builder = Schema::builder();\n    \n    // Required fields (all documents must have these)\n    builder.add_text_field('id', STRING | STORED);           // Unique document ID\n    builder.add_text_field('content', TEXT | STORED);         // Main searchable text\n    builder.add_i64_field('created_at', INDEXED | STORED | FAST);  // Timestamp for sorting\n    \n    // Optional metadata fields\n    builder.add_text_field('title', TEXT | STORED);           // Optional title\n    builder.add_text_field('doc_type', STRING | STORED);     // Document type tag\n    builder.add_text_field('source', STRING | STORED);       // Source identifier\n    builder.add_text_field('metadata', TEXT | STORED);       // JSON metadata blob\n    \n    // Prefix search support (edge n-grams)\n    builder.add_text_field('content_prefix', TEXT);           // Edge n-gram tokenized\n    builder.add_text_field('title_prefix', TEXT);\n    \n    builder.build()\n}\n\nCustom tokenizer (from cass src/search/tantivy.rs):\n- Hyphen-aware: prevents splitting 'POL-358' into 'POL' and '358'\n- Edge n-gram: generates prefixes for typeahead (configurable 2..=15 chars)\n- Lowercase normalization\n\npub struct LexicalIndex {\n    index: tantivy::Index,\n    schema: Schema,\n    reader: IndexReader,\n    writer: Option<IndexWriter>,\n}\n\nimpl LexicalIndex {\n    pub fn create(path: &Path) -> SearchResult<Self>;\n    pub fn open(path: &Path) -> SearchResult<Self>;\n    pub fn add_document(&mut self, doc: &IndexableDocument) -> SearchResult<()>;\n    pub fn add_documents_batch(&mut self, docs: &[IndexableDocument]) -> SearchResult<()>;\n    pub fn commit(&mut self) -> SearchResult<()>;\n    pub fn search(&self, query: &str, limit: usize) -> SearchResult<Vec<LexicalHit>>;\n}\n\npub struct IndexableDocument {\n    pub id: String,\n    pub content: String,\n    pub title: Option<String>,\n    pub created_at: i64,\n    pub doc_type: Option<String>,\n    pub source: Option<String>,\n    pub metadata: Option<serde_json::Value>,\n}\n\nSchema versioning: hash-based (e.g., 'tantivy-schema-v1-frankensearch') stored in a sentinel file. If hash changes, index needs rebuild.\n\nMerge strategy (from cass):\n- Merge cooldown: 5 minutes between merges\n- Threshold: merge when >= 4 segments\n\nFeature gating: Behind 'lexical' feature flag\nDependencies: tantivy = '0.22'\n\nReference:\n- cass: src/search/tantivy.rs (schema v6, custom tokenizer, edge n-grams)\n- xf: src/search.rs (simpler schema, separate prefix fields)\n- agent-mail: search-v3 architecture (custom tokenizer)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:54.185243489Z","created_by":"ubuntu","updated_at":"2026-02-13T20:25:32.585461337Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["lexical","phase5","tantivy"],"dependencies":[{"issue_id":"bd-3un.17","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:54.185243489Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.17","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:23.757391712Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.17","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:23.837343877Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":21,"issue_id":"bd-3un.17","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. TOKEN LENGTH FILTER: The custom tokenizer MUST include RemoveLongFilter::limit(256) to prevent pathologically long tokens from causing performance issues. From cass tantivy.rs line 482.\n\n2. WRITER BUFFER SIZE: Use 50MB writer buffer: writer(50_000_000). This matches cass tantivy.rs line 137 and is tuned for typical workloads.\n\n3. CORRUPTED INDEX RECOVERY: If an existing index fails to open despite matching schema hash, fall back to clean rebuild with a WARN log. From cass tantivy.rs lines 110-121. This prevents permanent broken states.\n\n4. PREVIEW FIELD: Add a 'preview' stored field with the first 400 chars of content. This enables result snippets without re-reading the full document from an external store.\n\n5. EDGE N-GRAM PERFORMANCE: Use ArrayVec (stack-allocated) for n-gram index collection to avoid heap allocation during bulk indexing. From cass tantivy.rs MAX_NGRAM_INDICES=21 with ArrayVec.\n\n6. SCHEMA VERSIONING: Store schema version as a string hash (e.g., \"tantivy-schema-v1-frankensearch\") in a sentinel file. On mismatch, wipe and rebuild. From cass \"tantivy-schema-v6-long-tokens\".\n","created_at":"2026-02-13T20:25:32Z"}]}
{"id":"bd-3un.18","title":"Implement Tantivy query parsing and search execution","description":"Implement query parsing and search execution for the Tantivy lexical index. This handles converting user queries into Tantivy query objects and executing them.\n\nQuery types to support:\n1. Simple term search: 'authentication' → term query on content field\n2. Phrase search: '\"error handling\"' → phrase query (exact sequence)\n3. Boolean: 'rust AND async' → BooleanQuery with AND/OR/NOT\n4. Prefix/wildcard: 'auth*' → prefix query on content_prefix field\n5. Filtered: doc_type:tweet → term query on doc_type field\n6. Date range: created_at > 2025-01-01 → range query on created_at\n\npub struct LexicalQuery {\n    pub text: String,\n    pub fields: Vec<String>,         // Which fields to search (default: content)\n    pub doc_types: Option<Vec<String>>,  // Filter by doc_type\n    pub date_range: Option<(Option<i64>, Option<i64>)>,  // (start, end) timestamps\n    pub limit: usize,\n    pub offset: usize,\n}\n\npub struct LexicalHit {\n    pub doc_id: String,\n    pub score: f32,         // BM25 score\n    pub rank: usize,        // 0-based rank\n    pub highlights: Vec<String>,  // Matched snippets with highlighting\n    pub doc: Option<serde_json::Value>,  // Retrieved stored fields\n}\n\nQuery explanation (for debugging):\npub enum QueryExplanation {\n    Simple,\n    Phrase,\n    Boolean,\n    Wildcard,\n    Filtered,\n    Empty,\n}\n\nSnippet generation:\n- Use Tantivy's built-in snippet generator\n- Configurable max snippet length (default: 200 chars)\n- HTML highlighting tags (configurable)\n\nReference:\n- cass: src/search/tantivy.rs (search method, query builder)\n- xf: src/search.rs (BM25 ranking with phrase/prefix)\n- agent-mail: crates/mcp-agent-mail-search-core/src/lexical_parser.rs (700+ lines), lexical_response.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:50:54.262211413Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:52.462116577Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["lexical","phase5","query","tantivy"],"dependencies":[{"issue_id":"bd-3un.18","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:50:54.262211413Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.18","depends_on_id":"bd-3un.17","type":"blocks","created_at":"2026-02-13T17:55:23.920801880Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":48,"issue_id":"bd-3un.18","author":"Dicklesworthstone","text":"REVISION: Tantivy Query Parsing Hardening\n\n1. Query Complexity Limits:\n   - Max boolean clauses: 64 (prevent OOM from deeply nested queries)\n   - Max query length: 10,000 chars (truncate with WARN log)\n   - Max wildcard expansions: 1000 terms (Tantivy default, document explicitly)\n   - Timeout: 500ms per search execution (configurable via TwoTierConfig)\n   - If limits exceeded: return SearchError::QueryError with human-friendly message\n\n2. Integration with Query Classification (bd-3un.43):\n   - QueryClass informs search strategy BEFORE Tantivy parsing\n   - Identifier queries: use exact match on 'id' field first, fall back to content\n   - ShortKeyword queries: boost title field 2x\n   - NaturalLanguage queries: standard BM25 across content + title\n   - Empty queries: return empty results immediately (no Tantivy round-trip)\n\n3. Error Messages:\n   - Parse errors: return the problematic token position and suggestion\n   - Example: \"Unmatched quote at position 15. Did you mean to search for: error handling\"\n   - Tantivy QueryParserError mapped to SearchError::QueryError with context\n   - Log at DEBUG: \"query_parsed input={} type={} clauses={} fields={}\"\n\n4. Performance:\n   - QueryParser is created once per LexicalIndex (not per search)\n   - Reuse Tantivy Searcher via SearcherManager (leased readers)\n   - For repeated queries: caller can cache results (frankensearch doesn't cache internally)\n   - Snippet generation is optional (skip if caller doesn't need highlights)\n\n5. Field Boosting:\n   - Default boost: title 2.0x, content 1.0x, metadata 0.5x\n   - Configurable via LexicalQuery.field_boosts: HashMap<String, f32>\n   - title_prefix and content_prefix fields: boost 1.5x for prefix matches\n","created_at":"2026-02-13T20:44:52Z"}]}
{"id":"bd-3un.19","title":"Implement score normalization (min-max)","description":"Implement score normalization utilities used throughout the fusion pipeline. Different search sources produce scores on different scales (BM25 scores vs cosine similarity), so normalization is required before combining them.\n\npub fn min_max_normalize(scores: &mut [f32]) {\n    let min = scores.iter().copied().fold(f32::INFINITY, f32::min);\n    let max = scores.iter().copied().fold(f32::NEG_INFINITY, f32::max);\n    let range = max - min;\n    \n    if range.abs() < f32::EPSILON {\n        // All scores equal → set to 1.0 (not 0.0, to avoid suppressing results)\n        for s in scores.iter_mut() { *s = 1.0; }\n        return;\n    }\n    \n    for s in scores.iter_mut() {\n        *s = (*s - min) / range;\n    }\n}\n\npub fn normalize_scores(scores: &[f32]) -> Vec<f32> {\n    let mut result = scores.to_vec();\n    min_max_normalize(&mut result);\n    result\n}\n\nAlso provide z-score normalization as an alternative:\npub fn z_score_normalize(scores: &mut [f32]) { ... }\n\nThese are used in:\n1. Two-tier blending (normalize fast + quality scores to [0,1] before weighted combination)\n2. RRF doesn't need normalization (rank-based, inherently normalized)\n3. Reranker score normalization (cross-encoder scores can be arbitrary scale)\n\nFile: frankensearch-fusion/src/normalize.rs\n\nReference:\n- cass: src/search/two_tier_search.rs lines 750-765 (normalize_scores)\n- xf: src/hybrid.rs (min_max_normalize)\n- agent-mail: same pattern","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:51:31.268523209Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:52.099481592Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fusion","phase6","scoring"],"dependencies":[{"issue_id":"bd-3un.19","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:51:31.268523209Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.19","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:24.004145339Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":29,"issue_id":"bd-3un.19","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Score Normalization)\n\n## Mathematical Upgrade: From Min-Max to Copula-Based Score Combination\n\nMin-max normalization is the most basic approach and has known failure modes. The alien-artifact approach uses copula theory for principled score combination.\n\n### Problem with Min-Max\n\nMin-max normalization maps scores to [0,1] but:\n1. Is sensitive to outliers (one extreme score compresses all others)\n2. Destroys distributional information (BM25 scores are roughly Gaussian; cosine similarities are Beta-distributed)\n3. Maps different distributions to the same [0,1] range, making them \"look similar\" when they aren't\n\n### 1. Rank-Based Copula Transform (Recommended Upgrade)\n\nTransform each score set to a uniform distribution via rank transform, then to standard normal:\n\n  fn copula_normalize(scores: &[f32]) -> Vec<f32> {\n      // Step 1: Rank transform → Uniform[0,1]\n      let ranks = rank_transform(scores);  // ties get averaged ranks\n      let uniform: Vec<f32> = ranks.iter().map(|&r| r / (scores.len() as f32 + 1.0)).collect();\n\n      // Step 2: Probit transform → Normal(0,1)\n      let normal: Vec<f32> = uniform.iter().map(|&u| probit(u)).collect();\n      normal\n  }\n\nThis preserves the RANKING within each source while making scores comparable across sources. The probit (inverse normal CDF) ensures Gaussian scores, which behave well under linear combination.\n\n### 2. Empirical Bayes Shrinkage for Small Result Sets\n\nWhen a source returns few results (< 20), min-max normalization is noisy. Apply James-Stein shrinkage toward the grand mean:\n\n  shrunk_score = grand_mean + shrinkage_factor × (raw_score - grand_mean)\n  shrinkage_factor = 1 - (n-2) × σ² / Σ(xᵢ - x̄)²\n\nThis is provably better than raw scores (dominates in mean squared error for n ≥ 3). It prevents small result sets from producing extreme normalized scores.\n\n### 3. When to Use Which\n\n| Scenario                        | Method              | Why                                     |\n|---------------------------------|---------------------|-----------------------------------------|\n| RRF fusion                      | None needed         | RRF is rank-based, doesn't use scores   |\n| Two-tier blending               | Copula + shrinkage  | Comparing different embedding spaces    |\n| Reranker integration            | Min-max is fine     | Same model, same score distribution     |\n| Cross-model comparison (bakeoff)| Copula              | Different models, different distributions|\n\n### 4. Keep Min-Max as Default\n\nThe copula transform is strictly better but adds complexity. Keep min-max as the default implementation, provide copula_normalize as an opt-in alternative:\n\n  pub enum NormalizationMethod {\n      MinMax,           // Simple, fast, good enough for most cases\n      CopulaNormal,     // Principled, handles distributional differences\n      JamesSteinShrink, // For small result sets (< 20 items)\n      None,             // Raw scores (for RRF which doesn't need normalization)\n  }\n\nNote: RRF does NOT depend on normalization (rank-based fusion). This was already correctly identified in the review. The normalization improvements apply to the BLENDING step (bd-3un.21) where we combine fast and quality scores.\n","created_at":"2026-02-13T20:29:52Z"}]}
{"id":"bd-3un.2","title":"Define core error types (SearchError)","description":"Create comprehensive error types in frankensearch-core that cover all failure modes across the search pipeline. These should be ergonomic for consumers and map cleanly to the error types already used in cass/xf/agent-mail.\n\nError hierarchy (from studying all 3 codebases):\n\npub enum SearchError {\n    // Embedding errors\n    EmbedderUnavailable { model: String, reason: String },\n    EmbeddingFailed { model: String, source: Box<dyn Error> },\n    ModelNotFound { name: String },\n    ModelLoadFailed { path: PathBuf, source: Box<dyn Error> },\n    \n    // Index errors\n    IndexCorrupted { path: PathBuf, detail: String },\n    IndexVersionMismatch { expected: u16, found: u16 },\n    DimensionMismatch { expected: usize, found: usize },\n    IndexNotFound { path: PathBuf },\n    \n    // Search errors\n    QueryParseError { query: String, detail: String },\n    SearchTimeout { elapsed_ms: u64, budget_ms: u64 },\n    \n    // Reranker errors\n    RerankerUnavailable { model: String },\n    RerankFailed { source: Box<dyn Error> },\n    \n    // IO errors\n    Io(std::io::Error),\n    \n    // Configuration errors\n    InvalidConfig { field: String, detail: String },\n}\n\nUse thiserror for derive(Error). Implement Display with actionable messages.\n\nAlso define type aliases:\npub type SearchResult<T> = Result<T, SearchError>;\n\nReference implementations:\n- cass: src/search/reranker.rs (RerankerError), src/search/daemon_client.rs (DaemonError)\n- xf: implicit in Result types across modules\n- agent-mail: SearchError in mcp-agent-mail-search-core","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:13.149991088Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:00.517424260Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1"],"dependencies":[{"issue_id":"bd-3un.2","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:13.149991088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.2","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T17:55:00.517386900Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.20","title":"Implement Reciprocal Rank Fusion (RRF)","description":"Implement Reciprocal Rank Fusion (RRF) for combining lexical and semantic search results. RRF is the standard fusion algorithm used across all 3 codebases and is well-established in IR literature.\n\nAlgorithm:\n  score(doc) = Σ 1/(K + rank_i + 1)   for each source i where doc appears\n  K = 60 (empirically optimal constant from literature)\n\npub struct RrfConfig {\n    /// RRF constant K (default: 60.0). Higher = more weight to high-ranked docs.\n    pub k: f64,\n    /// Tie-breaking epsilon (default: 1e-9).\n    pub epsilon: f64,\n}\n\npub fn rrf_fuse(\n    lexical: &[LexicalHit],\n    semantic: &[VectorHit],\n    limit: usize,\n    offset: usize,\n    config: &RrfConfig,\n) -> Vec<FusedHit> {\n    // 1. Build HashMap<doc_id, FusedHit>\n    // 2. For each lexical result (rank 0, 1, ...):\n    //    score += 1.0 / (K + rank + 1)\n    //    Record lexical_rank, lexical_score\n    // 3. For each semantic result (rank 0, 1, ...):\n    //    score += 1.0 / (K + rank + 1)\n    //    Record semantic_rank, semantic_score\n    // 4. Sort by: rrf_score (desc) → in_both_sources (tiebreak) → doc_id (stable)\n    // 5. Apply offset and limit\n}\n\nCandidate budget (from xf src/hybrid.rs):\npub const CANDIDATE_MULTIPLIER: usize = 3;  // Fetch 3x limit from each source\n\npub fn candidate_count(limit: usize, offset: usize) -> usize {\n    limit.saturating_add(offset).saturating_mul(CANDIDATE_MULTIPLIER)\n}\n\nThis is the heart of hybrid search — it rewards documents that appear in BOTH lexical and semantic results (they get scores from both sources), which typically produces better results than either source alone.\n\nEnvironment variable: FRANKENSEARCH_RRF_K (override K constant)\n\nFile: frankensearch-fusion/src/rrf.rs\n\nReference:\n- cass: src/search/query.rs (RRF with k=60, candidate multiplier 3x/4x)\n- xf: src/hybrid.rs (rrf_fuse function, K=60, CANDIDATE_MULTIPLIER=3)\n- agent-mail: crates/mcp-agent-mail-search-core/src/fusion.rs (DEFAULT_RRF_K=60)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:51:31.343268779Z","created_by":"ubuntu","updated_at":"2026-02-13T20:45:32.347176311Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fusion","phase6","rrf"],"dependencies":[{"issue_id":"bd-3un.20","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:51:31.343268779Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.20","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:24.084932128Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":5,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"RRF ALGORITHM CONTEXT: Reciprocal Rank Fusion was introduced by Cormack et al. (2009) and has become the standard for combining ranked lists from different retrieval systems.\n\nWhy K=60: The K parameter controls how much weight is given to high-ranked vs lower-ranked results. K=60 is the empirically optimal value from the original paper and has been independently validated across many IR systems. Higher K values make the scores more uniform (less distinction between ranks); lower K values amplify the importance of being highly ranked.\n\nWhy RRF over alternatives:\n- Simple: no training data or score calibration needed\n- Robust: works well even when score distributions differ wildly (BM25 scores vs cosine similarities)\n- Principled: theoretically grounded in rank aggregation\n- Proven: used in production at Elastic, Pinecone, Vespa, etc.\n\nThe candidate multiplier (3x) is important: if the user wants 10 results, we fetch 30 from each source. This ensures good coverage for docs that might rank differently across sources.","created_at":"2026-02-13T17:56:49Z"},{"id":18,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. DETERMINISTIC TIE-BREAKING: The RRF output must use a strict multi-level sort for reproducible results across runs. From xf hybrid.rs and agent-mail fusion.rs:\n\nSorting chain for FusedHit (ALL levels must be applied):\n  Level 1: rrf_score DESCENDING (with epsilon=1e-9 for float comparison)\n  Level 2: in_both_sources bonus (true > false) -- docs in both lexical+semantic rank higher on tie\n  Level 3: lexical_score DESCENDING (if available, with epsilon comparison)\n  Level 4: doc_id ASCENDING (absolute determinism -- string comparison)\n\nThis 4-level chain ensures that:\n- Results are reproducible across runs (no HashMap iteration order dependency)\n- Equal-RRF-score docs are ordered by which had better lexical coverage\n- Final doc_id tiebreak prevents any residual ordering ambiguity\n\n2. RANK CONVENTION: Use 0-based ranks internally with the formula:\n   score(doc) = 1.0 / (K + rank + 1.0)\n   This is equivalent to agent-mail's 1-based convention: 1.0 / (K + rank_1based)\n   Document this equivalence explicitly so implementors don't get confused.\n\n3. DEDUPLICATION KEY: After fusion, deduplicate by doc_id. If the same doc_id appears in both sources, its scores are summed (not duplicated). The FusedHit should track both source ranks for explainability.\n","created_at":"2026-02-13T20:24:37Z"},{"id":26,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (RRF Fusion)\n\n## Mathematical Upgrade: From Ad-Hoc RRF to Principled Rank Aggregation\n\nCurrent design uses fixed K=60 RRF. This works, but there's a richer mathematical framework from social choice theory and probabilistic rank aggregation that would make this genuinely alien-artifact quality.\n\n### 1. Plackett-Luce Rank Aggregation (replaces ad-hoc K constant)\n\nRRF's 1/(K+rank) is actually a special case of the Plackett-Luce model for rank aggregation. The PL model assigns probability to a ranking π as:\n\n  P(π) = ∏ᵢ wᵢ / (Σⱼ≥ᵢ wⱼ)\n\nWhere wᵢ are per-item \"worth\" parameters. When we set wᵢ = 1/(K+rankᵢ), we recover RRF. But the PL framework lets us LEARN optimal weights from implicit feedback (clicks, dwell time), giving us adaptive fusion that improves with usage.\n\nImplementation: Start with RRF (K=60) as the uninformative prior. As user interactions accumulate, update PL weights via MM algorithm (minorization-maximization). This converges in O(n·log n) per update and gives provably optimal rank aggregation under the PL model.\n\n### 2. Kemeny Distance for Fusion Quality Monitoring\n\nInstead of just Kendall's tau for rank correlation (already in bd-3un.21), use Kemeny distance to measure how much the fused ranking deviates from each source. Kemeny distance is the MINIMUM number of pairwise swaps to transform one ranking into another. This gives a principled metric for:\n- Detecting when lexical and semantic rankings diverge wildly (→ query is ambiguous)\n- Monitoring whether fusion is actually helping (→ average Kemeny distance should decrease)\n\n### 3. Conformal Prediction Sets for Score Calibration\n\nWrap RRF scores in conformal prediction sets to provide distribution-free coverage guarantees:\n\n  P(doc_relevance ∈ Ĉ(score)) ≥ 1 - α\n\nThis requires a calibration set (ground truth relevance judgments for a sample of queries), but provides FORMAL guarantees that the score-to-relevance mapping is calibrated. No distributional assumptions needed.\n\n### 4. Optimal K via Bayesian Online Learning\n\nInstead of fixed K=60, maintain a Gamma(α,β) posterior over K, updated by implicit relevance signals. The posterior predictive gives E[K] = α/β which adapts per-query-class:\n- Short queries (1-2 words): may benefit from higher K (more uniform weighting)\n- Long queries (5+ words): may benefit from lower K (sharper top-heavy weighting)\n\n### Performance Optimization: Parallel RRF with Sharded HashMap\n\nFor large result sets (1000+ from each source), the HashMap-based fusion becomes the bottleneck. Use a sharded HashMap (DashMap or 16 Mutex<HashMap>) partitioned by doc_id hash. Each shard can be updated independently. For the common case (< 300 results per source), the overhead isn't worth it, so add a threshold check.\n\n### Implementation Priority\n\n1. Implement standard RRF first (as designed) — this is correct and well-tested\n2. Add Kemeny distance monitoring alongside Kendall's tau — pure addition, no API change\n3. Add adaptive K via Beta posterior — simple addition with env var fallback\n4. Add Plackett-Luce upgrade path — optional, behind feature flag\n\nThese upgrades make the fusion system formally principled while keeping the simple RRF as the always-correct baseline.\n","created_at":"2026-02-13T20:29:49Z"},{"id":52,"issue_id":"bd-3un.20","author":"Dicklesworthstone","text":"CORRECTNESS NOTE: RRF Score Semantics and Edge Cases\n\n1. Zero-Based Rank Convention Documentation:\n   The formula 1/(K + rank + 1) with 0-based ranks means rank=0 gives score 1/(K+1).\n   For K=60: rank 0 -> 1/61 = 0.01639, rank 1 -> 1/62 = 0.01613.\n   This MUST be documented prominently because off-by-one here silently degrades quality.\n   The +1 in the denominator is because the original RRF paper uses 1-based ranks: 1/(K+r).\n   With 0-based ranks, this becomes 1/(K+r+1) to produce identical scores.\n\n2. Score Summation for Multi-Source Documents:\n   When a document appears in both lexical and semantic results, its RRF scores are SUMMED.\n   This means multi-source docs get up to 2x the score of single-source docs.\n   This is the standard RRF formulation and is correct -- being retrieved by multiple\n   systems is strong evidence of relevance.\n\n3. Dedup Semantics:\n   Documents are deduped by doc_id (String equality). When a duplicate is found,\n   scores are summed into the first occurrence. The HashMap insert-or-add pattern\n   must use entry().and_modify().or_insert() for correctness.\n\n4. Empty Input Handling:\n   - Both lists empty -> return empty results\n   - One list empty -> return the non-empty list's RRF scores only\n   - Both lists have same single doc -> sum both contributions\n","created_at":"2026-02-13T20:45:32Z"}]}
{"id":"bd-3un.21","title":"Implement two-tier score blending","description":"Implement the score blending algorithm that combines fast-tier and quality-tier semantic scores when the quality model finishes processing.\n\npub fn blend_two_tier(\n    fast_results: &[VectorHit],      // From fast embedder (already normalized)\n    quality_results: &[VectorHit],   // From quality embedder (already normalized)\n    blend_factor: f32,               // 0.0 = fast-only, 1.0 = quality-only\n) -> Vec<VectorHit> {\n    // For each document appearing in either result set:\n    // blended_score = fast_score * (1.0 - blend_factor) + quality_score * blend_factor\n    //\n    // Default blend_factor: 0.7 (70% quality, 30% fast)\n    //\n    // Algorithm:\n    // 1. Normalize both sets to [0, 1] via min_max_normalize\n    // 2. Build HashMap<doc_id, (fast_score, quality_score)>\n    // 3. For docs only in fast: quality_score = 0.0\n    // 4. For docs only in quality: fast_score = 0.0\n    // 5. Compute blended_score for each\n    // 6. Sort by blended_score descending\n}\n\nThe blend_factor of 0.7 was chosen based on empirical testing:\n- 0.7 gives quality model dominant influence while still valuing fast rankings\n- Fast rankings serve as a useful 'prior' that prevents quality model from making radical changes\n- This produces smooth visual transitions (results don't jump around too much)\n\nAlso provide rank correlation metrics for debugging/monitoring:\n\npub fn compute_rank_changes(\n    initial: &[VectorHit],\n    refined: &[VectorHit],\n) -> RankChanges {\n    // Returns counts of promoted/demoted/stable results\n}\n\npub fn kendall_tau(\n    initial: &[VectorHit],\n    refined: &[VectorHit],\n) -> Option<f64> {\n    // Kendall's tau coefficient (-1 to 1)\n    // Values near 1 = rankings barely changed\n    // Values near 0 = significant reordering\n    // Useful for monitoring whether quality model is doing useful work\n}\n\nFile: frankensearch-fusion/src/blend.rs\n\nReference:\n- xf: src/hybrid.rs (blend_two_tier, compute_rank_changes, kendall_tau)\n- cass: src/search/two_tier_search.rs lines 696-712 (weight blending)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 836-902","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:51:31.417801691Z","created_by":"ubuntu","updated_at":"2026-02-13T20:45:32.425118526Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["blending","fusion","phase6","two-tier"],"dependencies":[{"issue_id":"bd-3un.21","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:51:31.417801691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.21","depends_on_id":"bd-3un.19","type":"blocks","created_at":"2026-02-13T17:55:24.327257910Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.21","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:24.244555602Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":27,"issue_id":"bd-3un.21","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (Two-Tier Blending)\n\n## Mathematical Upgrade: From Fixed Blend Factor to Bayesian Adaptive Blending\n\nCurrent design uses a fixed 0.7 blend factor. This is a reasonable starting point but leaves significant quality on the table. The alien-artifact approach uses online Bayesian learning to adapt the blend factor per query characteristics.\n\n### 1. Beta-Bernoulli Adaptive Blend Weight\n\nModel the \"quality model agrees with ground truth\" as a Bernoulli trial. Maintain Beta(α,β) posterior, initialized as Beta(7, 3) to encode the 0.7 prior:\n\n  P(quality_is_better | data) ~ Beta(α + successes, β + failures)\n  E[blend_factor] = α / (α + β)\n\nAfter each query with implicit feedback (click on result = success for the scoring system that ranked it higher), update the posterior. The blend factor naturally adapts:\n- When quality consistently outperforms fast → α grows → blend_factor → 1.0\n- When quality adds noise (e.g., domain mismatch) → β grows → blend_factor → 0.5\n\nThis is a CONJUGATE prior, so updates are O(1) with no MCMC needed.\n\n### 2. Thompson Sampling for Exploration-Exploitation\n\nRather than always using E[blend_factor], occasionally SAMPLE from Beta(α,β) to explore:\n\n  blend = sample(Beta(α, β))\n\nThis automatically balances exploitation (use best known blend) with exploration (try different blends to learn faster). Thompson sampling is provably optimal in the Bayesian regret sense.\n\n### 3. Evidence Ledger for Blend Decisions\n\nFor every search query, log a structured evidence entry:\n\n  {\n    query_hash: u64,\n    fast_ndcg: f32,\n    quality_ndcg: f32,\n    blended_ndcg: f32,\n    blend_factor_used: f32,\n    rank_correlation: f32,  // Kendall's tau between fast and quality\n    quality_latency_ms: u64,\n    decision_reason: \"bayesian_posterior\" | \"fixed_default\" | \"fast_only_timeout\"\n  }\n\nThis provides complete explainability — you can always show exactly WHY the blend factor was what it was.\n\n### 4. Formal Regret Bound\n\nUnder the Beta-Bernoulli model with Thompson sampling, the expected regret after T queries is bounded by:\n\n  E[Regret(T)] ≤ O(√(T · log T))\n\nThis is a formal guarantee that the blend factor converges to optimal at a known rate.\n\n### 5. Performance: Zero-Overhead Adaptive Blending\n\nThe Beta posterior is just two floats (α, β). Sampling from Beta requires one call to the beta distribution RNG (< 1μs). The overhead vs. fixed 0.7 is literally unmeasurable.\n\nThe blend_factor computation itself is the same multiply-add — only the WEIGHT changes.\n\n### Implementation in bd-3un.21\n\nAdd to the blend module:\n\n  pub struct AdaptiveBlender {\n      alpha: AtomicF32,  // Beta posterior param (success count + prior)\n      beta: AtomicF32,   // Beta posterior param (failure count + prior)\n      min_samples: usize, // Don't adapt until N queries observed (default: 50)\n  }\n\n  impl AdaptiveBlender {\n      pub fn new() -> Self { Self { alpha: 7.0, beta: 3.0, min_samples: 50 } }\n      pub fn blend_factor(&self) -> f32 { self.alpha / (self.alpha + self.beta) }\n      pub fn update(&self, quality_was_better: bool) { ... }\n  }\n\nThe fixed 0.7 fallback is the zero-observation case (Beta(7,3) prior). Full backward compatibility.\n","created_at":"2026-02-13T20:29:50Z"},{"id":53,"issue_id":"bd-3un.21","author":"Dicklesworthstone","text":"CORRECTNESS NOTE: Two-Tier Blend Scoring for Partial Coverage\n\nDesign decision documentation for missing score handling:\n\nThe blend function uses 0.0 for missing scores: if a document has a fast_score but no\nquality_score, its blended score = fast_score * (1-blend_factor) + 0.0 * blend_factor.\n\nThis is INTENTIONAL and CORRECT in the two-tier pipeline because:\n\n1. Phase 1 only computes quality embeddings for the top N candidates from Phase 0.\n2. Documents outside top N never enter the blend function.\n3. Within the top N, all documents have fast scores. A missing quality score means\n   the quality model couldn't process it (e.g., timeout, error).\n4. Using 0.0 for missing quality scores penalizes failed-to-embed docs, which is\n   the correct conservative behavior (we don't know their quality score).\n\nAlternative considered and rejected:\n- Imputing missing quality scores from fast scores (e.g., quality = fast * 0.8)\n  This would be an unjustified assumption about score correlation.\n- Keeping fast score unchanged for missing quality\n  This would bypass the blend entirely, giving fast-only docs an unfair advantage\n  when blend_factor > 0.5 (which is the default at 0.7).\n\nIMPORTANT: The blend function MUST document this behavior clearly with an example\nin its doc comment showing the scoring math for a doc with missing quality score.\n","created_at":"2026-02-13T20:45:32Z"}]}
{"id":"bd-3un.22","title":"Implement TwoTierConfig with presets and env overrides","description":"Implement the configuration struct for the two-tier search system. This controls all tuning knobs for the progressive search experience.\n\npub struct TwoTierConfig {\n    /// Fast embedding model name (default: 'potion-multilingual-128M')\n    pub fast_model: Option<String>,\n    \n    /// Quality embedding model name (default: 'all-MiniLM-L6-v2')\n    pub quality_model: Option<String>,\n    \n    /// Fast embedding dimension (default: 256)\n    pub fast_dimension: usize,\n    \n    /// Quality embedding dimension (default: 384)\n    pub quality_dimension: usize,\n    \n    /// Blend factor: 0.0 = fast-only, 1.0 = quality-only (default: 0.7)\n    pub quality_weight: f32,\n    \n    /// Max documents to refine with quality model (default: 100)\n    pub max_refinement_docs: usize,\n    \n    /// Quality model timeout in ms (default: 500)\n    pub quality_timeout_ms: u64,\n    \n    /// Skip quality refinement (fast results only)\n    pub fast_only: bool,\n    \n    /// Skip fast phase (quality results only, slower but highest quality)\n    pub quality_only: bool,\n    \n    /// RRF K constant for hybrid fusion (default: 60.0)\n    pub rrf_k: f64,\n    \n    /// Candidate multiplier for retrieval (default: 3x)\n    pub candidate_multiplier: usize,\n}\n\nPresets:\n- TwoTierConfig::default() → Full two-tier with 0.7 quality weight\n- TwoTierConfig::fast_only() → No quality refinement, instant results\n- TwoTierConfig::quality_only() → Best quality, no fast phase\n- TwoTierConfig::balanced() → 0.5 quality weight\n\nEnvironment variable overrides (from xf src/config.rs):\n- FRANKENSEARCH_FAST_MODEL\n- FRANKENSEARCH_QUALITY_MODEL\n- FRANKENSEARCH_BLEND_FACTOR (or FRANKENSEARCH_QUALITY_WEIGHT)\n- FRANKENSEARCH_QUALITY_TIMEOUT_MS\n- FRANKENSEARCH_RRF_K\n- FRANKENSEARCH_CANDIDATE_MULTIPLIER\n\nConfig loading priority: explicit code > env vars > defaults\n\nBuilder pattern:\nTwoTierConfig::builder()\n    .fast_model('potion-multilingual-128M')\n    .quality_weight(0.8)\n    .max_refinement_docs(50)\n    .build()\n\nFile: frankensearch-fusion/src/config.rs\n\nReference:\n- xf: src/config.rs (TwoTierConfig with env vars)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 60-109","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:52:20.058628453Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:50.016651984Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","phase7","two-tier"],"dependencies":[{"issue_id":"bd-3un.22","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:20.058628453Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.22","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:29.667615367Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":45,"issue_id":"bd-3un.22","author":"Dicklesworthstone","text":"REVISION: TwoTierConfig Hardening\n\n1. Validation Logic (enforce at construction time):\n   - blend_factor: must be in [0.0, 1.0], panic-free clamp with WARN log if out of range\n   - rrf_k: must be > 0 (K=0 causes division by 1/(0+rank+1) which is valid but unintended)\n   - quality_timeout_ms: must be > 0, default 200ms, cap at 5000ms\n   - candidate_multiplier: must be >= 1.0, default 3.0\n   - refinement_doc_limit: must be > 0, default 50\n   - Log all overrides at INFO: \"config_override field={} value={} source=env|explicit\"\n\n2. Serde Serialization:\n   - Derive Serialize, Deserialize for config file persistence\n   - Support TOML format (frankensearch.toml) for human-readable config\n   - Round-trip test: serialize -> deserialize -> assert_eq\n\n3. Builder Pattern:\n   - TwoTierConfigBuilder with method chaining\n   - .build() returns Result<TwoTierConfig, ConfigError> with validation\n   - Each setter logs at TRACE level for config debugging\n\n4. Integration with Adaptive Fusion (bd-21g):\n   - TwoTierConfig gains optional field: adaptive_params: Option<AdaptiveParams>\n   - When Some, blend_factor and rrf_k become initial priors, not fixed values\n   - Adaptive mode is opt-in, defaults to None (static parameters)\n\n5. Environment Variable Priority:\n   - Document clearly: explicit code > env vars > defaults\n   - Use a helper fn: fn env_or<T: FromStr>(key: &str, default: T) -> T\n   - All env vars prefixed FRANKENSEARCH_ to avoid collisions\n","created_at":"2026-02-13T20:44:50Z"}]}
{"id":"bd-3un.23","title":"Implement TwoTierIndex (dual-index storage)","description":"Implement the TwoTierIndex that manages two parallel vector indices — one for fast-tier embeddings and one for quality-tier embeddings. This is the data structure that enables progressive search.\n\npub struct TwoTierIndex {\n    fast_index: VectorIndex,             // 256-dim potion embeddings\n    quality_index: Option<VectorIndex>,  // 384-dim MiniLM embeddings (may not exist yet)\n    doc_ids: Vec<String>,                // Shared doc ID list (both indices same order)\n    has_quality: Vec<bool>,              // Track which docs have quality embeddings\n    config: TwoTierConfig,\n}\n\nimpl TwoTierIndex {\n    /// Open from a directory containing vector.fast.idx and optionally vector.quality.idx\n    pub fn open(dir: &Path, config: TwoTierConfig) -> SearchResult<Self>;\n    \n    /// Create a new two-tier index from scratch\n    pub fn create(dir: &Path, config: TwoTierConfig) -> SearchResult<TwoTierIndexBuilder>;\n    \n    /// Search using fast embeddings only\n    pub fn search_fast(&self, query_vec: &[f32], k: usize) -> Vec<VectorHit>;\n    \n    /// Get quality scores for specific document indices (used during refinement)\n    pub fn quality_scores_for_indices(\n        &self,\n        query_vec: &[f32],\n        indices: &[usize],\n    ) -> Vec<f32>;\n    \n    /// Check if quality index is available\n    pub fn has_quality_index(&self) -> bool;\n    \n    /// Number of indexed documents\n    pub fn doc_count(&self) -> usize;\n}\n\nFile naming convention:\n- {dir}/vector.fast.idx  → fast-tier embeddings\n- {dir}/vector.quality.idx → quality-tier embeddings\n- {dir}/vector.idx → fallback single-tier index\n\nCaching (from xf src/main.rs):\npub struct VectorIndexCache {\n    fast: OnceLock<Option<VectorIndex>>,\n    quality: OnceLock<Option<VectorIndex>>,\n    // Staleness detection for auto-rebuild\n}\n\nThe cache uses OnceLock for thread-safe lazy initialization. The index is loaded once and reused for all queries in a session.\n\nReference:\n- xf: src/main.rs lines 50-150 (VectorIndexCache with fast/quality/default)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs lines 199-255 (TwoTierIndex)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:52:20.138236083Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:50.868397384Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["index","phase7","two-tier"],"dependencies":[{"issue_id":"bd-3un.23","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:20.138236083Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.23","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T17:55:29.743275010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.23","depends_on_id":"bd-3un.22","type":"blocks","created_at":"2026-02-13T17:55:29.821622919Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":46,"issue_id":"bd-3un.23","author":"Dicklesworthstone","text":"REVISION: TwoTierIndex Implementation Details\n\n1. Cache Invalidation Strategy:\n   - OnceLock means indices are loaded once and stay resident\n   - Invalidation via TwoTierIndex::reload() which replaces the OnceLock contents\n   - On index rebuild: writer creates new files with .tmp suffix, then atomic rename\n   - Readers see stale data until reload() is called (eventual consistency, not instant)\n   - Integration with bd-3un.41 (staleness): staleness detector triggers reload()\n\n2. Consistency Between Fast and Quality Indices:\n   - CRITICAL: fast and quality indices MUST share the same doc_id set\n   - During incremental indexing: fast index is always updated; quality index may lag\n   - has_quality HashMap tracks which doc_ids have quality embeddings\n   - Documents without quality embeddings get skipped during Phase 1 blend\n   - Consistency check at open(): verify doc_count matches, log WARN if mismatch\n\n3. Memory Pressure:\n   - Two memory-mapped indices (256d fast + 384d quality) for each data directory\n   - For 100K docs: fast ~49MB + quality ~73MB = 122MB total (f16)\n   - OnceLock prevents double-loading but also prevents releasing memory\n   - Future: consider LRU eviction for multi-directory deployments\n   - Document memory requirements in INFO log at open()\n\n4. File Locking:\n   - Use advisory file locks (flock) during write operations\n   - Readers do not lock (mmap provides read isolation)\n   - Writer holds exclusive lock during rebuild, released on commit\n   - Stale lock detection: check lock age, warn if > 5 minutes\n\n5. Error Recovery:\n   - Missing quality index: degrade to fast-only mode (not an error)\n   - Corrupted fast index: return SearchError::IndexError, log ERROR\n   - Partial write (crash during rebuild): .tmp files cleaned up on next open()\n","created_at":"2026-02-13T20:44:50Z"}]}
{"id":"bd-3un.24","title":"Implement TwoTierSearcher with progressive iterator","description":"Implement the TwoTierSearcher — the main orchestrator that ties everything together into the progressive search experience. This is the crown jewel of frankensearch.\n\npub struct TwoTierSearcher<'a> {\n    index: &'a TwoTierIndex,\n    fast_embedder: Arc<dyn Embedder>,\n    quality_embedder: Option<Arc<dyn Embedder>>,\n    lexical_index: Option<&'a LexicalIndex>,\n    reranker: Option<&'a dyn Reranker>,\n    config: TwoTierConfig,\n}\n\nimpl<'a> TwoTierSearcher<'a> {\n    pub fn new(\n        index: &'a TwoTierIndex,\n        embedder_stack: &EmbedderStack,\n        config: TwoTierConfig,\n    ) -> Self;\n    \n    /// Set optional lexical index for hybrid search\n    pub fn with_lexical(self, index: &'a LexicalIndex) -> Self;\n    \n    /// Set optional reranker\n    pub fn with_reranker(self, reranker: &'a dyn Reranker) -> Self;\n    \n    /// Execute progressive search, yielding phases as an iterator.\n    /// \n    /// Usage:\n    ///   for phase in searcher.search('my query', 10) {\n    ///       match phase {\n    ///           SearchPhase::Initial { results, latency_ms } => {\n    ///               // Display fast results immediately (~15ms)\n    ///           }\n    ///           SearchPhase::Refined { results, latency_ms } => {\n    ///               // Update display with refined rankings (~160ms)\n    ///           }\n    ///           SearchPhase::RefinementFailed { error } => {\n    ///               // Keep showing initial results\n    ///           }\n    ///       }\n    ///   }\n    pub fn search(&self, query: &str, k: usize) -> TwoTierSearchIter<'_>;\n}\n\nIterator implementation:\n\nstruct TwoTierSearchIter<'a> {\n    searcher: &'a TwoTierSearcher<'a>,\n    query: String,\n    k: usize,\n    phase: u8,              // 0 = fast, 1 = quality, 2 = done\n    fast_results: Option<Vec<VectorHit>>,\n    lexical_results: Option<Vec<LexicalHit>>,\n}\n\nimpl Iterator for TwoTierSearchIter<'_> {\n    type Item = SearchPhase;\n    \n    fn next(&mut self) -> Option<SearchPhase> {\n        match self.phase {\n            0 => {\n                // Phase 0: Fast tier\n                let start = Instant::now();\n                \n                // 1. Embed query with fast model (~1ms)\n                let query_vec = self.searcher.fast_embedder.embed(&self.query)?;\n                \n                // 2. Search fast index (~10ms)\n                let candidates = candidate_count(self.k, 0);\n                let fast_hits = self.searcher.index.search_fast(&query_vec, candidates);\n                \n                // 3. Optional: lexical search in parallel (if available)\n                let lexical_hits = self.searcher.lexical_index.map(|li| li.search(&self.query, candidates));\n                \n                // 4. Fuse results\n                let results = if let Some(lh) = &lexical_hits {\n                    rrf_fuse(lh, &fast_hits, self.k, 0, &self.searcher.config.rrf_config())\n                } else {\n                    fast_hits.into_scored_results()\n                };\n                \n                self.fast_results = Some(fast_hits);\n                self.lexical_results = lexical_hits;\n                self.phase = if self.searcher.quality_embedder.is_some() && !self.searcher.config.fast_only { 1 } else { 2 };\n                \n                Some(SearchPhase::Initial { results, latency_ms: start.elapsed().as_millis() as u64 })\n            }\n            1 => {\n                // Phase 1: Quality refinement\n                let start = Instant::now();\n                \n                let quality_embedder = self.searcher.quality_embedder.as_ref().unwrap();\n                \n                // 1. Embed query with quality model (~128ms)\n                let query_vec = match quality_embedder.embed(&self.query) {\n                    Ok(v) => v,\n                    Err(e) => {\n                        self.phase = 2;\n                        return Some(SearchPhase::RefinementFailed { error: e });\n                    }\n                };\n                \n                // 2. Get quality scores for top candidates\n                let fast = self.fast_results.as_ref().unwrap();\n                let max_refine = self.searcher.config.max_refinement_docs.min(fast.len());\n                let candidate_indices: Vec<usize> = fast[..max_refine].iter().map(|h| h.index).collect();\n                let quality_scores = self.searcher.index.quality_scores_for_indices(&query_vec, &candidate_indices);\n                \n                // 3. Blend fast + quality scores\n                let blended = blend_scored(fast, &quality_scores, self.searcher.config.quality_weight, max_refine);\n                \n                // 4. Re-fuse with lexical if available\n                let results = if let Some(lh) = &self.lexical_results {\n                    rrf_fuse(lh, &blended, self.k, 0, &self.searcher.config.rrf_config())\n                } else {\n                    blended.into_scored_results()\n                };\n                \n                // 5. Optional reranking\n                // (if reranker available, rerank top results)\n                \n                self.phase = 2;\n                Some(SearchPhase::Refined { results, latency_ms: start.elapsed().as_millis() as u64 })\n            }\n            _ => None,\n        }\n    }\n}\n\nThis is the most complex component. The key insight is that the iterator pattern allows callers to process fast results IMMEDIATELY while the quality refinement continues. A TUI can display initial results and then smoothly update when refinement completes. An API can return fast results first and stream refinements via SSE.\n\nPerformance budget:\n- Phase 0 (Initial): < 15ms total\n- Phase 1 (Refined): < 200ms total\n\nReference:\n- cass: src/search/two_tier_search.rs (TwoTierIndex, TwoTierSearcher, SearchPhase)\n- xf: src/main.rs lines 1538-1705 (two-tier search implementation)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs (TwoTierSearchIter)","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-13T17:52:20.217498515Z","created_by":"ubuntu","updated_at":"2026-02-13T20:32:42.064531676Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase7","progressive","search","two-tier"],"dependencies":[{"issue_id":"bd-3un.24","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:20.217498515Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.15","type":"blocks","created_at":"2026-02-13T17:55:29.973692711Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T17:55:30.048852207Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.21","type":"blocks","created_at":"2026-02-13T17:55:30.129420997Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T17:55:30.208957915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.43","type":"blocks","created_at":"2026-02-13T20:23:47.195037129Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.24","depends_on_id":"bd-3un.9","type":"blocks","created_at":"2026-02-13T17:55:29.898210902Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":2,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"CRITICAL PATH: This is the single most important component. The TwoTierSearcher is what consumers interact with. It must be:\n\n1. EASY TO USE: One-liner setup with auto-detection\n2. CORRECT: Iterator contract must be exact (Initial → Refined → done)\n3. FAST: Phase 0 must complete in < 15ms\n4. RESILIENT: Quality failure → graceful degradation to fast results\n5. COMPOSABLE: Optional lexical index, optional reranker\n\nThe iterator pattern is the key innovation — it lets callers process each phase independently:\n- TUI: display fast results, then animate ranking changes\n- HTTP API: return fast results, stream refinements via SSE\n- CLI: print fast results, then update display\n- Batch: just collect all phases\n\nThe blend factor of 0.7 (70% quality, 30% fast) was empirically chosen. Too high and fast rankings are ignored; too low and quality model adds little value. 0.7 is the sweet spot where quality dominates but fast provides a useful prior that smooths rankings.","created_at":"2026-02-13T17:56:21Z"},{"id":17,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. TWOTIERMETRICS: Add comprehensive metrics recording for each search operation. From xf hybrid.rs:\n\npub struct TwoTierMetrics {\n    pub query_len: usize,\n    pub k: usize,\n    pub phase1_latency_ms: u64,\n    pub phase2_latency_ms: Option<u64>,\n    pub phase1_candidates: usize,\n    pub phase2_candidates: Option<usize>,\n    pub blend_factor: f32,\n    pub kendall_tau: Option<f64>,     // Rank correlation between phases\n    pub promoted: usize,              // Docs that moved up in refined ranking\n    pub demoted: usize,               // Docs that moved down\n    pub stable: usize,                // Docs that stayed in place\n    pub skip_reason: Option<SkipReason>,\n    pub embedder_fast: String,\n    pub embedder_quality: Option<String>,\n    pub query_class: QueryClass,      // From bd-3un.43\n}\n\npub enum SkipReason {\n    HighConfidence,       // Fast results already high quality\n    Timeout,              // Quality model exceeded budget\n    IndexNotReady,        // Quality index not yet built\n    EmbedderUnavailable,  // Quality embedder not loaded\n    EmbeddingFailed,      // Quality embedding failed\n    FastOnly,             // Config set to fast-only mode\n}\n\nThe metrics should be:\n- Returned alongside SearchPhase results (added to Refined phase)\n- Optionally logged via tracing at INFO level\n- Optionally appended to a JSONL file for offline analysis (like xf's two_tier_metrics.jsonl)\n\n2. QUERY CANONICALIZATION: Before embedding the query, apply canonicalize_query() from bd-3un.42. This should be configurable (TwoTierConfig.canonicalize_queries: bool, default true).\n\n3. CONTENT HASH: When returning ScoredResult, optionally include the content_hash from the vector index (if available) for downstream dedup.\n","created_at":"2026-02-13T20:24:24Z"},{"id":38,"issue_id":"bd-3un.24","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (TwoTierSearcher)\n\n## The Crown Jewel Deserves Alien-Artifact Quality\n\nThe TwoTierSearcher is the consumer-facing API. It should embody all six alien-artifact characteristics.\n\n### 1. Expected Loss Minimization for Phase Decisions\n\nInstead of always running Phase 1 (quality refinement), use expected loss to decide:\n\n  L(skip_quality, quality_would_help) = quality_improvement_expected\n  L(skip_quality, quality_would_not_help) = 0\n  L(run_quality, quality_would_help) = -quality_improvement_expected + quality_latency_cost\n  L(run_quality, quality_would_not_help) = quality_latency_cost\n\n  P(quality_helps | query_features) = logistic(beta_0 + beta_1*query_len + beta_2*rank_spread + ...)\n\n  action* = argmin_a sum_s L(a,s) * P(s|query_features)\n\nThis means: for queries where the fast tier already produces tight rankings (rank_spread is small), SKIP the quality tier entirely. For queries where rankings are spread out, quality refinement is worth the 128ms.\n\nThis can save 50-70% of quality model invocations with < 1% quality loss.\n\n### 2. Galaxy-Brain Transparency Layer\n\nAdd optional \"explain\" mode that shows the math:\n\n  let results = searcher.search_explained(\"rust async\", 10);\n  // Returns SearchPhaseExplained with:\n  //   fast_embed_latency_us: 570\n  //   fast_scores: [(doc_3, 0.89), (doc_7, 0.85), ...]\n  //   quality_embed_latency_us: 128000\n  //   quality_scores: [(doc_7, 0.92), (doc_3, 0.87), ...]\n  //   blend_computation: \"0.3 * 0.89 + 0.7 * 0.87 = 0.876 (doc_3)\"\n  //   rrf_computation: \"1/(60+0) + 1/(60+2) = 0.0328 (doc_7 from both sources)\"\n  //   rank_changes: [(doc_7, +2), (doc_3, -1)]\n  //   kendall_tau: 0.73\n  //   decision: \"quality refinement improved NDCG by estimated 0.12\"\n\nThis makes the sophisticated math ACCESSIBLE, not intimidating.\n\n### 3. Performance: Speculative Quality Embedding\n\nStart the quality embedding IMMEDIATELY when the query arrives, in parallel with the fast tier:\n\n  // Current (sequential):\n  Phase 0: fast_embed → fast_search → fuse → yield Initial\n  Phase 1: quality_embed → blend → yield Refined\n\n  // Optimized (speculative parallel):\n  Phase 0: fast_embed → fast_search → fuse → yield Initial\n             |\n             + quality_embed starts HERE (spawn_blocking or rayon)\n  Phase 1: quality_embed already done → blend → yield Refined\n\nThis can reduce Phase 1 latency from ~140ms to ~20ms (quality embed runs during Phase 0 search).\n\nImplementation: spawn quality embedding on rayon thread pool in the iterator constructor. By the time Phase 0 finishes (~15ms), quality embedding (~128ms) has been running for 15ms already. Phase 1 just waits for the remaining ~113ms instead of all 128ms.\n\nCAVEAT: This trades latency for CPU usage. Only do this when quality_only is false and quality embedder is available. Add a config flag: speculative_quality (default: true).\n\n### 4. Formal Latency SLO\n\nDefine latency service-level objectives with formal monitoring:\n\n  pub struct LatencySLO {\n      phase_0_p99_ms: u64,  // Default: 20ms\n      phase_1_p99_ms: u64,  // Default: 250ms\n      total_p99_ms: u64,    // Default: 300ms\n  }\n\nTrack percentiles using a P2 quantile estimator (no-alloc, O(1) per observation, 5 markers):\n\n  pub struct P2Quantile {\n      markers: [f64; 5],     // Position markers\n      positions: [f64; 5],   // Desired positions\n      heights: [f64; 5],     // Marker heights (quantile estimates)\n  }\n\nThis gives accurate p50/p90/p99 estimates with ZERO allocations and O(1) per update. The P2 algorithm (Jain & Chlamtac 1985) is provably convergent.\n\n### 5. Isomorphism Note\n\nSpeculative quality embedding does NOT change results — the quality embedding is the same regardless of when it starts. Only latency changes. Golden outputs: sha256 identical.\n","created_at":"2026-02-13T20:32:42Z"}]}
{"id":"bd-3un.25","title":"Implement FlashRank cross-encoder reranker","description":"Implement the FlashRank nano cross-encoder reranker. Cross-encoders produce a single relevance score for a (query, document) pair by attending to both simultaneously, which is more accurate than bi-encoder cosine similarity but much slower (hence used as a second-pass reranker on top-k candidates only).\n\npub struct FlashRankReranker {\n    session: Mutex<ort::Session>,     // ONNX Runtime session\n    tokenizer: tokenizers::Tokenizer,\n    max_length: usize,                // 512 tokens\n    name: String,\n    model_dir: PathBuf,\n}\n\nimpl Reranker for FlashRankReranker {\n    fn rerank(&self, query: &str, documents: &[&str]) -> SearchResult<Vec<f32>> {\n        let session = self.session.lock()?;\n        let batch_size = 32;\n        let mut all_scores = Vec::new();\n        \n        for chunk in documents.chunks(batch_size) {\n            // 1. Tokenize (query, doc) pairs\n            // 2. Pad/truncate to max_length\n            // 3. Run ONNX inference\n            // 4. Extract logits → scores\n            all_scores.extend(chunk_scores);\n        }\n        Ok(all_scores)\n    }\n}\n\nModel: flashrank-nano (~4MB, MiniLM-distilled cross-encoder)\n- Very small model, fast inference\n- Good quality for reranking top-100 candidates\n\nAlternative model support:\n- ms-marco-MiniLM-L-6-v2 (baseline cross-encoder)\n- mxbai-rerank-xsmall-v1\n\nDependencies:\n- ort = '2.0.0-rc.9' (ONNX Runtime)\n- tokenizers = '0.21'\n\nFeature gating: Behind 'rerank' feature flag\nFile: frankensearch-rerank/src/flashrank.rs\n\nReference:\n- xf: src/flashrank_reranker.rs, src/mxbai_reranker.rs\n- cass: src/search/fastembed_reranker.rs (ms-marco model)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:52:46.655290201Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:28.666619011Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["flashrank","phase8","rerank"],"dependencies":[{"issue_id":"bd-3un.25","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:46.655290201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.25","depends_on_id":"bd-3un.4","type":"blocks","created_at":"2026-02-13T17:55:35.396759509Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.25","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:35.478072033Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":16,"issue_id":"bd-3un.25","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. SIGMOID ACTIVATION MISSING: The FlashRank ONNX model outputs raw logits, NOT probabilities. The reranker MUST apply sigmoid activation to produce meaningful scores:\n\n   fn sigmoid(x: f32) -> f32 { 1.0 / (1.0 + (-x).exp()) }\n   \n   After getting raw logits from ONNX session output, apply:\n   scores.iter().map(|&s| sigmoid(s)).collect()\n\n   Without sigmoid, the raw logits can be negative or arbitrarily large, making them useless for ranking. Reference: xf flashrank_reranker.rs applies sigmoid.\n\n2. OUTPUT TENSOR NAME FALLBACK: ONNX models use different output tensor names. Try in order: \"logits\", \"output\", \"sentence_embedding\". If none match, use the first output tensor by index. Reference: xf mxbai_reranker.rs has this fallback chain.\n\n3. ONNX SESSION CONFIG: Set GraphOptimizationLevel::Level3 and intra_threads = rayon thread count for best performance. Reference: xf flashrank_reranker.rs.\n","created_at":"2026-02-13T20:24:11Z"},{"id":75,"issue_id":"bd-3un.25","author":"Dicklesworthstone","text":"CRITICAL IMPLEMENTATION NOTE: Sigmoid Activation\n\nThe raw ONNX output from cross-encoder models is a LOGIT, not a probability.\nYou MUST apply sigmoid activation: score = 1.0 / (1.0 + (-logit).exp())\n\nWithout sigmoid:\n  - Logits range from roughly -10 to +10\n  - Comparing raw logits between different query-doc pairs is meaningless\n  - The reranker appears to work but produces garbage rankings\n\nWith sigmoid:\n  - Scores range from 0.0 to 1.0\n  - Scores are interpretable as P(relevant | query, document)\n  - Rankings become meaningful and comparable\n\nThis was a real bug in the source codebase and must not be repeated.\n\nAlso: ONNX output tensor name varies by model:\n  - \"logits\" (most common)\n  - \"output\"\n  - \"sentence_embedding\"\n  - Fallback: first output tensor by index\nUse a name fallback chain, not a hardcoded string.\n","created_at":"2026-02-13T20:46:28Z"}]}
{"id":"bd-3un.26","title":"Implement rerank step (pipeline integration)","description":"Implement the RerankStep that integrates reranking into the search pipeline. This is a composable step that takes top-k candidates and re-scores them for better relevance ordering.\n\npub struct RerankStep {\n    reranker: Box<dyn Reranker>,\n    top_k_rerank: usize,           // Default: 100 (only rerank top N)\n    min_candidates: usize,         // Default: 5 (skip if too few)\n}\n\nimpl RerankStep {\n    pub fn new(reranker: Box<dyn Reranker>) -> Self;\n    pub fn with_top_k(self, k: usize) -> Self;\n    pub fn with_min_candidates(self, min: usize) -> Self;\n    \n    /// Rerank candidates in-place.\n    /// Sets rerank_score on each candidate, then re-sorts by rerank_score.\n    pub fn rerank(&self, query: &str, candidates: &mut [ScoredResult]) -> SearchResult<()> {\n        if candidates.len() < self.min_candidates {\n            return Ok(());  // Skip: too few to benefit from reranking\n        }\n        \n        let rerank_count = candidates.len().min(self.top_k_rerank);\n        let texts: Vec<&str> = candidates[..rerank_count]\n            .iter()\n            .map(|r| r.text.as_str())\n            .collect();\n        \n        let scores = self.reranker.rerank(query, &texts)?;\n        \n        // Verify score count\n        assert_eq!(scores.len(), rerank_count);\n        \n        // Apply rerank scores\n        for (i, score) in scores.iter().enumerate() {\n            candidates[i].rerank_score = Some(*score);\n        }\n        \n        // Re-sort: reranked candidates by rerank_score (desc),\n        // then non-reranked by original score (desc)\n        candidates[..rerank_count].sort_by(|a, b| \n            b.rerank_score.unwrap().partial_cmp(&a.rerank_score.unwrap()).unwrap()\n        );\n    }\n}\n\nDesign decisions:\n- Only top-k candidates are reranked (bounded cost)\n- Candidates below top-k keep their original scores\n- Minimum threshold prevents wasted effort on tiny result sets\n- Logging: model name, candidate count, score range, elapsed time\n\nFile: frankensearch-rerank/src/step.rs\n\nReference:\n- xf: src/rerank_step.rs (RerankStep, DEFAULT_TOP_K_RERANK=100, DEFAULT_MIN_CANDIDATES=5)\n- cass: same pattern in search pipeline","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:52:46.733177577Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:35.561468131Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase8","pipeline","rerank"],"dependencies":[{"issue_id":"bd-3un.26","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:52:46.733177577Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.26","depends_on_id":"bd-3un.25","type":"blocks","created_at":"2026-02-13T17:55:35.561429909Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.27","title":"Implement embedding job queue with backpressure","description":"Implement a background embedding job queue for incremental index building. When new documents arrive, they're queued for embedding and the index is updated in the background.\n\npub struct EmbeddingQueue {\n    config: EmbeddingJobConfig,\n    pending: Mutex<QueueState>,\n}\n\npub struct EmbeddingJobConfig {\n    pub batch_size: usize,               // Default: 32\n    pub flush_interval_ms: u64,          // Default: 5000ms\n    pub backpressure_threshold: usize,   // Default: 1000 pending\n    pub max_retries: u32,                // Default: 3\n    pub retry_base_delay_ms: u64,        // Default: 100ms (exponential backoff)\n}\n\npub struct EmbeddingRequest {\n    pub doc_id: String,\n    pub text: String,\n    pub metadata: Option<serde_json::Value>,\n    pub submitted_at: Instant,\n}\n\nimpl EmbeddingQueue {\n    pub fn enqueue(&self, request: EmbeddingRequest) -> Result<(), BackpressureError>;\n    pub fn drain_batch(&self, max: usize) -> Vec<EmbeddingRequest>;\n    pub fn pending_count(&self) -> usize;\n}\n\npub struct EmbeddingJobRunner {\n    queue: Arc<EmbeddingQueue>,\n    embedder: Arc<dyn Embedder>,\n    index_writer: Arc<Mutex<VectorIndexWriter>>,\n}\n\nimpl EmbeddingJobRunner {\n    pub fn process_batch(&self) -> SearchResult<usize>;\n}\n\nBackpressure: if pending >= threshold, drop new requests and log warning\nDeduplication: same doc_id replaces older request (keeps latest text)\nRetry: exponential backoff (100ms → 200ms → 400ms), up to 3 retries\n\nReference: agent-mail crates/mcp-agent-mail-search-core/src/embedding_jobs.rs lines 225-420","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:53:06.274252563Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:55.449100182Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["background","phase9","queue"],"dependencies":[{"issue_id":"bd-3un.27","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:06.274252563Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.27","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:35.643322319Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.27","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T20:23:21.207347625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.27","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T17:55:35.721098919Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":22,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. CANONICALIZATION INTEGRATION: The EmbeddingJobRunner MUST canonicalize text before embedding. From agent-mail embedding_jobs.rs process_batch_limit() line 568:\n\n   let canonical = canonicalizer.canonicalize(&request.text);\n   if canonical.is_empty() {\n       // Skip low-signal content (returns JobResult::Skipped)\n       continue;\n   }\n   let embedding = embedder.embed(&canonical)?;\n\nThe queue should accept a Canonicalizer instance at construction time.\n\n2. CONTENT HASH FOR DEDUP: Compute SHA-256 of canonicalized text BEFORE embedding. Store alongside the embedding for change detection:\n   - If doc_id exists in queue with same content_hash, skip re-embedding\n   - If doc_id exists with different content_hash, replace (re-embed)\n   This prevents wasted embedding computation when text hasn't changed.\n\n3. HASH-ONLY SKIP: From agent-mail embedding_jobs.rs line 664: Skip upserting hash-only embeddings to the vector index. Hash embeddings are computed on-the-fly during search and don't need to be stored.\n\n4. JOB METRICS: Add atomic counters (from agent-mail JobMetrics):\n   total_succeeded, total_retryable, total_failed, total_skipped\n   total_batches, total_embed_time_us, total_docs_embedded\n   All AtomicU64 with Ordering::Relaxed for lock-free reads.\n","created_at":"2026-02-13T20:25:47Z"},{"id":32,"issue_id":"bd-3un.27","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT & OPTIMIZATION ENHANCEMENT (Embedding Job Queue)\n\n## Mathematical Upgrade: From Fixed Threshold to Queueing Theory + Adaptive Rate Control\n\nCurrent design uses a fixed backpressure_threshold of 1000 pending items. This is ad-hoc and either too aggressive (drops work unnecessarily) or too permissive (allows memory pressure).\n\n### 1. Little's Law for Steady-State Analysis\n\nLittle's Law: L = λW (queue length = arrival rate × service time)\n\nAt steady state, if embeddings arrive at λ docs/sec and each takes W seconds to process:\n  expected_queue_depth = λ × W\n\nFor MiniLM at 128ms per doc (batch of 32 → ~4ms amortized):\n  If λ = 100 docs/sec, expected queue = 100 × 0.004 = 0.4 (no backpressure needed)\n  If λ = 10,000 docs/sec (bulk indexing), expected queue = 10,000 × 0.004 = 40\n\nThe OPTIMAL threshold is a function of arrival rate, not a fixed constant.\n\n### 2. AIMD Adaptive Backpressure (Additive Increase, Multiplicative Decrease)\n\nBorrow from TCP congestion control:\n\n  pub struct AdaptiveBackpressure {\n      window: AtomicUsize,      // Current admission window\n      min_window: usize,        // Floor (default: 32)\n      max_window: usize,        // Ceiling (default: 10_000)\n      increase_step: usize,     // Additive increase (default: 16)\n      decrease_factor: f32,     // Multiplicative decrease (default: 0.5)\n  }\n\n  // On successful batch completion: window += increase_step\n  // On memory pressure detected: window *= decrease_factor\n\nThis automatically adapts to the system's capacity. During bulk indexing, the window opens up. When memory is tight, it contracts. Provably converges to the optimal operating point under stationary conditions.\n\n### 3. Token Bucket for Smooth Rate Limiting\n\nInstead of hard rejection when queue is full, use a token bucket for smooth rate control:\n\n  pub struct TokenBucket {\n      tokens: AtomicF64,\n      max_tokens: f64,         // Burst capacity\n      refill_rate: f64,        // Tokens per second\n      last_refill: Instant,\n  }\n\n  impl TokenBucket {\n      pub fn try_acquire(&self) -> bool {\n          self.refill();\n          if self.tokens.load() >= 1.0 {\n              self.tokens.fetch_sub(1.0);\n              true\n          } else {\n              false\n          }\n      }\n  }\n\nThe refill_rate should be set to the measured embedding throughput (self-tuning: measure actual batch processing rate and update).\n\n### 4. Expected Loss Decision for Drop vs Queue\n\nWhen the queue is near capacity, use expected loss minimization to decide whether to drop or queue:\n\n  L(queue, system_ok) = latency_cost(queue_depth + 1)  // queuing adds latency\n  L(drop, system_ok)  = document_value                  // losing the document\n  L(queue, system_overloaded) = crash_cost              // OOM / degraded service\n  L(drop, system_overloaded) = 0                        // safe, no impact\n\n  action* = argmin_a Σ_s L(a,s) × P(s|queue_depth, memory_usage)\n\nThis gives a principled drop policy that accounts for document importance (if available via metadata priority field).\n","created_at":"2026-02-13T20:29:55Z"}]}
{"id":"bd-3un.28","title":"Implement index refresh worker (background thread)","description":"Implement a background worker thread that periodically processes the embedding queue and refreshes the vector index. This runs on a dedicated OS thread (not async).\n\npub struct IndexRefreshWorker {\n    config: RefreshWorkerConfig,\n    runner: Arc<EmbeddingJobRunner>,\n    shutdown: Arc<AtomicBool>,\n}\n\npub struct RefreshWorkerConfig {\n    pub refresh_interval_ms: u64,    // Default: 1000ms\n    pub max_docs_per_cycle: usize,   // Default: 1000\n}\n\nimpl IndexRefreshWorker {\n    pub fn new(config: RefreshWorkerConfig, runner: Arc<EmbeddingJobRunner>) -> Self;\n    \n    /// Run blocking loop on dedicated thread. Returns when shutdown signal received.\n    pub fn run(&self) {\n        loop {\n            if self.shutdown.load(Ordering::Acquire) { return; }\n            self.run_cycle();  // Process up to max_docs_per_cycle\n            std::thread::sleep(Duration::from_millis(self.config.refresh_interval_ms));\n        }\n    }\n    \n    /// Signal graceful shutdown.\n    pub fn shutdown(&self) {\n        self.shutdown.store(true, Ordering::Release);\n    }\n}\n\nUsage pattern:\nlet worker = Arc::new(IndexRefreshWorker::new(config, runner));\nlet w = worker.clone();\nstd::thread::spawn(move || w.run());  // Dedicated thread\n\n// Later:\nworker.shutdown();\n\nKey design: all search operations are synchronous (no tokio needed). The worker is the only background component, and it uses std::thread::sleep for simplicity. This avoids pulling in an async runtime for consumers that don't need one.\n\nReference: agent-mail crates/mcp-agent-mail-search-core/src/embedding_jobs.rs lines 732-847","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:53:06.349608297Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:51.643798882Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["background","phase9","worker"],"dependencies":[{"issue_id":"bd-3un.28","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:06.349608297Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.28","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T17:55:35.803372793Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.28","depends_on_id":"bd-3un.27","type":"blocks","created_at":"2026-02-13T17:55:35.885948744Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":47,"issue_id":"bd-3un.28","author":"Dicklesworthstone","text":"REVISION: Index Refresh Worker Details\n\n1. Graceful Shutdown Semantics:\n   - AtomicBool::store(true, Ordering::Release) signals shutdown\n   - Worker checks flag at top of each cycle, not mid-batch\n   - On shutdown signal: finish current batch, flush pending writes, then exit\n   - join() on worker thread with timeout (5s), then log WARN if exceeded\n   - Drop impl calls shutdown + join to prevent leaked threads\n\n2. Error Recovery:\n   - Failed embedding: log WARN, increment retry counter, re-queue with exponential backoff\n   - Failed index write: log ERROR, skip this cycle, retry next cycle\n   - Repeated failures (3+ consecutive): pause worker for 30s, log ERROR with stack\n   - Never panic in worker thread (catch_unwind wrapper with error logging)\n\n3. Metrics (cross-reference bd-3un.39 tracing):\n   - Counter: documents_embedded_total, documents_failed_total\n   - Gauge: queue_depth, worker_state (idle/processing/error/shutdown)\n   - Histogram: batch_duration_ms, docs_per_second\n   - Log at INFO per cycle: \"index_refresh_cycle docs={n} duration_ms={ms} queue_remaining={q}\"\n\n4. Integration Points:\n   - Receives jobs from bd-3un.27 (embedding job queue) via crossbeam channel\n   - Triggers reload on bd-3un.23 (TwoTierIndex) after successful write\n   - Reports staleness metrics to bd-3un.41 (staleness detection)\n   - Worker is the ONLY component that writes to vector indices (single-writer guarantee)\n\n5. Thread Configuration:\n   - Dedicated OS thread (std::thread::spawn), NOT tokio/async\n   - Thread name: \"frankensearch-refresh\" for debuggability\n   - Thread priority: normal (not elevated, to avoid starving search threads)\n   - Stack size: default (8MB) is sufficient\n","created_at":"2026-02-13T20:44:51Z"}]}
{"id":"bd-3un.29","title":"Design and implement Cargo feature flags","description":"Design the feature flag system that allows consumers to pick exactly the components they need. Feature flags control which dependencies are compiled in, keeping the crate lightweight by default.\n\nFeature hierarchy:\n\n[features]\ndefault = ['hash']  # Minimal: hash embedder only, always works\n\n# Individual components\nhash = []                                    # FNV-1a hash embedder (zero deps)\nmodel2vec = ['dep:safetensors', 'dep:tokenizers', 'dep:dirs']  # Potion-128M fast embedder\nfastembed = ['dep:fastembed']                # MiniLM-L6 quality embedder (brings ONNX)\nlexical = ['dep:tantivy']                   # Tantivy full-text search\nrerank = ['dep:ort', 'dep:tokenizers']      # Cross-encoder rerankers\nann = ['dep:hnsw_rs']                       # HNSW approximate nearest neighbors\ndownload = ['dep:reqwest']                  # Model download from HuggingFace\n\n# Bundles\nsemantic = ['hash', 'model2vec', 'fastembed']  # All embedding models\nhybrid = ['semantic', 'lexical']               # Semantic + lexical + RRF fusion\nfull = ['hybrid', 'rerank', 'ann', 'download'] # Everything\n\n# Performance\nsimd = ['dep:wide']  # SIMD dot product (recommended, default in most bundles)\n\nDesign principles:\n1. 'hash' is always available (zero deps, always works)\n2. Each ML model is independently selectable\n3. 'full' gives you everything but isn't the default\n4. Consumer picks their budget: just 'hash' for testing, 'semantic' for embeddings, 'hybrid' for production\n\nConditional compilation in code:\n#[cfg(feature = 'model2vec')]\npub mod model2vec_embedder;\n\n#[cfg(feature = 'fastembed')]\npub mod fastembed_embedder;\n\nWorkspace-level feature forwarding:\nEach sub-crate exposes its features, and the facade crate re-exports them with forwarding.\n\nReference: agent-mail crates/mcp-agent-mail-search-core/Cargo.toml (semantic, hybrid features)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:53:36.360407614Z","created_by":"ubuntu","updated_at":"2026-02-13T20:13:42.387820372Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","features","phase10"],"dependencies":[{"issue_id":"bd-3un.29","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:36.360407614Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.29","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T17:55:40.689834540Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":14,"issue_id":"bd-3un.29","author":"Dicklesworthstone","text":"REVISION NOTES (review pass):\n\nSIMD CHANGE: The 'simd' feature flag should be REMOVED. The 'wide' crate should be an unconditional dependency of frankensearch-index. Rationale:\n1. wide provides portable SIMD (x86 SSE2/AVX2, ARM NEON) with automatic scalar fallback\n2. There is NO scalar dot product implementation defined anywhere in the beads\n3. Without wide, vector search would have no dot product function at all\n4. wide is zero-overhead on non-SIMD platforms (degrades to scalar loops)\n5. It's a small, well-maintained crate with no transitive dependencies\n\nSimilarly, 'half' (f16) should be unconditional in frankensearch-index since the FSVI format uses f16 by default.\n\nUPDATED feature list (remove 'simd', keep everything else):\n  default = ['hash']\n  hash = []\n  model2vec = ['dep:safetensors', 'dep:tokenizers', 'dep:dirs']\n  fastembed = ['dep:fastembed']\n  lexical = ['dep:tantivy']\n  rerank = ['dep:ort', 'dep:tokenizers']\n  ann = ['dep:hnsw_rs']\n  download = ['dep:reqwest']\n  semantic = ['hash', 'model2vec', 'fastembed']\n  hybrid = ['semantic', 'lexical']\n  full = ['hybrid', 'rerank', 'ann', 'download']\n\nIn frankensearch-index/Cargo.toml:\n  [dependencies]\n  wide = \"0.7\"    # Always available, portable SIMD\n  half = \"2.4\"    # Always available, f16 support\n  memmap2 = \"0.9\" # Always available, memory-mapped I/O\n","created_at":"2026-02-13T20:13:42Z"}]}
{"id":"bd-3un.3","title":"Define Embedder trait and core embedding types","description":"Define the core Embedder trait in frankensearch-core. This is the central abstraction that all embedding models implement. Must be object-safe (dyn Embedder) for runtime polymorphism.\n\nTrait design (synthesized from all 3 codebases):\n\npub trait Embedder: Send + Sync {\n    /// Generate embedding for a single text.\n    fn embed(&self, text: &str) -> SearchResult<Vec<f32>>;\n    \n    /// Batch embedding for multiple texts (default: sequential).\n    fn embed_batch(&self, texts: &[&str]) -> SearchResult<Vec<Vec<f32>>> {\n        texts.iter().map(|t| self.embed(t)).collect()\n    }\n    \n    /// Embedding dimension (e.g., 256 for potion, 384 for MiniLM).\n    fn dimension(&self) -> usize;\n    \n    /// Unique identifier string (e.g., 'minilm-384', 'fnv1a-384').\n    fn id(&self) -> &str;\n    \n    /// Human-readable model name.\n    fn model_name(&self) -> &str;\n    \n    /// Whether this produces semantically meaningful embeddings.\n    /// Hash embedders return false; ML models return true.\n    fn is_semantic(&self) -> bool;\n    \n    /// Performance category for tiering decisions.\n    fn category(&self) -> ModelCategory;\n    \n    /// Whether this model supports Matryoshka Representation Learning (dim truncation).\n    fn supports_mrl(&self) -> bool { false }\n    \n    /// Truncate embedding to target dimension (only valid if supports_mrl()).\n    fn truncate_embedding(&self, embedding: &[f32], target_dim: usize) -> SearchResult<Vec<f32>> {\n        if target_dim >= embedding.len() { return Ok(embedding.to_vec()); }\n        Ok(l2_normalize(&embedding[..target_dim]))\n    }\n}\n\nSupporting types:\n\npub enum ModelCategory {\n    /// Hash-based (FNV-1a): ~0.07ms, no semantic meaning\n    HashEmbedder,\n    /// Static token embeddings (Model2Vec/potion): ~0.5-1ms, decent semantics\n    StaticEmbedder,\n    /// Transformer inference (MiniLM, BGE): ~100-500ms, best quality\n    TransformerEmbedder,\n}\n\npub enum ModelTier {\n    /// Ultra-fast for immediate results (hash or static embedder)\n    Fast,\n    /// High-quality for refinement (transformer embedder)\n    Quality,\n}\n\npub struct ModelInfo {\n    pub id: String,\n    pub name: String,\n    pub dimension: usize,\n    pub category: ModelCategory,\n    pub tier: ModelTier,\n    pub is_semantic: bool,\n    pub supports_mrl: bool,\n    pub huggingface_id: Option<String>,\n    pub size_bytes: Option<u64>,\n    pub license: Option<String>,\n}\n\nAlso provide helper functions:\n- pub fn l2_normalize(vec: &[f32]) -> Vec<f32>\n- pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32\n\nReference implementations:\n- cass: src/search/embedder.rs lines 60-151\n- xf: src/embedder.rs (adds category(), supports_mrl())\n- agent-mail: crates/mcp-agent-mail-search-core/src/embedder.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:30.625424732Z","created_by":"ubuntu","updated_at":"2026-02-13T20:26:03.315792992Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1","trait"],"dependencies":[{"issue_id":"bd-3un.3","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:30.625424732Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.3","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:00.598580912Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":23,"issue_id":"bd-3un.3","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. MODEL SEARCH PATHS: Embedder implementations should check multiple standard paths for model files, in priority order:\n   a. FRANKENSEARCH_MODEL_DIR env var (explicit override)\n   b. ~/.cache/frankensearch/models/{model_name}/\n   c. ~/.local/share/frankensearch/models/{model_name}/\n   d. ~/.cache/huggingface/hub/models--{org}--{model}/snapshots/{latest}/\n   Reference: xf flashrank_reranker.rs model search paths.\n\n2. is_ready() METHOD: Add an is_ready() method (default: true) that checks if the model is loaded and functional. From agent-mail embedder.rs:\n   fn is_ready(&self) -> bool { true }\n   The hash embedder is always ready. ML models check if ONNX session is loaded.\n\n3. EMBEDDER ERROR TYPES: The EmbedderResult should use SearchError variants, not a separate EmbedderError. Keep the error hierarchy unified. However, from xf embedder.rs, define clear variant mappings:\n   - Unavailable -> SearchError::EmbedderUnavailable\n   - EmbeddingFailed -> SearchError::EmbeddingFailed\n   - InvalidInput -> SearchError::InvalidConfig\n\n4. BATCH EMBEDDING DEFAULT: The default embed_batch() calls embed() sequentially. For ML models, override with actual batch processing for 2-3x throughput during indexing.\n","created_at":"2026-02-13T20:26:03Z"}]}
{"id":"bd-3un.30","title":"Design public API surface and facade re-exports","description":"Design the public API surface of the frankensearch facade crate. This is what consumers actually import and use. It should provide a clean, ergonomic API that hides internal crate boundaries.\n\nThe frankensearch crate (facade) re-exports from sub-crates:\n\n// frankensearch/src/lib.rs\n\n// Core types (always available)\npub use frankensearch_core::{\n    SearchError, SearchResult,\n    Embedder, ModelCategory, ModelTier, ModelInfo,\n    Reranker,\n    ScoredResult, VectorHit, FusedHit, SourceContribution,\n    SearchMode, SearchPhase,\n    l2_normalize, cosine_similarity,\n};\n\n// Embedders\npub use frankensearch_embed::{HashEmbedder};\n#[cfg(feature = 'model2vec')]\npub use frankensearch_embed::{Model2VecEmbedder};\n#[cfg(feature = 'fastembed')]\npub use frankensearch_embed::{FastEmbedEmbedder};\npub use frankensearch_embed::{EmbedderStack, TwoTierAvailability};\n\n// Vector index\npub use frankensearch_index::{VectorIndex, VectorIndexWriter};\n#[cfg(feature = 'simd')]\npub use frankensearch_index::{dot_product_f16_f32};\n\n// Lexical search\n#[cfg(feature = 'lexical')]\npub use frankensearch_lexical::{LexicalIndex, LexicalQuery, LexicalHit, IndexableDocument};\n\n// Fusion\npub use frankensearch_fusion::{\n    TwoTierConfig, TwoTierIndex, TwoTierSearcher,\n    RrfConfig, rrf_fuse,\n    blend_two_tier, min_max_normalize,\n};\n\n// Reranking\n#[cfg(feature = 'rerank')]\npub use frankensearch_rerank::{FlashRankReranker, RerankStep};\n\n// Model management\npub use frankensearch_embed::{EmbedderRegistry, RegisteredEmbedder, ModelManifest};\n\nErgonomic convenience:\n// One-liner to get started:\nlet search = frankensearch::TwoTierSearcher::auto(data_dir)?;\nfor phase in search.search('my query', 10) {\n    // handle results\n}\n\nThe 'auto' constructor should detect available models and build the appropriate stack automatically.\n\nFile: frankensearch/src/lib.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:53:36.438488543Z","created_by":"ubuntu","updated_at":"2026-02-13T20:47:17.478533216Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","facade","phase10"],"dependencies":[{"issue_id":"bd-3un.30","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:53:36.438488543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.12","type":"blocks","created_at":"2026-02-13T17:55:41.126092025Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T17:55:40.871811355Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T17:55:40.785987719Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.26","type":"blocks","created_at":"2026-02-13T17:55:40.956981386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.30","depends_on_id":"bd-3un.29","type":"blocks","created_at":"2026-02-13T17:55:41.042653297Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":78,"issue_id":"bd-3un.30","author":"Dicklesworthstone","text":"REVISION: Facade API Design Principles\n\nThe facade is the critical bottleneck (betweenness 100%). Its design determines UX quality.\n\n1. One-Liner Entry Point:\n   let searcher = frankensearch::TwoTierSearcher::auto(data_dir)?;\n   This must work with zero configuration. auto() calls:\n   - EmbedderStack::auto_detect(data_dir) for model discovery\n   - TwoTierConfig::default() for sane defaults\n   - TwoTierIndex::open(data_dir) for index loading\n   - Returns error ONLY if data_dir doesn't exist or has no index files\n\n2. Error Propagation:\n   Facade functions return SearchResult<T> (alias for Result<T, SearchError>).\n   NEVER panic. NEVER unwrap. All failures are expressed as SearchError variants.\n   The facade adds NO new error types -- it re-uses SearchError from core.\n\n3. Conditional Compilation:\n   Each sub-module is gated:\n   #[cfg(feature = \"semantic\")] pub mod embed;\n   #[cfg(feature = \"lexical\")] pub mod lexical;\n   #[cfg(feature = \"rerank\")] pub mod rerank;\n   The facade crate with default features (hash only) should compile in <5s.\n\n4. Re-export Strategy:\n   - Re-export types users need: TwoTierSearcher, SearchPhase, ScoredResult, TwoTierConfig\n   - Re-export traits users implement: Embedder, Reranker\n   - Do NOT re-export internal types: VectorIndex internals, SIMD functions, queue internals\n   - pub use frankensearch_core::{SearchError, SearchResult, ScoredResult, SearchPhase};\n   - pub use frankensearch_fusion::{TwoTierSearcher, TwoTierConfig};\n\n5. Version Compatibility:\n   The facade's public API is the stability surface. Internal crate APIs can change freely.\n   Document which types are part of the public API vs internal in rustdoc.\n","created_at":"2026-02-13T20:47:17Z"}]}
{"id":"bd-3un.31","title":"Write unit tests for all core components","description":"Write comprehensive unit tests for all frankensearch components. Tests should cover happy paths, edge cases, and error conditions.\n\nTest categories by component:\n\n1. Hash Embedder Tests:\n   - Known input→output regression tests (deterministic)\n   - Dimension configuration (128, 256, 384)\n   - Empty input handling\n   - Unicode text handling\n   - L2 normalization verification (unit length)\n\n2. Score Normalization Tests:\n   - min_max_normalize: [0.5, 1.0, 0.0] → [0.5, 1.0, 0.0]\n   - All equal scores → [1.0, 1.0, 1.0]\n   - Single score → [1.0]\n   - Empty input → []\n   - Negative scores handling\n\n3. RRF Fusion Tests:\n   - Lexical-only (no semantic results)\n   - Semantic-only (no lexical results)\n   - Overlap scoring (docs in both get higher scores)\n   - Offset and limit handling\n   - Empty inputs\n\n4. Two-Tier Blending Tests:\n   - blend_factor=0.0 → fast scores only\n   - blend_factor=1.0 → quality scores only\n   - blend_factor=0.7 → weighted combination\n   - Docs only in fast set\n   - Docs only in quality set\n   - Rank correlation metrics\n\n5. Vector Index Tests:\n   - Write and read back (round-trip)\n   - f16 quantization accuracy\n   - Header CRC32 verification\n   - Corrupted file detection\n   - Dimension mismatch detection\n\n6. SIMD Dot Product Tests:\n   - Known vectors → expected dot product\n   - Orthogonal vectors → 0.0\n   - Identical vectors → 1.0 (if normalized)\n   - Various dimensions (including non-8-aligned for remainder handling)\n   - Scalar fallback equivalence\n\n7. Embedder Stack Tests:\n   - Auto-detection with all models available\n   - Fallback to hash-only\n   - Fast-only availability\n   - Quality-only availability\n\nTarget: > 80% line coverage for core modules.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:54:09.804163360Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:55.131766384Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["phase11","testing","unit"],"dependencies":[{"issue_id":"bd-3un.31","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:09.804163360Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T17:55:49.431519547Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.19","type":"blocks","created_at":"2026-02-13T17:55:49.270170941Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.20","type":"blocks","created_at":"2026-02-13T17:55:49.350892548Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.24","type":"blocks","created_at":"2026-02-13T17:55:49.191034775Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.31","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:13:16.243990500Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":12,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"REVISION NOTES (review pass):\n\n1. INLINE TESTS: Per the epic's TESTING POLICY, each component should have its own #[cfg(test)] inline tests. This bead (bd-3un.31) covers CROSS-COMPONENT and ORCHESTRATION-LEVEL unit tests that verify interactions between components.\n\n2. LOGGING IN TESTS: All tests must configure tracing-test subscriber to capture log events. Key assertions should verify that expected log events are emitted:\n   - Verify \"search_completed\" event is logged with correct fields\n   - Verify \"quality_model_unavailable\" WARN is logged when quality model missing\n   - Verify per-phase timing spans are emitted\n\n3. TEST FIXTURES: Use the ground truth corpus from bd-3un.38 (tests/fixtures/) for any tests needing realistic data. Do NOT generate random data inline -- use the shared fixtures.\n\n4. COVERAGE REPORT: Include a #[cfg(test)] function that lists all tested component interactions and their coverage status, so we can track what's tested.\n","created_at":"2026-02-13T20:13:12Z"},{"id":51,"issue_id":"bd-3un.31","author":"Dicklesworthstone","text":"REVISION: Unit Tests Edge Case Enumeration\n\nBeyond the 7 test categories already specified, each category MUST include these edge cases:\n\n1. Scoring Edge Cases (applies to normalization, RRF, blending):\n   - All scores identical: verify no NaN/Inf, all outputs equal\n   - All scores zero: verify no division-by-zero\n   - Single result: verify single-element collections work\n   - Empty result set: verify empty input -> empty output (no panic)\n   - NaN score: verify NaN-safe handling via total_cmp() (NaN sorts last)\n   - Negative scores: BM25 can produce negative scores in some configurations\n   - Very large scores: f32::MAX, verify no overflow in summation\n   - Score of exactly 0.0 and exactly 1.0: boundary conditions\n\n2. RRF-Specific Tests:\n   - Same document in both lexical and semantic: verify score addition\n   - Document in only one source: verify single contribution score\n   - K=0 edge case: formula becomes 1/(rank+1), still valid\n   - K=60 with rank=0: verify 1/61 = 0.016393...\n   - 4-level tie-breaking: construct cases that exercise each tiebreaker\n   - Deterministic ordering: same inputs always produce same output order\n\n3. Two-Tier Blending Tests:\n   - blend_factor=0.0: output should equal fast-tier scores exactly\n   - blend_factor=1.0: output should equal quality-tier scores exactly\n   - blend_factor=0.5: output should be arithmetic mean\n   - Missing quality score: verify 0.0 default is applied correctly\n   - kendall_tau with identical rankings: should return 1.0\n   - kendall_tau with reversed rankings: should return -1.0\n\n4. Vector Index Tests:\n   - Zero-dimension vector: should be rejected at index creation\n   - Dimension mismatch: query dim != index dim -> clear error\n   - Empty index (0 records): search returns empty, no crash\n   - Index with 1 record: top-k=10 returns 1 result\n   - top-k > record_count: returns all records, no padding\n   - CRC32 verification: corrupt 1 byte in header -> detect on open\n\n5. Hash Embedder Tests:\n   - Empty string: should produce zero vector (or handled gracefully)\n   - Single token: verify deterministic output\n   - Same input twice: verify identical embeddings (determinism)\n   - Very long input (100K chars): verify truncation + consistent output\n   - Unicode input: verify no panic on multi-byte UTF-8\n\n6. Embedder Stack Tests:\n   - No models available: should return HashOnly availability\n   - Only fast model: should return FastOnly\n   - Only quality model: should return QualityOnly\n   - Both models: should return Full\n   - DimReduceEmbedder: verify truncation preserves direction (cosine > 0.99)\n\n7. Coverage Tracking:\n   - Each test file includes a coverage_check() function\n   - Lists all public functions in the module under test\n   - Asserts each function has at least one test (compile-time reminder)\n","created_at":"2026-02-13T20:44:55Z"}]}
{"id":"bd-3un.32","title":"Write integration tests (end-to-end search pipeline)","description":"Write integration tests that exercise the full search pipeline end-to-end using the hash embedder (no ML model downloads needed).\n\nTest scenarios:\n\n1. Basic Two-Tier Flow (with hash embedder as both tiers):\n   - Index 100 test documents\n   - Search with a query\n   - Verify SearchPhase::Initial is yielded first\n   - Verify SearchPhase::Refined is yielded second\n   - Verify result count <= k\n\n2. Hybrid Search (lexical + semantic):\n   - Index documents in both Tantivy and vector index\n   - Search with hybrid mode\n   - Verify RRF fusion produces results from both sources\n   - Verify documents appearing in both rank higher\n\n3. Reranking Integration:\n   - Search produces initial results\n   - Apply rerank step\n   - Verify rerank_score is set\n   - Verify re-ordering occurred\n\n4. Progressive Iterator Contract:\n   - fast_only mode: only Initial phase, no Refined\n   - quality_only mode: no Initial, only one result set\n   - Normal mode: Initial then Refined (exactly 2 phases)\n   - Quality failure: Initial then RefinementFailed\n\n5. Configuration Tests:\n   - Env var overrides work\n   - Builder pattern produces correct config\n   - Invalid blend_factor rejected\n   - Invalid dimensions rejected\n\n6. Persistence Tests:\n   - Create index, close, reopen, search\n   - Multiple embedder IDs in separate index files\n   - Concurrent reads (no data races)\n\nTest data: Use a fixed corpus of 100 synthetic documents covering diverse topics, stored in tests/fixtures/.\n\nAll integration tests should work with 'default' features (hash embedder only, no ML downloads).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:54:09.885746030Z","created_by":"ubuntu","updated_at":"2026-02-13T20:13:29.179745878Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["integration","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.32","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:09.885746030Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.32","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T17:55:49.509909806Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.32","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:13:29.179707716Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":13,"issue_id":"bd-3un.32","author":"Dicklesworthstone","text":"REVISION NOTES (review pass):\n\n1. LOGGING REQUIREMENTS: Every integration test must:\n   - Initialize tracing-test subscriber for log capture\n   - Log timing for each pipeline stage (embed, search, fuse, rerank)\n   - Assert that expected tracing events were emitted with correct fields\n   - On failure: dump full tracing output to stderr for debugging\n\n2. TEST FIXTURES: Use the shared corpus from bd-3un.38 (tests/fixtures/) -- do NOT create ad-hoc test data inline. The ground truth relevance data enables NDCG@10 regression checks.\n\n3. DETAILED FAILURE OUTPUT: When a test fails, it should print:\n   - Query text\n   - Expected top-10 doc_ids (from ground truth)\n   - Actual top-10 doc_ids with scores\n   - Phase (Initial vs Refined)\n   - Latency breakdown\n   - All tracing spans for the failed query\n\n4. FEATURE MATRIX: Tests should run across feature combinations:\n   - default (hash only): basic pipeline works\n   - semantic: embedder stack works\n   - lexical: Tantivy integration works\n   - hybrid: fusion works\n   - full: everything works together\n\nUse #[cfg(feature = \"...\")] to conditionally compile feature-specific tests.\n","created_at":"2026-02-13T20:13:25Z"}]}
{"id":"bd-3un.33","title":"Write benchmarks for performance-critical paths","description":"Write criterion benchmarks for the performance-critical paths. These establish regression baselines and verify we meet performance budgets.\n\nBenchmark groups:\n\n1. SIMD Dot Product:\n   - f16×f32 dot product: 128, 256, 384, 768 dimensions\n   - f32×f32 dot product: same dimensions\n   - Scalar vs SIMD comparison\n   - Target: < 1μs for 384-dim\n\n2. Hash Embedder:\n   - Short text (10 words): target < 0.1ms\n   - Medium text (100 words): target < 0.5ms\n   - Long text (1000 words): target < 2ms\n\n3. Vector Search (brute-force):\n   - 1K vectors × 384-dim: target < 1ms\n   - 10K vectors × 384-dim: target < 15ms\n   - 100K vectors × 384-dim: target < 150ms\n\n4. RRF Fusion:\n   - 100 lexical + 100 semantic results: target < 0.5ms\n   - 1000 + 1000 results: target < 5ms\n\n5. Score Normalization:\n   - 100 scores: target < 0.01ms\n   - 10K scores: target < 0.1ms\n\n6. Vector Index I/O:\n   - Write 10K records: target < 500ms\n   - Open/mmap existing index: target < 10ms\n\nPerformance budgets (from cass/xf):\n- Full pipeline (hash embed + search + fusion): < 50ms for 10K docs\n- These match the perf.rs budgets from the source projects\n\nDependencies: criterion = '0.5' (dev-dependency)\nFile: benches/search_bench.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:09.967451570Z","created_by":"ubuntu","updated_at":"2026-02-13T20:32:42.903149113Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["benchmarks","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.33","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:09.967451570Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.33","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T17:55:49.592472722Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":39,"issue_id":"bd-3un.33","author":"Dicklesworthstone","text":"EXTREME OPTIMIZATION ENHANCEMENT (Benchmarks)\n\n## Profile-Driven Benchmark Design\n\n### Mandatory Baseline Protocol\n\nBEFORE any optimization, capture golden baselines per the extreme-optimization methodology:\n\n  hyperfine --warmup 3 --runs 10 'cargo run --example bench_quick'\n\n### Benchmark Groups with Opportunity Matrices\n\nEach benchmark should compute AND REPORT an opportunity matrix:\n\n  | Function            | p50 (us) | p99 (us) | Budget (us) | Headroom | Priority |\n  |---------------------|----------|----------|-------------|----------|----------|\n  | dot_product_f16_f32 | 1.2      | 1.8      | 2.0         | 10%      | Low      |\n  | search_top_k_10K    | 12000    | 15000    | 15000       | 0%       | HIGH     |\n  | rrf_fuse_200        | 45       | 80       | 500         | 84%      | Low      |\n\nAuto-classify: if headroom < 20%, priority = HIGH (at risk of budget violation).\n\n### Regression Detection with Statistical Rigor\n\nInstead of simple \"is it faster?\", use proper statistical testing:\n\n1. Welch's t-test for paired comparisons (accounts for unequal variances)\n2. Effect size (Cohen's d) to distinguish meaningful vs statistically-significant-but-tiny\n3. Only report regressions when:\n   - p-value < 0.01 AND\n   - Cohen's d > 0.5 (medium effect) AND\n   - Absolute change > 5% of budget\n\nThis prevents false alarm fatigue from noisy CI benchmarks.\n\n### Cache State Awareness\n\nBenchmarks MUST test both cold and warm cache states:\n\n  // Cold cache: drop OS page cache before each run\n  sync && echo 3 > /proc/sys/vm/drop_caches  // (requires root)\n\n  // Warm cache: run 3x warmup before measuring\n  for _ in 0..3 { search_top_k(index, query, 10); }\n\nReport both. Cold cache numbers are what users experience on first query; warm cache is steady-state.\n\n### Flamegraph Integration\n\nAuto-generate flamegraphs when a benchmark exceeds budget:\n\n  if measured_p99 > budget * 0.9 {\n      // Trigger: cargo flamegraph --bench search_bench -- --bench search_top_k_10K\n      // Save to benches/flamegraphs/{date}_{benchmark}.svg\n  }\n\n### Isomorphism Proofs in Benchmarks\n\nEvery benchmark that tests an optimization should VERIFY golden outputs:\n\n  #[bench]\n  fn bench_search_with_prefetch(b: &mut Bencher) {\n      // Setup: compute golden results WITHOUT optimization\n      let golden = search_top_k_baseline(index, query, 10);\n\n      b.iter(|| {\n          let results = search_top_k_optimized(index, query, 10);\n          // VERIFY: same results as golden\n          assert_eq!(results.len(), golden.len());\n          for (r, g) in results.iter().zip(golden.iter()) {\n              assert_eq!(r.doc_id, g.doc_id);\n              assert!((r.score - g.score).abs() < 1e-6);\n          }\n      });\n  }\n\nThis ensures optimizations NEVER silently change behavior.\n","created_at":"2026-02-13T20:32:42Z"}]}
{"id":"bd-3un.34","title":"Write API documentation and usage examples","description":"Write comprehensive API documentation (rustdoc) and usage examples for frankensearch.\n\nDocumentation requirements:\n1. Crate-level docs (lib.rs) with:\n   - Overview of the 2-tier hybrid search architecture\n   - Quick start example\n   - Feature flag guide\n   - Performance characteristics\n\n2. Module-level docs for each public module explaining purpose and usage\n\n3. All public types and functions must have rustdoc comments with:\n   - Description of what it does\n   - Parameter explanations\n   - Return value semantics\n   - Error conditions\n   - Usage example (at least for key APIs)\n\n4. Examples directory (examples/):\n   - basic_search.rs: Hash embedder + vector search (no ML deps)\n   - hybrid_search.rs: Tantivy + semantic fusion\n   - two_tier_search.rs: Progressive search with fast + quality\n   - custom_embedder.rs: Implementing a custom Embedder trait\n   - index_documents.rs: Building an index from scratch\n\n5. Architecture overview in docs:\n   - Explain the 2-tier concept and why it exists\n   - Link to the X post / bakeoff results\n   - Explain RRF fusion algorithm\n   - Show the data flow diagram\n   - Performance comparison table (hash vs potion vs MiniLM)\n\nEach example should be self-contained and compilable with appropriate feature flags.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:20.698803786Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:49.673762474Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","examples","phase12"],"dependencies":[{"issue_id":"bd-3un.34","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:20.698803786Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.34","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T17:55:49.673691571Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.35","title":"Migrate xf to use frankensearch crate","description":"Replace xf's bespoke search implementation with frankensearch. xf was one of the two original projects (along with cass) where the 2-tier hybrid search was developed.\n\nFiles to replace in /data/projects/xf:\n- src/embedder.rs → use frankensearch::Embedder trait\n- src/hash_embedder.rs → use frankensearch::HashEmbedder\n- src/fastembed_embedder.rs → use frankensearch::FastEmbedEmbedder\n- src/model2vec_embedder.rs → use frankensearch::Model2VecEmbedder\n- src/static_mrl_embedder.rs → use frankensearch MRL support\n- src/vector.rs → use frankensearch::VectorIndex\n- src/hybrid.rs → use frankensearch::{rrf_fuse, blend_two_tier}\n- src/search.rs → use frankensearch::LexicalIndex\n- src/reranker.rs → use frankensearch::Reranker trait\n- src/flashrank_reranker.rs → use frankensearch::FlashRankReranker\n- src/mxbai_reranker.rs → adapt to frankensearch\n- src/rerank_step.rs → use frankensearch::RerankStep\n- src/model_registry.rs → use frankensearch::EmbedderRegistry\n- src/config.rs (TwoTierConfig) → use frankensearch::TwoTierConfig\n\nMigration strategy:\n1. Add frankensearch as workspace dependency\n2. Replace trait definitions with re-exports\n3. Replace implementations one at a time, verifying tests pass\n4. Remove replaced source files\n5. Update Cargo.toml to remove now-unused direct deps\n6. Run full test suite + bakeoff validation\n\nBinary format migration:\n- xf uses XFVI magic → frankensearch uses FSVI magic\n- Need a one-time migration tool or support reading legacy format\n- OR: rebuild indices (simpler, preferred)\n\nVerify: all xf search tests pass, bakeoff results unchanged.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:54.296387127Z","created_by":"ubuntu","updated_at":"2026-02-13T20:13:47.786123577Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["migration","phase13","xf"],"dependencies":[{"issue_id":"bd-3un.35","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:54.296387127Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.35","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T17:55:49.754624203Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.35","depends_on_id":"bd-3un.40","type":"blocks","created_at":"2026-02-13T20:13:47.786058255Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":8,"issue_id":"bd-3un.35","author":"Dicklesworthstone","text":"XF MIGRATION NOTES: xf has the most mature two-tier implementation since it was the first project where this was developed. The xf migration is the easiest because its search code is relatively self-contained in individual files.\n\nBinary index migration: xf currently stores vector.fast.idx and vector.quality.idx with XFVI magic bytes. After migration, these become FSVI format. Since index rebuilding is fast (seconds for typical X archive sizes), the simplest approach is to delete old indices and rebuild with 'xf index --rebuild'.\n\nImportant xf-specific code to NOT migrate (keep in xf):\n- Tweet/Like/DM/Grok type handling (domain-specific)\n- X archive parsing (totally unrelated to search)\n- TUI rendering (uses frankensearch search results but renders its own way)\n- Config file format (xf has its own config.toml structure)","created_at":"2026-02-13T17:57:22Z"}]}
{"id":"bd-3un.36","title":"Migrate cass to use frankensearch crate","description":"Replace cass (coding_agent_session_search) search implementation with frankensearch.\n\nFiles to replace in /data/projects/coding_agent_session_search:\n- src/search/embedder.rs → frankensearch::Embedder\n- src/search/hash_embedder.rs → frankensearch::HashEmbedder\n- src/search/fastembed_embedder.rs → frankensearch::FastEmbedEmbedder\n- src/search/vector_index.rs → frankensearch::VectorIndex\n- src/search/ann_index.rs → frankensearch HNSW (if 'ann' feature)\n- src/search/two_tier_search.rs → frankensearch::TwoTierSearcher\n- src/search/tantivy.rs → frankensearch::LexicalIndex\n- src/search/reranker.rs → frankensearch::Reranker\n- src/search/fastembed_reranker.rs → frankensearch reranker impl\n- src/search/embedder_registry.rs → frankensearch::EmbedderRegistry\n- src/search/reranker_registry.rs → frankensearch reranker registry\n- src/search/model_download.rs → frankensearch model download\n- src/search/daemon_client.rs → keep (cass-specific daemon protocol)\n- src/search/query.rs → adapt to use frankensearch types\n\nNote: cass has a daemon_client.rs that forwards embedding requests to a background daemon process. This is cass-specific and should NOT be in frankensearch. Instead, cass should implement frankensearch::Embedder with a DaemonEmbedder that wraps the daemon protocol.\n\nBinary format: cass uses CVVI magic with 70-byte domain-specific rows. These domain fields (MessageID, AgentID, WorkspaceID, Role, ChunkIdx) don't belong in frankensearch. The migration should store these in a separate metadata index, with frankensearch only owning the vector data.\n\nMigration strategy:\n1. Add frankensearch dependency with features = ['full']\n2. Create adapter types for cass-specific needs (DaemonEmbedder, MetadataIndex)\n3. Replace search modules one at a time\n4. Verify cass_bakeoff_validation.sh passes (NDCG@10 >= 0.25)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:54.384496776Z","created_by":"ubuntu","updated_at":"2026-02-13T20:13:47.869343256Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cass","migration","phase13"],"dependencies":[{"issue_id":"bd-3un.36","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:54.384496776Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.36","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T17:55:49.835226386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.36","depends_on_id":"bd-3un.40","type":"blocks","created_at":"2026-02-13T20:13:47.869309673Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":9,"issue_id":"bd-3un.36","author":"Dicklesworthstone","text":"CASS MIGRATION NOTES: cass is the most complex migration because it has:\n1. A daemon_client.rs that forwards embedding requests to a hot background process\n2. Domain-specific 70-byte CVVI rows with MessageID, AgentID, WorkspaceID, Role, ChunkIdx\n3. Tight coupling between search and the session data model\n\nStrategy: cass should implement frankensearch::Embedder with a DaemonEmbedder wrapper that uses the existing daemon protocol. The daemon itself can use frankensearch internally for the actual embedding computation.\n\nFor the CVVI row metadata: create a cass-specific MetadataIndex that maps doc_id → (MessageID, AgentID, etc.). The frankensearch VectorIndex only stores (doc_id, embedding). This separation is cleaner and lets frankensearch stay domain-agnostic.","created_at":"2026-02-13T17:57:22Z"}]}
{"id":"bd-3un.37","title":"Migrate mcp_agent_mail_rust to use frankensearch crate","description":"Replace mcp_agent_mail_rust search implementation with frankensearch. Agent-mail was the most recent adopter and its search-core crate already has the cleanest separation.\n\nFiles to replace in /data/projects/mcp_agent_mail_rust:\n- crates/mcp-agent-mail-search-core/src/two_tier.rs → frankensearch::TwoTierSearcher\n- crates/mcp-agent-mail-search-core/src/embedder.rs → frankensearch::Embedder\n- crates/mcp-agent-mail-search-core/src/model2vec.rs → frankensearch::Model2VecEmbedder\n- crates/mcp-agent-mail-search-core/src/fastembed.rs → frankensearch::FastEmbedEmbedder\n- crates/mcp-agent-mail-search-core/src/auto_init.rs → frankensearch::EmbedderStack\n- crates/mcp-agent-mail-search-core/src/vector_index.rs → frankensearch::VectorIndex\n- crates/mcp-agent-mail-search-core/src/fusion.rs → frankensearch::{rrf_fuse, RrfConfig}\n- crates/mcp-agent-mail-search-core/src/embedding_jobs.rs → frankensearch background queue\n\nKeep agent-mail-specific:\n- crates/mcp-agent-mail-search-core/src/hybrid_candidates.rs (domain-specific budget logic)\n- crates/mcp-agent-mail-search-core/src/lexical_parser.rs (agent-mail query syntax)\n- crates/mcp-agent-mail-db/src/search_planner.rs (domain-specific query planning)\n- crates/mcp-agent-mail-db/src/embeddings.rs (SQLite persistence)\n\nThe mcp-agent-mail-search-core crate should become a thin wrapper around frankensearch with agent-mail-specific query parsing and persistence.\n\nMigration strategy:\n1. Add frankensearch = { path = '../../frankensearch', features = ['hybrid'] }\n2. Replace types and implementations incrementally\n3. Run search V3 quality gates (SPEC-search-v3-quality-gates.md)\n4. Verify TUI search still works with progressive display","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:54:54.462089901Z","created_by":"ubuntu","updated_at":"2026-02-13T20:13:47.954306908Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["agent-mail","migration","phase13"],"dependencies":[{"issue_id":"bd-3un.37","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:54:54.462089901Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.37","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T17:55:49.917870995Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.37","depends_on_id":"bd-3un.40","type":"blocks","created_at":"2026-02-13T20:13:47.954171906Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":10,"issue_id":"bd-3un.37","author":"Dicklesworthstone","text":"AGENT-MAIL MIGRATION NOTES: agent-mail is the most recent adopter and already has the cleanest separation (its search-core crate). The migration should:\n1. Replace mcp-agent-mail-search-core internals with frankensearch\n2. Keep mcp-agent-mail-db as the persistence layer (SQLite embeddings table)\n3. Keep domain-specific query planning in search_planner.rs\n\nagent-mail's feature-gated approach (semantic, hybrid features) maps directly to frankensearch's feature flags. The workspace Cargo.toml can forward features:\n  frankensearch = { version = '0.1', features = ['hybrid'] }\n\nThe TUI progressive display already consumes SearchPhase — it should work identically with frankensearch's SearchPhase enum.","created_at":"2026-02-13T17:57:22Z"}]}
{"id":"bd-3un.38","title":"Create test fixture corpus and ground truth relevance data","description":"Create a synthetic test fixture corpus for use by all test beads. This corpus must be realistic enough to validate search quality without requiring ML model downloads.\n\nCorpus spec (stored in tests/fixtures/):\n- 100 synthetic documents across 5 topic clusters:\n  1. Programming/Rust (20 docs): ownership, borrowing, async, traits, error handling\n  2. Machine Learning (20 docs): embeddings, transformers, tokenization, ONNX, fine-tuning\n  3. System Admin (20 docs): Docker, Kubernetes, networking, monitoring, deployment\n  4. Cooking/Recipes (20 docs): completely unrelated domain for negative testing\n  5. Mixed/Overlap (20 docs): documents spanning 2+ topics for fusion testing\n\nEach document has:\n- doc_id: deterministic format \"test-{cluster}-{n:03}\" (e.g., \"test-rust-007\")\n- content: 50-200 words of realistic text (NOT lorem ipsum)\n- title: short descriptive title\n- created_at: staggered timestamps across a 30-day range\n- doc_type: cluster name\n- metadata: JSON with word_count, reading_level, language\n\nGround truth relevance file (tests/fixtures/relevance.json):\n- For 20 predefined test queries, the expected top-10 relevant doc_ids\n- This enables NDCG@10 computation for regression testing\n- Queries span exact match, semantic similarity, cross-topic, and negative cases\n\nExample ground truth entries:\n  \"rust ownership\" -> [test-rust-001, test-rust-003, test-rust-012, ...]\n  \"how to deploy containers\" -> [test-sysadmin-005, test-sysadmin-011, ...]\n  \"chocolate chip cookies\" -> [test-cooking-002, test-cooking-015, ...] (NO programming docs)\n\nAlso include:\n- tests/fixtures/edge_cases.json: Empty strings, unicode, very long text, single word, special chars\n- tests/fixtures/README.md: Documenting the corpus structure and how to regenerate\n\nThe corpus should be committed to the repo (small enough) so tests are reproducible without any setup.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:11:37.574321889Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:53.362373398Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fixtures","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.38","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:11:37.574321889Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.38","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T20:11:41.570461740Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":49,"issue_id":"bd-3un.38","author":"Dicklesworthstone","text":"REVISION: Test Fixture Corpus Enhancement\n\nThe current spec covers 100 docs across 5 topic clusters with 20 ground truth queries. This is a good foundation but needs the following additions for production-quality testing:\n\n1. Adversarial Inputs (add 20 docs to corpus, total 120):\n   - Empty string document (doc_id: \"adversarial_empty\")\n   - Single-character document: \"x\"\n   - Very long document: 50,000 chars of repeated text (tests truncation)\n   - Unicode stress test: CJK characters, RTL Arabic, emoji sequences, combining diacriticals\n   - Mixed-script: \"Hello\" in 10 languages within one document\n   - Code-heavy: 95% code block with minimal natural language\n   - Low-signal: document of only stopwords (\"the a an is are was were\")\n   - HTML injection attempt: \"<script>alert('xss')</script>\"\n   - SQL-like content: \"DROP TABLE documents; SELECT * FROM users\"\n   - Null bytes and control characters in content\n   - Document with identical content but different doc_id (tests dedup)\n   - Document with very long doc_id (1000 chars)\n\n2. Multilingual Samples (potion-128M is \"multilingual\"):\n   - 5 docs in German, 5 in French (within the 20 additional docs)\n   - Ground truth queries in non-English: test that potion handles multilingual correctly\n   - Cross-lingual query: English query should find relevant German doc\n\n3. Canonicalization Edge Cases:\n   - Document with nested markdown: headers, lists, code blocks, tables\n   - Document with only a code block (no natural language)\n   - Document with excessive whitespace and newlines\n   - Document with markdown frontmatter (YAML header)\n\n4. Pre-computed Hash Embeddings:\n   - Store golden hash embeddings for 10 key documents\n   - Regression test: hash_embedder(doc) == stored_golden_embedding\n   - This catches accidental changes to the hash function\n\n5. Ground Truth Enhancement:\n   - Add 5 adversarial queries to the existing 20:\n     * Empty query: \"\" (should return empty results)\n     * Single char: \"x\" (edge case)\n     * Very long query: 500 words (tests truncation)\n     * Query matching no documents (should return empty, not error)\n     * Boolean query: \"rust AND NOT cooking\"\n   - Total: 25 ground truth queries with expected top-10 doc_ids\n\n6. Fixture Format:\n   - fixtures/corpus.json: all documents\n   - fixtures/queries.json: all ground truth queries with relevance judgments\n   - fixtures/golden_embeddings.json: pre-computed hash embeddings\n   - fixtures/edge_cases.json: adversarial inputs with expected behaviors\n   - fixtures/README.md: document the fixture design decisions\n","created_at":"2026-02-13T20:44:53Z"}]}
{"id":"bd-3un.39","title":"Add structured tracing/logging throughout pipeline","description":"Add structured tracing and logging throughout the entire frankensearch pipeline using the tracing crate. This is critical for debugging, performance monitoring, and the detailed logging the e2e test scripts need.\n\nTracing configuration:\n\n1. Crate-level subscriber setup (optional, consumer can bring their own):\n   pub fn init_default_tracing(level: tracing::Level) {\n       tracing_subscriber::fmt()\n           .with_env_filter(EnvFilter::from_default_env()\n               .add_directive(format!(\"frankensearch={}\", level).parse().unwrap()))\n           .with_timer(tracing_subscriber::fmt::time::uptime())\n           .with_target(true)\n           .with_thread_ids(true)\n           .init();\n   }\n\n2. Instrument key operations with spans:\n\n   Embedding:\n   #[tracing::instrument(skip(text), fields(model = %self.id(), dim = self.dimension(), text_len = text.len()))]\n   fn embed(&self, text: &str) -> SearchResult<Vec<f32>>\n\n   Vector search:\n   #[tracing::instrument(skip(index, query), fields(index_size = index.record_count(), dim = index.dimension(), k))]\n   fn search_top_k(...)\n\n   RRF fusion:\n   #[tracing::instrument(skip(lexical, semantic), fields(lexical_count = lexical.len(), semantic_count = semantic.len(), k = config.k))]\n   fn rrf_fuse(...)\n\n   Two-tier search (the big one):\n   #[tracing::instrument(skip(self), fields(query_len = query.len(), k, mode))]\n   fn search(&self, query: &str, k: usize) -> TwoTierSearchIter\n\n3. Key events to log:\n\n   INFO level:\n   - \"index_opened\" { path, record_count, dimension, embedder_id, format_version }\n   - \"model_loaded\" { model_name, dimension, load_time_ms, category }\n   - \"search_completed\" { phase, result_count, latency_ms, query_len }\n   - \"index_rebuilt\" { old_count, new_count, rebuild_time_ms }\n\n   DEBUG level:\n   - \"query_embedded\" { model, dimension, latency_us }\n   - \"vector_search_done\" { candidates_scanned, results_returned, latency_ms }\n   - \"rrf_fused\" { lexical_candidates, semantic_candidates, fused_count, overlap_count }\n   - \"scores_blended\" { fast_count, quality_count, blend_factor, rank_correlation }\n   - \"rerank_done\" { model, candidates_in, latency_ms, score_range }\n\n   WARN level:\n   - \"quality_model_unavailable\" { reason, fallback }\n   - \"refinement_timeout\" { budget_ms, elapsed_ms }\n   - \"backpressure_triggered\" { queue_depth, threshold }\n\n   TRACE level (very hot path, disabled by default):\n   - Individual dot product scores\n   - Per-record filter decisions\n   - Token-level embedding details\n\n4. Performance timing via tracing spans:\n   Every span automatically gets duration. In tests, use tracing-test crate to capture and assert on logged events.\n\nDependencies:\n- tracing = \"0.1\"\n- tracing-subscriber = \"0.3\" (with fmt, env-filter features)\n- tracing-test = \"0.2\" (dev-dependency, for test assertions on logs)\n\nFile: frankensearch-core/src/tracing_config.rs (optional helpers)\nIntegration: spans and events go in each component's source file\n\nThis bead is about DEFINING the logging schema and ensuring every component instruments properly. The actual log lines go in the component code (per the epic's LOGGING POLICY comment).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:12:04.368694010Z","created_by":"ubuntu","updated_at":"2026-02-13T20:47:17.567446568Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["logging","phase10","quality","tracing"],"dependencies":[{"issue_id":"bd-3un.39","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:12:04.368694010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.39","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T20:12:08.162501866Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":79,"issue_id":"bd-3un.39","author":"Dicklesworthstone","text":"REVISION: Structured Tracing Instrumentation Plan\n\nSystematic tracing is essential for debugging the 2-tier pipeline. Key requirements:\n\n1. Span Hierarchy (parent-child relationship):\n   frankensearch::search (root span per query)\n   +-- phase0::fast_embed (fast-tier embedding)\n   +-- phase0::fast_search (brute-force vector search)\n   +-- phase0::lexical_search (Tantivy BM25, if hybrid)\n   +-- phase0::rrf_fuse (RRF fusion)\n   +-- phase1::quality_embed (quality-tier embedding)\n   +-- phase1::blend (fast+quality score blending)\n   +-- phase1::rerank (cross-encoder reranking, if enabled)\n   +-- phase1::final_fuse (re-fusion with lexical)\n\n2. Mandatory Fields per Span:\n   - query_len: usize (chars, not bytes)\n   - query_class: &str (\"empty\", \"identifier\", \"short_keyword\", \"natural_language\")\n   - phase: &str (\"initial\", \"refined\", \"refinement_failed\")\n   - duration_us: u64 (microseconds)\n\n3. Key Events (structured fields, not string interpolation):\n   INFO events:\n   - index_opened { path, doc_count, fast_dim, quality_dim, format_version }\n   - model_loaded { model_id, dimension, load_time_ms, memory_bytes }\n   - search_completed { query_len, phase, result_count, total_ms, fast_ms, quality_ms }\n   - index_rebuilt { doc_count, duration_ms, fast_size_bytes, quality_size_bytes }\n\n   DEBUG events:\n   - query_embedded { model_id, dimension, duration_us, query_class }\n   - vector_search_done { candidates, top_k, duration_us }\n   - rrf_fused { lexical_count, semantic_count, fused_count, duration_us }\n   - scores_blended { blend_factor, kendall_tau, promoted, demoted, stable }\n\n   WARN events:\n   - quality_model_unavailable { reason, fallback }\n   - refinement_timeout { timeout_ms, phase }\n   - index_stale { reason, last_rebuild, doc_count_mismatch }\n\n   ERROR events:\n   - embedding_failed { model_id, error, query_len }\n   - index_corrupted { path, expected_crc, actual_crc }\n\n4. Subscriber Setup Helper:\n   pub fn init_tracing(level: tracing::Level) -> tracing::subscriber::DefaultGuard\n   Sets up fmt subscriber with:\n   - JSON output when FRANKENSEARCH_LOG_FORMAT=json (for log aggregation)\n   - Pretty output otherwise (human-readable with colors)\n   - Timer with microsecond precision\n   - Thread name in each event\n   - Span events on close (with duration)\n\n5. Performance Budget:\n   Tracing overhead MUST be < 1% of total search time.\n   Use tracing's compile-time filtering (max_level_info for release builds).\n   DEBUG and TRACE events compiled out in release mode.\n","created_at":"2026-02-13T20:47:17Z"}]}
{"id":"bd-3un.4","title":"Define Reranker trait and reranking types","description":"Define the Reranker trait in frankensearch-core. Cross-encoder rerankers are used as a second pass after initial retrieval to improve relevance ordering. Must be object-safe.\n\npub trait Reranker: Send + Sync {\n    /// Score query-document pairs. Returns relevance scores (higher = more relevant).\n    /// The returned Vec must have the same length as documents.\n    fn rerank(&self, query: &str, documents: &[&str]) -> SearchResult<Vec<f32>>;\n    \n    /// Model name identifier.\n    fn model_name(&self) -> &str;\n    \n    /// Maximum input sequence length in tokens.\n    fn max_length(&self) -> usize;\n    \n    /// Whether this reranker is currently available (model loaded).\n    fn is_available(&self) -> bool;\n}\n\nThis is distinct from bi-encoders (Embedder trait) because cross-encoders attend to query+document simultaneously, producing a relevance score rather than a vector.\n\nReference implementations:\n- cass: src/search/reranker.rs (217 lines) - trait with RerankerError variants\n- xf: src/reranker.rs - same trait, slightly different error handling\n- agent-mail: not yet integrated (planned)\n\nThe trait goes in frankensearch-core, implementations in frankensearch-rerank.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:39.952093270Z","created_by":"ubuntu","updated_at":"2026-02-13T17:55:00.678155961Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1","trait"],"dependencies":[{"issue_id":"bd-3un.4","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:39.952093270Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.4","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:00.678125073Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.40","title":"Write e2e validation scripts with detailed logging","description":"Write standalone e2e validation scripts that exercise the full frankensearch pipeline with detailed, colorized logging output. These are NOT cargo tests -- they are runnable scripts/binaries that a developer can execute to validate everything works correctly and see exactly what is happening at each stage.\n\nScripts to create:\n\n1. tests/e2e/validate_full_pipeline.rs (binary target in Cargo.toml)\n   - Requires: hash feature only (no ML model downloads)\n   - Steps:\n     a. Create temp directory\n     b. Load test fixture corpus (from tests/fixtures/)\n     c. Initialize hash embedder\n     d. Build vector index (FSVI format) with progress bar\n     e. Build Tantivy lexical index with progress bar\n     f. Execute 20 predefined queries from ground truth\n     g. For each query:\n        - Log: query text, expected results\n        - Phase 1 (Initial): log latency, top-5 results with scores\n        - Phase 2 (Refined): log latency, top-5 results with scores, rank changes\n        - Compute NDCG@10 against ground truth\n        - Log: pass/fail with detailed explanation if failed\n     h. Summary: queries tested, pass rate, avg latency per phase, overall NDCG\n   - Exit code: 0 if all pass, 1 if any fail\n\n2. tests/e2e/validate_index_io.rs (binary target)\n   - Tests vector index round-trip integrity\n   - Steps:\n     a. Create vectors of known values\n     b. Write FSVI index\n     c. Log: file size, records written, format details\n     d. Reopen index via mmap\n     e. Read back every vector, compare with originals\n     f. Log: max error per vector (f16 quantization loss)\n     g. Verify header CRC32\n     h. Test corruption detection (flip a byte, verify error)\n   - Exit code: 0 if all checks pass\n\n3. tests/e2e/validate_fusion.rs (binary target)\n   - Tests RRF fusion and score blending correctness\n   - Steps:\n     a. Create synthetic lexical and semantic result sets with known rankings\n     b. Run RRF fusion, verify output order matches expected\n     c. Run score blending with various blend factors\n     d. Log: input rankings, fusion scores, output rankings, expected vs actual\n     e. Test edge cases: empty inputs, single source, all-overlap, zero-overlap\n\n4. tests/e2e/bench_quick.rs (binary target)\n   - Quick performance smoke test (< 30 seconds)\n   - Steps:\n     a. Generate 10K random vectors (384-dim)\n     b. Write to FSVI index\n     c. Run 100 searches, measure latency distribution\n     d. Log: p50, p90, p99 latency, throughput (queries/sec)\n     e. Assert p99 < 50ms for 10K vectors\n     f. Run SIMD dot product microbenchmark\n     g. Log: throughput in GFLOP/s\n\nLogging format for all scripts:\n  [TIMESTAMP] [LEVEL] [STEP] message\n  [2026-01-15T10:30:00Z] [INFO] [INDEX] Built vector index: 100 records, 384 dims, 76.8 KB\n  [2026-01-15T10:30:00Z] [DEBUG] [SEARCH] Query \"rust ownership\": Phase::Initial 12.3ms, 10 results\n  [2026-01-15T10:30:00Z] [PASS] [QUERY] \"rust ownership\" NDCG@10=0.89 (threshold: 0.25)\n\nUse colored output (via colored crate or owo-colors):\n- GREEN: pass, success\n- RED: fail, error\n- YELLOW: warning, degraded\n- CYAN: info, timing\n- DIM: debug details\n\nEach script should be runnable with:\n  cargo run --example validate_full_pipeline\n  cargo run --example validate_index_io\n  cargo run --example validate_fusion\n  cargo run --example bench_quick\n\nAnd also accessible via a master script:\n  ./tests/e2e/run_all.sh (runs all 4, reports summary)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:12:32.915337598Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:54.466506217Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","logging","phase11","testing"],"dependencies":[{"issue_id":"bd-3un.40","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:12:32.915337598Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.40","depends_on_id":"bd-3un.32","type":"blocks","created_at":"2026-02-13T20:12:37.666997010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.40","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:12:37.750726993Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.40","depends_on_id":"bd-3un.39","type":"blocks","created_at":"2026-02-13T20:12:37.835096814Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":50,"issue_id":"bd-3un.40","author":"Dicklesworthstone","text":"REVISION: E2E Validation Scripts Enhancement\n\nThe current 4 scripts are solid. Add these critical test scenarios:\n\n1. Degradation / Fallback Test (add to validate_full_pipeline.rs):\n   - Test A: Quality model unavailable -> system falls back to fast-only mode\n   - Test B: Fast model unavailable -> system falls back to hash-only mode\n   - Test C: Both ML models unavailable -> system uses hash embedder\n   - Test D: Lexical index corrupted -> semantic-only search works\n   - Test E: Vector index corrupted -> lexical-only search works\n   - Each test: verify graceful degradation (no panic, correct SearchPhase)\n   - Colorized output: YELLOW for expected degradation, RED for unexpected failure\n\n2. Concurrent Access Test (new script: validate_concurrency.rs):\n   - Spawn 4 reader threads + 1 writer thread\n   - Readers: continuous search queries against existing index\n   - Writer: rebuilds index with new documents\n   - Assert: readers never see corrupted results (no partial writes)\n   - Assert: readers may see stale results (eventual consistency OK)\n   - Assert: no panics, no deadlocks (timeout after 10s)\n   - This validates the OnceLock + atomic rename concurrency model\n\n3. Memory Baseline Test (add to bench_quick.rs):\n   - Measure RSS before and after full pipeline run\n   - Track peak RSS during search (via /proc/self/status on Linux)\n   - Budget: RSS delta < 50MB for 10K doc index\n   - Log: \"memory_baseline rss_before={mb} rss_peak={mb} rss_after={mb} delta={mb}\"\n   - YELLOW if delta > 30MB, RED if delta > 50MB\n\n4. Progressive Iterator Contract Test (add to validate_full_pipeline.rs):\n   - Verify iterator yields exactly 2 phases when quality model available\n   - Verify iterator yields exactly 1 phase (Initial only) when quality unavailable\n   - Verify SearchPhase::Refined results are >= quality of SearchPhase::Initial\n   - Verify SearchPhase::RefinementFailed carries the Initial results unchanged\n   - Time each phase: Initial < 20ms, Refined < 300ms (with hash embedders in test)\n\n5. Logging Verification:\n   - Each script captures tracing output to a StringWriter\n   - At end: verify key tracing events were emitted (model_loaded, search_completed, etc.)\n   - Missing events: YELLOW warning (tracing coverage regression)\n\n6. Master Script Enhancement (run_all.sh):\n   - Run all 5 scripts (original 4 + concurrency test)\n   - Summary table at end with pass/fail/warn counts per script\n   - Exit code: 0 if all pass, 1 if any fail, 2 if any warn\n   - --verbose flag for full output, default is summary only\n   - --quick flag to skip concurrency and memory tests (for CI)\n","created_at":"2026-02-13T20:44:54Z"}]}
{"id":"bd-3un.41","title":"Implement index staleness detection and cache management","description":"Implement index staleness detection and cache management for the TwoTierIndex. When source data changes but the index hasn't been rebuilt, the system should detect this and optionally trigger a rebuild.\n\nStaleness detection (from xf VectorIndexCache pattern):\n\npub struct IndexStaleness {\n    pub is_stale: bool,\n    pub index_modified: SystemTime,\n    pub newest_source: Option<SystemTime>,\n    pub index_record_count: usize,\n    pub estimated_source_count: Option<usize>,\n    pub reason: Option<String>,\n}\n\nDetection strategies:\n1. Timestamp comparison: compare index file mtime with source data directory mtime\n2. Count mismatch: compare record_count in index header with count of source documents (if provided by caller)\n3. Sentinel file: write a .index_built_at sentinel with build timestamp + source hash\n4. Manual invalidation: caller can explicitly mark index as stale\n\npub struct IndexCache {\n    fast: OnceLock<Option<VectorIndex>>,\n    quality: OnceLock<Option<VectorIndex>>,\n    lexical: OnceLock<Option<LexicalIndex>>,\n    data_dir: PathBuf,\n}\n\nimpl IndexCache {\n    pub fn new(data_dir: PathBuf) -> Self;\n    \n    /// Get or lazily load the fast-tier index\n    pub fn fast_index(&self) -> Option<&VectorIndex>;\n    \n    /// Get or lazily load the quality-tier index\n    pub fn quality_index(&self) -> Option<&VectorIndex>;\n    \n    /// Get or lazily load the lexical index\n    pub fn lexical_index(&self) -> Option<&LexicalIndex>;\n    \n    /// Check if any index is stale\n    pub fn check_staleness(&self) -> IndexStaleness;\n    \n    /// Invalidate cache (next access will reload from disk)\n    /// Note: OnceLock can't be reset, so this creates a new cache\n    pub fn invalidate(self) -> Self;\n    \n    /// Get the two-tier index (combines fast + quality)\n    pub fn two_tier_index(&self, config: &TwoTierConfig) -> Option<TwoTierIndex>;\n}\n\nSentinel file format (.frankensearch_index_meta):\n{\n    \"built_at\": \"2026-01-15T10:30:00Z\",\n    \"source_count\": 1234,\n    \"source_hash\": \"sha256:abc123...\",   // optional, hash of source file list\n    \"fast_embedder\": \"potion-multilingual-128M\",\n    \"quality_embedder\": \"all-MiniLM-L6-v2\",\n    \"fast_dimension\": 256,\n    \"quality_dimension\": 384,\n    \"format_version\": 1\n}\n\nAuto-rebuild policy (configurable):\n- AutoRebuild::Never: just report staleness\n- AutoRebuild::Prompt: log warning, let caller decide\n- AutoRebuild::Auto: rebuild in background when stale detected\n\nLogging:\n- INFO: \"index_cache_hit\" { fast: true, quality: true, lexical: true }\n- WARN: \"index_stale\" { reason, index_age_secs, source_count_mismatch }\n- INFO: \"index_rebuild_triggered\" { strategy, estimated_docs }\n\nReference:\n- xf: src/main.rs VectorIndexCache with OnceLock (lines 50-150)\n- cass: src/search/vector_index.rs (index staleness checks)\n\nFile: frankensearch-fusion/src/cache.rs","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:12:58.215966879Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:52.817641703Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cache","phase7","staleness"],"dependencies":[{"issue_id":"bd-3un.41","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:12:58.215966879Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.41","depends_on_id":"bd-3un.23","type":"blocks","created_at":"2026-02-13T20:13:02.312195181Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":30,"issue_id":"bd-3un.41","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Index Staleness Detection)\n\n## Mathematical Upgrade: From Timestamp-Based to Information-Theoretic Staleness\n\nCurrent design uses timestamp/count comparison for staleness. This is crude — an index can be \"fresh\" by timestamp but stale by content (if the distribution of new documents differs from indexed documents), or \"stale\" by timestamp but still perfectly representative.\n\n### 1. KL Divergence Staleness Score\n\nMaintain a term frequency distribution for the indexed corpus (computed at build time) and compare against a running term frequency distribution of incoming documents:\n\n  staleness_score = D_KL(P_new || P_indexed)\n  = Σᵢ P_new(termᵢ) × log(P_new(termᵢ) / P_indexed(termᵢ))\n\nWhen KL divergence exceeds a threshold (e.g., 0.5 nats), the index is \"stale\" in a formally meaningful sense — the distribution of content has shifted enough that the index no longer represents it well.\n\nImplementation: Maintain a Count-Min Sketch (O(k) space, O(1) update) for both indexed and new term distributions. Compute KL divergence periodically (every 100 new documents).\n\n### 2. CUSUM Change-Point Detection\n\nUse the CUSUM (Cumulative Sum Control Chart) algorithm to detect when search quality degrades:\n\n  S_n = max(0, S_{n-1} + (x_n - μ₀ - k))\n\nWhere x_n is the search quality metric (e.g., mean reciprocal rank from click data), μ₀ is the expected quality, and k is the allowable slack. When S_n exceeds threshold h, a change point is detected → trigger rebuild.\n\nCUSUM is formally optimal for detecting mean shifts (Lorden 1971) and requires O(1) state.\n\n### 3. Survival Analysis for Index Lifetime\n\nModel index lifetime as a Weibull distribution:\n\n  h(t) = (k/λ)(t/λ)^{k-1}  // hazard rate\n  S(t) = exp(-(t/λ)^k)       // survival function\n\nFit k and λ from historical rebuild intervals. This gives:\n- P(index still good at time t): S(t)\n- Expected time until rebuild needed: λ × Γ(1 + 1/k)\n- Optimal rebuild schedule: minimize expected cost of staleness + rebuild\n\n### 4. Practical Implementation\n\nCombine all three into a single staleness score:\n\n  pub struct StalenessDetector {\n      // Lightweight (< 1KB total state)\n      term_sketch_indexed: CountMinSketch,  // Frozen at build time\n      term_sketch_new: CountMinSketch,      // Updated with each new document\n      cusum_state: f32,                     // Running CUSUM statistic\n      docs_since_build: u64,\n      build_timestamp: SystemTime,\n  }\n\n  impl StalenessDetector {\n      pub fn staleness_score(&self) -> StalenessReport {\n          StalenessReport {\n              kl_divergence: self.compute_kl(),\n              cusum_alarm: self.cusum_state > CUSUM_THRESHOLD,\n              docs_since_build: self.docs_since_build,\n              estimated_quality_loss: self.estimated_quality_loss(),\n              rebuild_recommended: self.kl_divergence > 0.5 || self.cusum_alarm,\n          }\n      }\n  }\n\nThis is the alien-artifact version: formally principled staleness detection with provable properties, but operationally simple (O(1) per document, O(k) space).\n","created_at":"2026-02-13T20:29:52Z"}]}
{"id":"bd-3un.42","title":"Implement text canonicalization pipeline","description":"Implement a text canonicalization/preprocessing pipeline in frankensearch-core that normalizes text before embedding. This is CRITICAL for search quality -- without proper preprocessing, embeddings are noisy and search results degrade.\n\nAll three source codebases have this:\n- cass: canonicalize.rs (1,039 lines) with streaming implementation\n- agent-mail: CanonPolicy (Full, TitleOnly) with embed_document() helper\n- xf: simpler preprocessing (queries are naturally short)\n\nDesign: Trait-based with a default implementation, customizable per consumer.\n\npub trait Canonicalizer: Send + Sync {\n    fn canonicalize(&self, text: &str) -> String;\n    fn canonicalize_query(&self, query: &str) -> String {\n        // Query canonicalization is simpler (no markdown stripping, no code collapsing)\n        self.canonicalize(query)\n    }\n}\n\nDefault implementation pipeline (from cass canonicalize.rs):\npub struct DefaultCanonicalizer {\n    pub max_chars: usize,          // Default: 2000 (MAX_EMBED_CHARS from cass)\n    pub strip_markdown: bool,      // Default: true\n    pub collapse_code_blocks: bool,// Default: true\n    pub code_head_lines: usize,    // Default: 20\n    pub code_tail_lines: usize,    // Default: 10\n    pub filter_low_signal: bool,   // Default: true\n    pub normalize_unicode: bool,   // Default: true (NFC)\n}\n\nProcessing steps (in order):\n1. Unicode NFC normalization (via unicode-normalization crate)\n   - Ensures hash stability: different Unicode representations → same bytes\n   - \"café\" (e + combining acute) → \"café\" (single precomposed char)\n\n2. Markdown stripping (pure text extraction)\n   - Remove headers (#, ##), bold/italic (**,*,_), links [text](url)→text\n   - Keep text content, remove formatting syntax\n   - Configurable (some consumers may want to preserve structure)\n\n3. Code block collapsing\n   - For code blocks > (head + tail) lines:\n     Keep first `code_head_lines` (20) + last `code_tail_lines` (10)\n     Replace middle with \"... [N lines elided] ...\"\n   - Prevents long code from dominating embedding signal\n\n4. Whitespace normalization\n   - Collapse multiple spaces/tabs/newlines to single space\n   - Trim leading/trailing whitespace\n\n5. Low-signal content filtering\n   - Filter out very short, meaningless responses\n   - Configurable list: [\"ok\", \"done\", \"got it\", \"understood\", \"sure\", \"yes\", \"no\", \"thanks\", \"thank you\"]\n   - Only filter if entire text matches (not substrings)\n   - Returns empty string for filtered content (caller decides to skip)\n\n6. Truncation to max_chars\n   - Truncate at word boundary if possible\n   - Never split mid-word\n\nAlso provide:\npub fn content_hash(text: &str) -> String {\n    // SHA-256 hex digest of the canonicalized text\n    // Used for dedup and change detection throughout the pipeline\n    use sha2::{Sha256, Digest};\n    hex::encode(Sha256::digest(text.as_bytes()))\n}\n\nQuery-specific canonicalization (simpler):\n1. Unicode NFC\n2. Whitespace normalization\n3. Truncation (shorter limit, e.g., 500 chars)\n(No markdown stripping, no code collapsing, no low-signal filtering)\n\nFile: frankensearch-core/src/canonicalize.rs\n\nDependencies:\n- unicode-normalization (for NFC) - lightweight, no transitive deps\n- sha2 + hex (for content hashing) - already used for model verification\n\nReference: cass src/search/canonicalize.rs (1,039 lines with streaming impl)\nReference: agent-mail embed_document() helper in embedder.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:23:11.362478088Z","created_by":"ubuntu","updated_at":"2026-02-13T20:23:15.507642526Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","phase1","preprocessing"],"dependencies":[{"issue_id":"bd-3un.42","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:23:11.362478088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.42","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:23:15.507609794Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.43","title":"Implement query classification and candidate budgeting","description":"Implement query classification and adaptive candidate budgeting for hybrid search. Different types of queries benefit from different retrieval strategies -- technical identifiers should lean lexical, natural language queries should lean semantic.\n\nFrom agent-mail hybrid_candidates.rs (1,234 lines):\n\nQuery Classification:\n\npub enum QueryClass {\n    /// Empty or whitespace-only query\n    Empty,\n    /// Technical identifier: \"br-123\", \"thread:abc\", mixed alpha+digits\n    Identifier,\n    /// Short keyword phrase: 1-2 short tokens\n    ShortKeyword,\n    /// Natural language: 3+ tokens or long average token length\n    NaturalLanguage,\n}\n\nimpl QueryClass {\n    pub fn classify(query: &str) -> Self {\n        let tokens: Vec<&str> = query.split_whitespace().collect();\n        if tokens.is_empty() { return Self::Empty; }\n        \n        // Identifier heuristics (from agent-mail):\n        // - Starts with known prefix (\"br-\", \"thread:\", \"bd-\")\n        // - Contains underscores or slashes\n        // - Mixed alpha+digit tokens\n        // - All-hyphenated tokens\n        let looks_like_id = tokens.iter().any(|t| {\n            t.contains('_') || t.contains('/') ||\n            (t.contains('-') && t.chars().any(|c| c.is_ascii_digit())) ||\n            (t.chars().any(|c| c.is_ascii_alphabetic()) && t.chars().any(|c| c.is_ascii_digit()))\n        });\n        if looks_like_id { return Self::Identifier; }\n        \n        let avg_len = tokens.iter().map(|t| t.len()).sum::<usize>() / tokens.len();\n        if tokens.len() <= 2 && avg_len <= 10 { return Self::ShortKeyword; }\n        \n        Self::NaturalLanguage\n    }\n}\n\nCandidate Budget System:\n\npub struct CandidateConfig {\n    /// Base multiplier for lexical candidates (default: 3x)\n    pub lexical_multiplier: f32,\n    /// Base multiplier for semantic candidates (default: 3x)\n    pub semantic_multiplier: f32,\n    /// Minimum candidates per source (default: 20)\n    pub min_per_source: usize,\n    /// Maximum candidates per source (default: 1000)\n    pub max_per_source: usize,\n    /// Maximum total candidates (default: 2000)\n    pub max_combined: usize,\n}\n\nQuery-class adjustments (from agent-mail):\n- Identifier: lex_mult *= 1.5, sem_mult *= 0.5 (lean lexical for exact matches)\n- ShortKeyword: lex_mult *= 1.25, sem_mult *= 0.75 (slight lexical preference)\n- NaturalLanguage: lex_mult *= 0.9, sem_mult *= 1.35 (lean semantic for meaning)\n- Empty: lex_mult *= 1.0, sem_mult = 0.0 (lexical only, no semantic)\n\npub struct CandidateBudget {\n    pub lexical_count: usize,\n    pub semantic_count: usize,\n    pub combined_limit: usize,\n    pub query_class: QueryClass,\n}\n\nimpl CandidateBudget {\n    pub fn derive(\n        requested_limit: usize,\n        config: &CandidateConfig,\n        query: &str,\n    ) -> Self {\n        let class = QueryClass::classify(query);\n        let (lex_adj, sem_adj) = class.multiplier_adjustments();\n        let lex = ((requested_limit as f32) * config.lexical_multiplier * lex_adj)\n            .ceil() as usize;\n        let sem = ((requested_limit as f32) * config.semantic_multiplier * sem_adj)\n            .ceil() as usize;\n        let lex = lex.clamp(config.min_per_source, config.max_per_source);\n        let sem = sem.clamp(config.min_per_source, config.max_per_source);\n        CandidateBudget {\n            lexical_count: lex,\n            semantic_count: sem,\n            combined_limit: (lex + sem).min(config.max_combined),\n            query_class: class,\n        }\n    }\n}\n\nThis replaces the simple CANDIDATE_MULTIPLIER=3 constant in bd-3un.20 (RRF) with a query-aware system that adapts to what the user is searching for.\n\nFile: frankensearch-fusion/src/candidates.rs\n\nReference:\n- agent-mail: crates/mcp-agent-mail-search-core/src/hybrid_candidates.rs (1,234 lines)\n- cass: query.rs HYBRID_CANDIDATE_MULTIPLIER=3 (simple version)\n- xf: hybrid.rs CANDIDATE_MULTIPLIER=3 (simple version)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:23:42.760610565Z","created_by":"ubuntu","updated_at":"2026-02-13T20:23:47.107017079Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["fusion","phase6","query"],"dependencies":[{"issue_id":"bd-3un.43","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:23:42.760610565Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.43","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:23:47.106974750Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3un.5","title":"Define core result types (ScoredResult, VectorHit, FusedHit)","description":"Define the core result types that flow through the entire search pipeline. These need to be generic enough to work with any document type (tweets, agent sessions, mail messages) while carrying enough scoring metadata for fusion and display.\n\n/// A scored search result from any search phase.\npub struct ScoredResult {\n    /// Opaque document identifier (caller-defined).\n    pub doc_id: String,\n    /// Primary relevance score (normalized 0.0-1.0 after fusion).\n    pub score: f32,\n    /// Optional reranker score (set by rerank step).\n    pub rerank_score: Option<f32>,\n    /// Which sources contributed to this result.\n    pub sources: SourceContribution,\n    /// Arbitrary metadata (caller can attach doc-specific data).\n    pub metadata: Option<serde_json::Value>,\n}\n\n/// A raw hit from vector similarity search.\npub struct VectorHit {\n    /// Index into the vector store.\n    pub index: usize,\n    /// Cosine similarity score (raw, not normalized).\n    pub score: f32,\n    /// Document identifier.\n    pub doc_id: String,\n}\n\n/// A hit from hybrid fusion (lexical + semantic combined).\npub struct FusedHit {\n    /// Document identifier.\n    pub doc_id: String,\n    /// RRF-fused score.\n    pub rrf_score: f64,\n    /// Individual source scores for debugging/display.\n    pub lexical_rank: Option<usize>,\n    pub semantic_rank: Option<usize>,\n    pub lexical_score: Option<f32>,\n    pub semantic_score: Option<f32>,\n}\n\n/// Tracks which retrieval sources contributed to a result.\npub struct SourceContribution {\n    pub lexical: bool,\n    pub semantic_fast: bool,\n    pub semantic_quality: bool,\n    pub reranked: bool,\n}\n\n/// Search mode selector.\npub enum SearchMode {\n    Lexical,    // BM25 keyword matching only\n    Semantic,   // Embedding similarity only\n    Hybrid,     // RRF fusion of lexical + semantic\n    TwoTier,    // Progressive: fast semantic → quality refinement + lexical fusion\n}\n\n/// Progressive search phases for two-tier display.\npub enum SearchPhase {\n    /// Initial results from fast tier (displayed immediately).\n    Initial {\n        results: Vec<ScoredResult>,\n        latency_ms: u64,\n    },\n    /// Refined results after quality tier completes.\n    Refined {\n        results: Vec<ScoredResult>,\n        latency_ms: u64,\n    },\n    /// Quality refinement failed; initial results remain valid.\n    RefinementFailed {\n        error: SearchError,\n    },\n}\n\nAll types should derive: Debug, Clone, Serialize, Deserialize (where appropriate).\n\nReference implementations:\n- cass: src/search/two_tier_search.rs (SearchPhase, ScoredResult)\n- xf: src/hybrid.rs (FusedHit), src/model.rs (SearchResult)\n- agent-mail: crates/mcp-agent-mail-search-core/src/two_tier.rs (SearchPhase)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:47:57.813894307Z","created_by":"ubuntu","updated_at":"2026-02-13T20:45:32.504385139Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","foundation","phase1","types"],"dependencies":[{"issue_id":"bd-3un.5","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:47:57.813894307Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.5","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T17:55:00.758278275Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":54,"issue_id":"bd-3un.5","author":"Dicklesworthstone","text":"REVISION: SearchPhase Data Semantics\n\nSearchPhase enum needs explicit documentation of what data each variant carries:\n\n1. SearchPhase::Initial(Vec<ScoredResult>):\n   - Contains fast-tier results after RRF fusion of lexical + fast semantic\n   - Scores are RRF scores (not raw similarity), range ~0.01-0.03\n   - Results are sorted by RRF score descending, with deterministic tie-breaking\n   - Available within ~15ms of query submission\n\n2. SearchPhase::Refined(Vec<ScoredResult>):\n   - Contains quality-blended results after Phase 1 processing\n   - Scores are blended RRF scores (not raw similarity)\n   - Results may have different ordering than Initial (quality reranking)\n   - ScoredResult.rerank_score is Some(_) if reranker was applied\n   - Available within ~200ms of query submission\n\n3. SearchPhase::RefinementFailed { initial: Vec<ScoredResult>, error: SearchError }:\n   - Carries the ORIGINAL Initial results unchanged (consumer can use them as-is)\n   - error field explains why refinement failed (timeout, model error, etc.)\n   - Consumer should display Initial results and log/display the error\n   - This is NOT an error state -- it's graceful degradation\n\nThe iterator contract:\n- Always yields Initial first\n- Then yields either Refined or RefinementFailed (never both)\n- Iterator is fused after yielding 2 phases (next() returns None)\n- Consumer can stop after Initial if latency-sensitive (skip Phase 1)\n\nDocument these semantics in the SearchPhase doc comments with examples showing\nhow a TUI consumer would handle each variant.\n","created_at":"2026-02-13T20:45:32Z"}]}
{"id":"bd-3un.6","title":"Implement FNV-1a hash embedder (always-available fallback)","description":"Implement the FNV-1a hash-based embedder in frankensearch-embed. This is the zero-dependency, always-available fallback that produces deterministic (but non-semantic) embeddings. It's critical as the baseline that works even when no ML models are downloaded.\n\nAlgorithm (from cass src/search/hash_embedder.rs):\n1. Tokenize: lowercase input, split on non-alphanumeric chars, filter tokens < 2 chars\n2. Hash each token with FNV-1a (u64):\n   - offset_basis = 0xcbf29ce484222325\n   - prime = 0x100000001b3\n   - For each byte: hash ^= byte; hash = hash.wrapping_mul(prime)\n3. Project into embedding space:\n   - index = hash % dimension (default 384)\n   - sign = if (hash >> 63) == 1 { 1.0 } else { -1.0 }\n   - embedding[index] += sign\n4. L2 normalize to unit length\n\nKey properties:\n- Dimension: 384 (matching MiniLM for index compatibility, configurable)\n- No model files required (pure algorithmic)\n- ~0.07ms per embedding (fastest possible)\n- ID: 'fnv1a-{dimension}' (e.g., 'fnv1a-384')\n- ModelCategory::HashEmbedder\n- is_semantic() returns false\n- Deterministic: same input always produces same output\n\nImplementation notes:\n- No external dependencies needed\n- Include unit tests with known input→output pairs for regression\n- The hash embedder serves as the 'test double' for all pipeline testing\n- In cass, this is always the fallback when ML models aren't available\n\nThis goes in frankensearch-embed/src/hash_embedder.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:48:40.235364554Z","created_by":"ubuntu","updated_at":"2026-02-13T20:33:28.955159023Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","hash","phase2"],"dependencies":[{"issue_id":"bd-3un.6","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:40.235364554Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.6","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:07.536936514Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":25,"issue_id":"bd-3un.6","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. CONFIGURABLE DIMENSION: The hash embedder should default to 384 (matching MiniLM for compatibility) but allow any dimension via constructor:\n   pub fn new(dimension: usize) -> Self\n   pub fn default_384() -> Self { Self::new(384) }\n   pub fn default_256() -> Self { Self::new(256) }  // for fast-tier compatibility\n\n2. REGRESSION TEST VALUES: Include deterministic test cases:\n   - HashEmbedder::default_384().embed(\"hello world\") must produce the exact same vector every time\n   - Document the expected output for this input in the test so any algorithm change is detected\n\n3. TOKEN MINIMUM LENGTH: From cass hash_embedder.rs, filter tokens with length < 2 chars. This removes noise from single-character tokens.\n\n4. THE HASH EMBEDDER IS ALSO THE TEST DOUBLE: In integration tests, the hash embedder serves as both fast AND quality tier (since we can't download ML models in CI). The 2-tier pipeline works with hash as both tiers -- quality \"refinement\" just produces the same rankings, which is fine for testing the pipeline mechanics.\n","created_at":"2026-02-13T20:26:31Z"},{"id":40,"issue_id":"bd-3un.6","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Hash Embedder)\n\n## Upgrading the \"Dumb\" Embedder with Principled Hashing\n\nThe FNV-1a hash embedder is the zero-dependency fallback. It produces non-semantic embeddings. But we can make it MUCH better with principled random projection theory, while keeping zero dependencies.\n\n### 1. Random Hyperplane Hashing (Johnson-Lindenstrauss Projection)\n\nInstead of FNV-1a hash → modular projection, use a seeded random hyperplane approach:\n\n  For each token:\n    1. Hash token to u64 seed (FNV-1a, as now)\n    2. Use seed to generate d random signs via xorshift64:\n       for each dimension j:\n         bit = (seed >> (j % 64)) & 1\n         embedding[j] += if bit == 1 { 1.0 } else { -1.0 }\n    3. L2 normalize\n\nWHY THIS IS BETTER: The Johnson-Lindenstrauss lemma guarantees that random projections preserve pairwise distances with high probability. Specifically, for n points in R^D projected to R^d:\n\n  (1-epsilon) * ||u-v||^2 <= ||f(u)-f(v)||^2 <= (1+epsilon) * ||u-v||^2\n\nwith d = O(log(n) / epsilon^2). For d=384 and epsilon=0.3, this works for up to n=10^17 tokens.\n\nThe current modular projection (hash % dimension) creates collisions at rate 1/d. The random hyperplane approach spreads each token's contribution across ALL dimensions, which is provably better for preserving distance structure.\n\n### 2. Locality-Sensitive Hashing (LSH) Variant\n\nFor the hash embedder to be useful as a pre-filter (e.g., \"quickly find candidate docs before semantic search\"), we can use SimHash:\n\n  SimHash(text) = sign(sum of random_hyperplane_embedding(token) for token in text)\n\nSimHash has the property that:\n  P(SimHash(a) == SimHash(b)) = 1 - angle(a,b)/pi\n\nThis means the hash embedder's cosine similarity APPROXIMATES the true angular distance between documents' token distributions. It's not semantic, but it captures lexical overlap with formal guarantees.\n\n### 3. Weighted Token Contribution\n\nInstead of equal weight per token, use IDF-like weighting:\n\n  weight(token) = 1.0 / log(1.0 + estimated_frequency(token))\n\nEstimate frequency using the hash itself (tokens that hash to common buckets are likely common). This is a rough heuristic but mathematically motivated by TF-IDF theory.\n\n### 4. Keep It Simple\n\nThese improvements are all zero-dependency and add < 50 lines of code. The hash embedder stays fast (~0.07ms) and deterministic. The JL projection is the highest-value change — it's a single-line algorithmic improvement with formal guarantees from random matrix theory.\n","created_at":"2026-02-13T20:33:28Z"}]}
{"id":"bd-3un.7","title":"Implement Model2Vec embedder (potion-128M fast tier)","description":"Implement the Model2Vec static embedder for the fast tier. This wraps potion-multilingual-128M (and optionally potion-retrieval-32M) which are static token embedding models — they look up pre-computed embeddings per token and mean-pool them, with no transformer inference needed.\n\nArchitecture (from xf src/model2vec_embedder.rs and agent-mail src/model2vec.rs):\n1. Load BPE tokenizer from HuggingFace tokenizer.json\n2. Load static embedding matrix from safetensors file\n3. For each input text:\n   a. Tokenize with BPE → token IDs\n   b. Look up embedding vector for each token ID in the matrix\n   c. Mean-pool all token embeddings\n   d. L2 normalize\n4. Return f32 vector\n\nModels to support:\n- potion-multilingual-128M: 256 dims, ~128MB, ~0.5-0.9ms\n  - HuggingFace: minishlab/potion-multilingual-128M\n  - This is the PRIMARY fast-tier model\n- potion-retrieval-32M: 512 dims, ~32MB, ~0.9ms (optional)\n\nDependencies:\n- tokenizers = '0.21' (HuggingFace BPE tokenizer)\n- safetensors = '0.5' (loading model weights)\n\nFeature gating: Behind 'model2vec' feature flag in frankensearch-embed\n\nKey design decisions:\n- The tokenizer and embedding matrix are loaded once and held in memory\n- Thread-safe via immutable state (no Mutex needed after init)\n- Supports MRL (Matryoshka) truncation for flexible dimension reduction\n- ModelCategory::StaticEmbedder, ModelTier::Fast\n\nFile location: frankensearch-embed/src/model2vec_embedder.rs\n\nBakeoff results (from xf results/bakeoff/BAKEOFF_REPORT.md):\n- potion-multilingual-128M: 0.574ms p50, 52,144 embeddings/sec, 223x faster than MiniLM\n- Good enough semantics for initial results that get refined\n\nReference implementations:\n- xf: src/model2vec_embedder.rs\n- agent-mail: crates/mcp-agent-mail-search-core/src/model2vec.rs\n- cass: not directly (uses daemon forwarding)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:48:40.314647546Z","created_by":"ubuntu","updated_at":"2026-02-13T20:26:17.583838257Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","fast-tier","model2vec","phase2"],"dependencies":[{"issue_id":"bd-3un.7","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:40.314647546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.7","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:07.628008904Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":6,"issue_id":"bd-3un.7","author":"Dicklesworthstone","text":"MODEL2VEC / POTION CONTEXT: Model2Vec (from the minishlab project) represents a clever middle ground between hash embeddings and full transformers. Instead of running attention at inference time, it pre-computes per-token embeddings during training and stores them as a static lookup table.\n\nAt inference time, it's just: tokenize → lookup → mean pool → normalize. No matrix multiplies, no attention, no GPU needed. This gives 223x speedup over MiniLM while retaining meaningful semantic similarity.\n\npotion-multilingual-128M specifically:\n- 128M parameters (the embedding table itself)\n- 256 output dimensions\n- Multilingual (works across languages)\n- ~0.57ms per embedding on CPU\n- Semantic quality: good enough for initial results that will be refined\n\nThe safetensors format is used for the embedding weights (efficient memory-mapped loading). The tokenizer is standard HuggingFace BPE (tokenizer.json).\n\nThis is the 'secret sauce' that makes the 2-tier system viable — without a fast-enough embedding model, you'd just use MiniLM and accept the latency.","created_at":"2026-02-13T17:56:49Z"},{"id":24,"issue_id":"bd-3un.7","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. TENSOR DISCOVERY: The safetensors file may use different tensor names across model versions. From xf model2vec_embedder.rs, search in order: \"embeddings\", \"embedding\", \"word_embeddings\", \"embed\", \"emb\". If only one tensor exists in the file, use it regardless of name. This prevents breakage when model authors rename tensors.\n\n2. REQUIRED FILES: Only 2 files needed (from xf):\n   REQUIRED_FILES = [\"tokenizer.json\", \"model.safetensors\"]\n   Not the 5+ files that ONNX models need. This is a key advantage of Model2Vec.\n\n3. MEMORY LAYOUT: The embedding matrix is loaded as Vec<Vec<f32>> with shape [vocab_size x dimension]. For potion-128M with vocab ~32K and dim 256, this is ~32MB resident. The matrix is immutable after load so no Mutex needed.\n\n4. EXPECTED TENSOR SHAPE: Expect a 2D f32 tensor of shape [vocab_size, dimensions]. Validate both dimensions on load and return SearchError::ModelLoadFailed if mismatched.\n","created_at":"2026-02-13T20:26:17Z"}]}
{"id":"bd-3un.8","title":"Implement FastEmbed embedder (MiniLM-L6-v2 quality tier)","description":"Implement the FastEmbed/ONNX-based embedder for the quality tier. This wraps all-MiniLM-L6-v2 via the fastembed crate (which uses ONNX Runtime under the hood). This is the 'gold standard' embedding model for search quality.\n\nArchitecture (from cass src/search/fastembed_embedder.rs and xf src/fastembed_embedder.rs):\n1. Load ONNX model from local directory (model.onnx + tokenizer.json + config.json)\n2. Create ONNX Runtime session (CPU execution provider)\n3. For each input:\n   a. Tokenize with WordPiece tokenizer\n   b. Run ONNX inference (attention + pooling)\n   c. Mean-pool hidden states\n   d. L2 normalize\n\nModel details:\n- all-MiniLM-L6-v2: 384 dims, ~90MB model file, ~128ms inference\n  - HuggingFace: sentence-transformers/all-MiniLM-L6-v2\n  - Revision: c9745ed1d9f207416be6d2e6f8de32d1f16199bf (pinned)\n  - Note: model moved to onnx/ subdirectory in 2026-01 restructuring\n\nDependencies:\n- fastembed = '4.9' with features ['ort-download-binaries']\n  - OR use ort directly for more control\n\nFeature gating: Behind 'fastembed' feature flag\n\nKey design decisions:\n- Model wrapped in Mutex<TextEmbedding> because ONNX sessions aren't Send+Sync\n- Batch support via embed_batch() for 3x throughput during indexing\n- ModelCategory::TransformerEmbedder, ModelTier::Quality\n- Load time: ~100ms (one-time cost)\n\nRequired model files (per cass src/search/model_download.rs):\n- onnx/model.onnx (90MB, SHA256: 6fd5d72fe4589f189f8ebc006442dbb529bb7ce38f8082112682524616046452)\n- tokenizer.json (466KB)\n- config.json\n- special_tokens_map.json\n- tokenizer_config.json\n\nFile location: frankensearch-embed/src/fastembed_embedder.rs\n\nBakeoff results:\n- all-MiniLM-L6-v2: 128ms p50, 228 embeddings/sec (baseline)\n- Best quality-to-speed ratio of all tested models\n- Significantly better semantics than static embedders\n\nReference implementations:\n- cass: src/search/fastembed_embedder.rs (210 lines)\n- xf: src/fastembed_embedder.rs\n- agent-mail: crates/mcp-agent-mail-search-core/src/fastembed.rs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T17:48:40.395258044Z","created_by":"ubuntu","updated_at":"2026-02-13T20:44:48.982359043Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","fastembed","phase2","quality-tier"],"dependencies":[{"issue_id":"bd-3un.8","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:40.395258044Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.8","depends_on_id":"bd-3un.3","type":"blocks","created_at":"2026-02-13T17:55:07.716683310Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":44,"issue_id":"bd-3un.8","author":"Dicklesworthstone","text":"REVISION: FastEmbed Embedder Hardening\n\nCritical implementation details for ONNX Runtime integration:\n\n1. Sigmoid Activation on Reranker Outputs:\n   While FastEmbed produces embeddings (not logit scores), this bead should document that\n   embeddings are L2-normalized post-inference. If any future cross-encoder mode is added,\n   raw ONNX logits need sigmoid activation (see bd-3un.25 for the reranker case).\n\n2. ONNX Error Handling:\n   - Model not found: Return SearchError::EmbeddingError with model path\n   - Inference failure: Catch ort::Error, wrap in SearchError, log at WARN with query length\n   - Session creation failure: Fail fast at init, not at first embed() call\n   - Thread pool exhaustion: ort uses rayon internally; if calling from rayon, use spawn_blocking\n     to avoid thread starvation (nested rayon deadlock)\n\n3. Mutex Contention Under Concurrent Access:\n   TextEmbedding is !Send + !Sync, hence Mutex wrapping. Under high concurrency:\n   - Single-threaded embedding is the bottleneck (~128ms per call)\n   - Consider multiple sessions (2-4) behind a round-robin or channel-based pool\n   - For V1: single Mutex is fine; document the contention risk for V2\n\n4. Batch Processing:\n   - batch_size=32 is optimal for MiniLM (fits in L2 cache for typical token lengths)\n   - Batch embed reduces per-call overhead from session lock acquisition\n   - Pre-allocate output Vec with exact capacity (batch_size * dimension)\n   - Track batch timing: log at DEBUG level \"embedded {n} docs in {ms}ms ({per_doc}ms/doc)\"\n\n5. Memory Footprint:\n   - ONNX model: ~90MB resident after load\n   - Session creation: one-time ~2s startup cost\n   - GraphOptimizationLevel::Level3 for production (Level1 for faster startup in tests)\n   - Document in tracing: INFO \"quality_model_loaded\" with model_size_bytes, load_time_ms\n\n6. Model File Verification:\n   - SHA256 check on model.onnx at load time (cross-reference bd-3un.10 manifest)\n   - If verification fails: log ERROR, return EmbeddingError, fall back to fast tier\n","created_at":"2026-02-13T20:44:48Z"}]}
{"id":"bd-3un.9","title":"Implement embedder auto-detection and fallback chain","description":"Implement automatic embedder detection and graceful fallback chain. When a consumer creates a search context, the system should automatically detect which models are available and build the appropriate embedder stack.\n\nFallback chain (priority order):\n1. Quality: MiniLM-L6-v2 (if ONNX model files present)\n2. Fast: potion-multilingual-128M (if safetensors files present)\n3. Hash: FNV-1a (always available, zero deps)\n\nAvailability detection:\n- Check model directory for required files (model.onnx, tokenizer.json, etc.)\n- Use platform-specific data dirs (dirs crate) for model cache location\n- Environment variable overrides: FRANKENSEARCH_MODEL_DIR\n- Log which models are available/unavailable at startup (tracing)\n\npub enum TwoTierAvailability {\n    Full,         // Both fast (potion) + quality (MiniLM) available\n    FastOnly,     // Only potion available → no quality refinement\n    QualityOnly,  // Only MiniLM → slower but no fast phase\n    HashOnly,     // Only hash → lexical-dominant search, no real semantics\n}\n\npub struct EmbedderStack {\n    fast: Arc<dyn Embedder>,\n    quality: Option<Arc<dyn Embedder>>,\n    availability: TwoTierAvailability,\n}\n\nimpl EmbedderStack {\n    pub fn auto_detect(model_dir: &Path) -> Self { ... }\n    pub fn fast(&self) -> &dyn Embedder { ... }\n    pub fn quality(&self) -> Option<&dyn Embedder> { ... }\n}\n\nThis is the 'resolver' that consumers call instead of manually instantiating embedders.\n\nReference: agent-mail crates/mcp-agent-mail-search-core/src/auto_init.rs (TwoTierContext, TwoTierAvailability)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T17:48:53.173644672Z","created_by":"ubuntu","updated_at":"2026-02-13T20:29:56.842010752Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["embedder","fallback","phase2"],"dependencies":[{"issue_id":"bd-3un.9","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T17:48:53.173644672Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.6","type":"blocks","created_at":"2026-02-13T17:55:07.798634Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.7","type":"blocks","created_at":"2026-02-13T17:55:07.881752898Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3un.9","depends_on_id":"bd-3un.8","type":"blocks","created_at":"2026-02-13T17:55:07.963140142Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":19,"issue_id":"bd-3un.9","author":"Dicklesworthstone","text":"REVISION (review pass 2 - cross-codebase verification):\n\n1. MRL DIMENSION REDUCTION WRAPPER: Add a DimReduceEmbedder that transparently applies Matryoshka Representation Learning (MRL) dimension reduction. From xf model_registry.rs:\n\npub struct DimReduceEmbedder {\n    inner: Arc<dyn Embedder>,\n    target_dim: usize,\n}\n\nimpl Embedder for DimReduceEmbedder {\n    fn embed(&self, text: &str) -> SearchResult<Vec<f32>> {\n        let full = self.inner.embed(text)?;\n        // Take first target_dim elements, then L2 normalize\n        Ok(l2_normalize(&full[..self.target_dim]))\n    }\n    fn dimension(&self) -> usize { self.target_dim }\n    fn id(&self) -> &str { /* format!(\"{}-mrl{}\", self.inner.id(), self.target_dim) */ }\n    fn supports_mrl(&self) -> bool { true }\n    // delegate all other methods to self.inner\n}\n\nConstruction validation:\n- inner.supports_mrl() must be true (error if not)\n- target_dim must be in [1, inner.dimension()] (error if out of range)\n\nThe EmbedderStack should auto-wrap with DimReduceEmbedder when:\n- User requests a specific dimension via config\n- The best available model supports MRL\n- Requested dimension is smaller than model's native dimension\n\nThis makes MRL \"just work\" -- users set target_dim in config and the stack handles everything.\n\n2. AUTO-DETECT LOGGING: Log detailed availability info at startup:\n   INFO \"embedder_detected\" { model, tier, dimension, path, load_time_ms }\n   WARN \"embedder_unavailable\" { model, tier, reason, checked_paths }\n   INFO \"embedder_stack_ready\" { availability, fast_model, quality_model }\n","created_at":"2026-02-13T20:25:03Z"},{"id":34,"issue_id":"bd-3un.9","author":"Dicklesworthstone","text":"ALIEN-ARTIFACT ENHANCEMENT (Embedder Auto-Detection & Quality)\n\n## Mathematical Upgrade: Formal Quality Assessment with Conformal Prediction\n\nThe current EmbedderStack auto-detects available models but has no formal quality measurement. Add principled quality assessment that provides distribution-free guarantees.\n\n### 1. Conformal Prediction for Score Reliability\n\nGiven a calibration set of (query, relevant_docs) pairs, conformal prediction provides sets C(q) such that:\n\n  P(relevant_doc ∈ top_k(search(q))) ≥ 1 - α\n\nfor any α. No distributional assumptions needed. Implementation:\n\n  pub struct ConformalCalibration {\n      nonconformity_scores: Vec<f32>,  // Sorted from calibration\n      alpha: f32,                       // Desired coverage (default: 0.1)\n  }\n\n  impl ConformalCalibration {\n      /// Calibrate using a set of (query, known_relevant_doc) pairs\n      pub fn calibrate(searcher: &TwoTierSearcher, cal_set: &[(String, String)]) -> Self {\n          let scores: Vec<f32> = cal_set.iter().map(|(query, relevant_doc_id)| {\n              let results = searcher.search_flat(query, 100);\n              // Nonconformity = rank of the relevant doc (lower = better)\n              results.iter().position(|r| r.doc_id == *relevant_doc_id)\n                  .map(|r| r as f32)\n                  .unwrap_or(f32::INFINITY)\n          }).collect();\n          let mut sorted = scores;\n          sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());\n          Self { nonconformity_scores: sorted, alpha: 0.1 }\n      }\n\n      /// For a new query, how many results to return to guarantee coverage\n      pub fn required_k(&self) -> usize {\n          let quantile_idx = ((1.0 - self.alpha) * self.nonconformity_scores.len() as f32).ceil() as usize;\n          self.nonconformity_scores[quantile_idx.min(self.nonconformity_scores.len() - 1)] as usize + 1\n      }\n  }\n\nThis tells you: \"to guarantee 90% probability of including the relevant document, you need to return at least k results.\" FORMAL coverage guarantee with no distributional assumptions.\n\n### 2. Bayesian A/B Testing for Embedder Comparison (Bakeoff)\n\nInstead of point estimates (avg NDCG), use Bayesian A/B testing with Beta posteriors:\n\n  // For each embedder pair (A, B), on each query:\n  //   If A ranks relevant doc higher: A_wins += 1\n  //   If B ranks relevant doc higher: B_wins += 1\n\n  // P(A better than B) = P(Beta(A_wins+1, B_wins+1) > 0.5)\n  // = regularized incomplete beta function\n\n  // Decision: if P(A > B) > 0.95, declare A the winner\n  //           if P(A > B) < 0.05, declare B the winner\n  //           else: need more queries (continue testing)\n\nThis provides:\n- Formal stopping criterion (don't over-test or under-test)\n- Probability of correctness (not just \"statistically significant\")\n- Natural handling of ties and near-ties\n\n### 3. Mutual Information for Feature Selection\n\nWhen choosing which embedder for which query type, compute mutual information:\n\n  MI(embedder, relevance | query_type) = Σ p(e,r|q) log(p(e,r|q) / p(e|q)p(r|q))\n\nThis tells you which embedder is most informative for each query type. High MI = the embedder's rankings correlate with actual relevance. Low MI = the embedder adds noise for this query type.\n\nUse this in EmbedderStack to ROUTE queries to the best embedder per query type, rather than always using the same fast/quality pair.\n\n### Implementation Priority\n\n1. Conformal calibration: add as optional constructor on TwoTierSearcher (requires calibration data)\n2. Bayesian A/B: add to bakeoff infrastructure (bd-3un.12)\n3. MI-based routing: add to EmbedderStack as adaptive mode (requires usage data)\n\nAll are optional enhancements that don't change the core API.\n","created_at":"2026-02-13T20:29:56Z"}]}
{"id":"bd-3w1","title":"Epic: FrankenSQLite + pervasive RaptorQ integration","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-13T20:36:01.147613828Z","created_by":"ubuntu","updated_at":"2026-02-13T20:36:29.796657411Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["epic","frankensqlite","raptorq"],"comments":[{"id":42,"issue_id":"bd-3w1","author":"Dicklesworthstone","text":"EPIC OVERVIEW: FrankenSQLite + Pervasive RaptorQ Integration\n\nIntegrate FrankenSQLite (clean-room Rust SQLite reimplementation from /dp/frankensqlite) into the frankensearch crate for two synergistic purposes:\n\n1. FrankenSQLite as the document metadata store and persistent embedding job queue, replacing ad-hoc in-memory structures with crash-safe MVCC-capable SQL storage\n2. Applying the 'pervasive RaptorQ' concept (RFC 6330 fountain codes) to frankensearch's own persistent artifacts (FSVI vector indices, Tantivy segments) for self-healing durability\n\nBACKGROUND ON FRANKENSQLITE:\n- 24-crate Cargo workspace, clean-room Rust reimplementation of SQLite\n- #![forbid(unsafe_code)] throughout, Rust edition 2024 nightly\n- Two architectural innovations:\n  a) Page-level MVCC concurrent writers (multiple writers commit simultaneously)\n  b) RaptorQ-pervasive durability (RFC 6330 fountain codes at every persistent layer)\n- 100% file format compatibility with C SQLite in Compatibility mode\n- Native mode with content-addressed ECS (Erasure-Coded Stream) objects\n- Built-in FTS5, R-Tree, JSON1, Session extensions\n\nWHAT \"PERVASIVE RAPTORQ\" MEANS:\n- RaptorQ (RFC 6330) is a fountain code: source data splits into K symbols, repair symbols generated (can be infinite), any K of (K+R) symbols recover the original\n- \"Pervasive\" means fountain codes woven into EVERY persistent layer, not bolted on:\n  - WAL: Each committed frame group gets repair symbols in .wal-fec sidecar\n  - ECS Objects: Every durable object is content-addressed with BLAKE3, carries erasure coding\n  - Snapshot Transfer: Rateless coding for bandwidth-optimal replication\n  - Deterministic Repair: Given object + repair count R, symbols always identical (seed = xxh3_64 of object_id)\n- Key config: PRAGMA raptorq_repair_symbols = N (default: 2), DEFAULT_OVERHEAD_PERCENT = 20%\n\nKEY FRANKENSQLITE APIS:\n  Connection::open(path) -> Result<Self>\n  conn.execute(sql, params) -> Result<u64>\n  conn.prepare(sql) -> Result<PreparedStatement>\n  conn.transaction() -> Result<Transaction>\n\nRAPTORQ INTEGRATION TRAITS:\n  trait SymbolCodec: Send + Sync {\n      fn encode(&self, source_data: &[u8], symbol_size: u32, repair_overhead: f64) -> Result<CodecEncodeResult>;\n      fn decode(&self, symbols: &[(u32, Vec<u8>)], k_source: u32, symbol_size: u32) -> Result<CodecDecodeResult>;\n  }\n\nINTEGRATION TIERS:\n- TIER 1 (Document Store, P1): FrankenSQLite tables for doc metadata, content hashes, embedding status, persistent job queues\n- TIER 2 (Self-Healing Indices, P1): RaptorQ repair symbol trailers on FSVI files and Tantivy wrappers\n- TIER 3 (FTS5 Alternative, P2): FrankenSQLite FTS5 as alternative/fallback lexical engine\n- TIER 4 (Future: Native Mode, P3): ECS commits, time-travel, quorum durability\n\nThis epic is a SIBLING of bd-3un (the main frankensearch epic). Tasks here either add new capabilities or enhance existing bd-3un tasks with FrankenSQLite integration.\n","created_at":"2026-02-13T20:36:29Z"}]}
{"id":"bd-3w1.1","title":"Add frankensearch-storage crate for FrankenSQLite integration","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:36:36.342688123Z","created_by":"ubuntu","updated_at":"2026-02-13T20:37:07.796723516Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation","frankensqlite","scaffold","tier1"],"dependencies":[{"issue_id":"bd-3w1.1","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T20:37:07.676851691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.1","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:37:07.796680615Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.1","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:36:36.342688123Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":43,"issue_id":"bd-3w1.1","author":"Dicklesworthstone","text":"TASK: Add a new frankensearch-storage sub-crate to the workspace.\n\nThis crate is the bridge between frankensearch and FrankenSQLite. It owns all SQL schema, document metadata persistence, and the embedding job queue backed by FrankenSQLite tables.\n\nCRATE STRUCTURE:\n  crates/frankensearch-storage/\n    Cargo.toml\n    src/\n      lib.rs           -- Public API re-exports\n      connection.rs    -- FrankenSQLite connection pool and initialization\n      schema.rs        -- SQL schema definitions and migrations\n      document.rs      -- Document metadata CRUD operations\n      job_queue.rs     -- Persistent embedding job queue\n      content_hash.rs  -- SHA-256 content dedup tracking\n      metrics.rs       -- Storage-level metrics (atomic counters)\n\nCARGO.TOML DEPENDENCIES:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite = { path = \"/dp/frankensqlite/fsqlite\" }  # The facade crate\n  sha2 = \"0.10\"       # SHA-256 for content hashing\n  tracing = \"0.1\"     # Structured logging\n  serde = { version = \"1\", features = [\"derive\"] }\n  serde_json = \"1\"\n\nFEATURE FLAG INTEGRATION:\n  The storage crate should be feature-gated in the workspace:\n  [features]\n  storage = [\"dep:frankensearch-storage\"]\n\n  This keeps FrankenSQLite optional for consumers who only need in-memory search.\n  But 'full' and 'hybrid' bundles should include 'storage' by default.\n\nDESIGN DECISIONS:\n1. FrankenSQLite is used as a LOCAL dependency (path dep), not a crates.io dep, because it's a sibling project\n2. The crate owns the Connection lifecycle -- consumers never touch raw SQL\n3. All tables use WAL mode by default for concurrent read/write\n4. Schema versioning via a 'schema_version' table for safe migrations\n5. Thread-safe: Connection wrapped in Arc for sharing across threads\n\nINITIALIZATION PATTERN:\n  pub struct StorageConfig {\n      pub db_path: PathBuf,\n      pub wal_mode: bool,            // Default: true\n      pub busy_timeout_ms: u64,      // Default: 5000\n      pub raptorq_repair_symbols: u32, // Default: 2 (FrankenSQLite PRAGMA)\n      pub cache_size_pages: i32,     // Default: 2000 (~8MB at 4KB pages)\n  }\n\n  pub struct Storage {\n      conn: Connection,   // FrankenSQLite connection\n      config: StorageConfig,\n  }\n\n  impl Storage {\n      pub fn open(config: StorageConfig) -> SearchResult<Self>;\n      pub fn open_in_memory() -> SearchResult<Self>;  // For tests\n      pub fn connection(&self) -> &Connection;\n      pub fn transaction(&self) -> SearchResult<Transaction>;\n  }\n\nWHY FRANKENSQLITE OVER RUSQLITE:\n1. MVCC concurrent writers: embedding workers and search queries don't block each other\n2. Pervasive RaptorQ: automatic self-healing for the metadata store itself\n3. FTS5 built-in: can serve as alternative lexical engine (Tier 3)\n4. Same Rust ecosystem: safe Rust, no FFI, no C SQLite dependency\n5. Future: Native mode enables time-travel queries and distributed replication\n","created_at":"2026-02-13T20:37:03Z"}]}
{"id":"bd-3w1.10","title":"Implement FTS5 alternative lexical engine adapter","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:37:28.727069396Z","created_by":"ubuntu","updated_at":"2026-02-13T20:47:07.572987879Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","fts5","lexical","tier3"],"dependencies":[{"issue_id":"bd-3w1.10","depends_on_id":"bd-3un.17","type":"blocks","created_at":"2026-02-13T20:42:29.354997908Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3un.18","type":"blocks","created_at":"2026-02-13T20:46:38.969559724Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:47:07.572949758Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:28.727069396Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:29.234471157Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.10","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:38.878690Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":63,"issue_id":"bd-3w1.10","author":"Dicklesworthstone","text":"TASK: Implement FrankenSQLite FTS5 as an alternative lexical search engine.\n\nFrankenSQLite includes a full FTS5 implementation with BM25 ranking. This task creates an adapter that implements frankensearch's LexicalIndex trait using FTS5, providing an alternative to Tantivy for text search.\n\nWHY FTS5 AS ALTERNATIVE:\n1. Single dependency: FrankenSQLite already provides FTS5, no need for separate Tantivy dep\n2. Smaller binary: FTS5 is embedded in the SQLite engine vs Tantivy (~5MB added binary size)\n3. Transactional consistency: FTS5 index and document metadata are in the SAME database, atomically consistent\n4. MVCC: concurrent readers and writers without blocking\n5. Simpler deployment: one .db file contains everything (data + FTS index + vector metadata)\n\nTRADE-OFFS vs TANTIVY:\n| Feature | Tantivy | FTS5 |\n|---------|---------|------|\n| BM25 ranking | Yes | Yes |\n| Phrase queries | Yes | Yes |\n| Prefix queries | Yes (edge n-grams) | Yes (prefix*) |\n| Boolean operators | AND, OR, NOT | AND, OR, NOT |\n| Tokenizer customization | Full control | unicode61, porter, trigram |\n| Performance (search) | Faster (purpose-built) | Adequate (embedded) |\n| Performance (index) | Faster (batch-oriented) | Adequate (row-at-a-time) |\n| Snippet generation | Via tantivy-highlights | Built-in highlight(), snippet() |\n| Binary size impact | ~5MB | 0 (already in FrankenSQLite) |\n| Concurrent write | Requires IndexWriter lock | MVCC (fully concurrent) |\n\nADAPTER API:\n\n  pub struct Fts5LexicalIndex {\n      storage: Arc<Storage>,\n      table_name: String,  // FTS5 virtual table name\n  }\n\n  impl Fts5LexicalIndex {\n      pub fn create(storage: Arc<Storage>, config: Fts5Config) -> SearchResult<Self>;\n\n      /// Create FTS5 virtual table with content sync\n      /// CREATE VIRTUAL TABLE {name} USING fts5(\n      ///     doc_id,\n      ///     title,\n      ///     content,\n      ///     content_preview,\n      ///     tokenize='unicode61 remove_diacritics 2',\n      ///     content=documents,          -- External content from documents table\n      ///     content_rowid=rowid\n      /// );\n      fn create_fts5_table(&self) -> SearchResult<()>;\n  }\n\n  // Implement the same LexicalIndex trait that Tantivy uses\n  impl LexicalIndex for Fts5LexicalIndex {\n      fn search(&self, query: &str, limit: usize, offset: usize) -> SearchResult<Vec<LexicalHit>>;\n      fn index_document(&self, doc: &IndexableDocument) -> SearchResult<()>;\n      fn index_batch(&self, docs: &[IndexableDocument]) -> SearchResult<usize>;\n      fn delete_document(&self, doc_id: &str) -> SearchResult<bool>;\n      fn document_count(&self) -> SearchResult<usize>;\n      fn optimize(&self) -> SearchResult<()>;\n  }\n\n  pub struct Fts5Config {\n      pub tokenizer: Fts5Tokenizer,      // Default: Unicode61\n      pub content_mode: Fts5ContentMode, // Default: External (references documents table)\n      pub prefix_sizes: Vec<usize>,      // Default: [2, 3] (for prefix queries)\n  }\n\n  pub enum Fts5Tokenizer {\n      Unicode61 { remove_diacritics: bool },\n      Porter,                             // English stemming\n      Trigram,                            // Substring search (slower but more flexible)\n  }\n\n  pub enum Fts5ContentMode {\n      Regular,                            // FTS5 stores its own copy\n      External,                           // References documents table (saves space)\n      Contentless,                        // Index-only (no snippets)\n  }\n\nSEARCH QUERY TRANSLATION:\n  frankensearch queries need translation to FTS5 syntax:\n  - Simple terms: \"hello world\" -> 'hello world' (implicit AND in FTS5)\n  - Phrase: '\"exact match\"' -> '\"exact match\"'\n  - Boolean: 'cat OR dog' -> 'cat OR dog'\n  - Prefix: 'hel*' -> 'hel*'\n  - Column filter: 'title:hello' -> 'title:hello'\n\n  FTS5 returns: doc_id, rank (BM25 score), snippet\n\nCONTENT SYNC:\n  When using External content mode, the FTS5 index references the documents table.\n  On document insert/update/delete, FTS5 must be notified:\n  - INSERT: INSERT INTO fts5_table(rowid, ...) VALUES (...)\n  - DELETE: INSERT INTO fts5_table(fts5_table, rowid, ...) VALUES ('delete', ...)\n  - UPDATE: DELETE then INSERT (FTS5 doesn't support in-place update)\n\n  This sync is handled in the document upsert flow (bd-3w1.2).\n\nFEATURE FLAG:\n  This adapter is gated behind 'fts5' feature (separate from 'lexical' which is Tantivy):\n  [features]\n  fts5 = [\"dep:frankensearch-storage\"]  # FTS5 requires the storage crate (FrankenSQLite)\n  lexical = [\"dep:tantivy\"]              # Tantivy (existing)\n\n  Consumers choose one or both. The fusion layer (RRF) works with either.\n\nFile: frankensearch-storage/src/fts5_adapter.rs\n","created_at":"2026-02-13T20:46:11Z"}]}
{"id":"bd-3w1.11","title":"Implement index metadata persistence in FrankenSQLite","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:29.786720687Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:39.837915258Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","metadata","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.11","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:29.786720687Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.11","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.954346477Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.11","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:39.837876315Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":64,"issue_id":"bd-3w1.11","author":"Dicklesworthstone","text":"TASK: Implement index metadata persistence in FrankenSQLite.\n\nStore all index-related metadata in FrankenSQLite tables so the system can track what's been indexed, with which models, and when. This replaces the sentinel file approach from bd-3un.41 with a proper relational schema.\n\nSQL SCHEMA:\n\n  CREATE TABLE IF NOT EXISTS index_metadata (\n      index_name TEXT PRIMARY KEY,          -- e.g., \"vector.fast\", \"vector.quality\", \"lexical\"\n      index_type TEXT NOT NULL,             -- \"fsvi\" | \"tantivy\" | \"fts5\" | \"hnsw\"\n      embedder_id TEXT,                     -- For vector indices: which model\n      embedder_revision TEXT,               -- Model commit SHA\n      dimension INTEGER,                    -- For vector indices: embedding dimension\n      record_count INTEGER NOT NULL DEFAULT 0,\n      file_path TEXT,                       -- Absolute path to index file\n      file_size_bytes INTEGER,\n      file_hash TEXT,                       -- xxh3_64 hex of index file (for staleness)\n      schema_version TEXT,                  -- e.g., \"tantivy-schema-v1-frankensearch\"\n      built_at INTEGER NOT NULL,            -- Unix timestamp millis\n      build_duration_ms INTEGER,            -- How long the build took\n      source_doc_count INTEGER,             -- How many source documents existed at build time\n      config_json TEXT,                     -- Serialized build config for reproducibility\n      fec_path TEXT,                        -- Path to .fec sidecar (if durability enabled)\n      fec_size_bytes INTEGER,\n      last_verified_at INTEGER,             -- When durability was last verified\n      last_repair_at INTEGER,               -- When last repair occurred (null if never)\n      repair_count INTEGER DEFAULT 0        -- Total repairs performed on this index\n  );\n\n  CREATE TABLE IF NOT EXISTS index_build_history (\n      build_id INTEGER PRIMARY KEY AUTOINCREMENT,\n      index_name TEXT NOT NULL REFERENCES index_metadata(index_name),\n      built_at INTEGER NOT NULL,\n      build_duration_ms INTEGER,\n      record_count INTEGER,\n      source_doc_count INTEGER,\n      trigger TEXT,                          -- \"initial\" | \"content_change\" | \"model_update\" | \"corruption_repair\" | \"schema_migration\" | \"manual\"\n      config_json TEXT,\n      notes TEXT\n  );\n\nAPI:\n\n  impl Storage {\n      /// Record that an index was built/rebuilt\n      pub fn record_index_build(&self, meta: &IndexBuildRecord) -> SearchResult<()>;\n\n      /// Get current metadata for an index\n      pub fn get_index_metadata(&self, index_name: &str) -> SearchResult<Option<IndexMetadata>>;\n\n      /// Check if an index needs rebuilding (content changed since last build)\n      pub fn check_index_staleness(&self, index_name: &str) -> SearchResult<StalenessCheck>;\n\n      /// Record a durability verification event\n      pub fn record_verification(&self, index_name: &str, result: &VerifyResult) -> SearchResult<()>;\n\n      /// Record a repair event\n      pub fn record_repair(&self, index_name: &str, result: &RepairResult) -> SearchResult<()>;\n\n      /// Get build history for an index\n      pub fn get_build_history(&self, index_name: &str, limit: usize) -> SearchResult<Vec<IndexBuildRecord>>;\n  }\n\n  pub struct StalenessCheck {\n      pub is_stale: bool,\n      pub reason: Option<StalenessReason>,\n      pub docs_since_build: usize,          -- Documents added/changed since last build\n      pub index_age: Duration,\n      pub source_doc_count: usize,          -- Current document count\n      pub index_record_count: usize,        -- Records in the index\n  }\n\n  pub enum StalenessReason {\n      NewDocuments { count: usize },\n      ContentChanged { count: usize },\n      ModelUpdated { old_rev: String, new_rev: String },\n      SchemaChanged { old: String, new: String },\n      IndexMissing,\n      IndexCorrupted,\n  }\n\nSTALENESS DETECTION QUERY:\n  -- Count documents added/changed since the index was built\n  SELECT COUNT(*) FROM documents d\n  WHERE d.updated_at > (SELECT built_at FROM index_metadata WHERE index_name = ?name)\n  OR NOT EXISTS (\n      SELECT 1 FROM embedding_status es\n      WHERE es.doc_id = d.doc_id\n      AND es.embedder_id = ?embedder_id\n      AND es.status = 'embedded'\n  );\n\n  This is O(1) with the partial index on embedding_status.\n\nINTEGRATION WITH bd-3un.41 (Staleness Detection):\n  This bead provides the STORAGE LAYER for staleness detection.\n  bd-3un.41 provides the CACHE AND POLICY layer that decides what to do about staleness.\n  The two work together:\n  - This bead: \"is the index stale?\" (database query)\n  - bd-3un.41: \"what should we do about it?\" (auto-rebuild, prompt, ignore)\n\nBUILD HISTORY VALUE:\n  The index_build_history table enables:\n  1. Debugging: \"when was this index last rebuilt and why?\"\n  2. Performance tracking: \"are builds getting slower?\"\n  3. Audit: \"how many times has this index been repaired?\"\n  4. Capacity planning: \"what's the growth rate of indexed documents?\"\n\nFile: frankensearch-storage/src/schema.rs (extension of bd-3w1.2 schema)\n","created_at":"2026-02-13T20:46:15Z"}]}
{"id":"bd-3w1.12","title":"Implement storage-backed staleness detector integration","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:37:30.614377855Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:16.066671561Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","staleness","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.12","depends_on_id":"bd-3un.41","type":"blocks","created_at":"2026-02-13T20:42:29.597868661Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.12","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:30.614377855Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.12","depends_on_id":"bd-3w1.11","type":"blocks","created_at":"2026-02-13T20:42:29.478253046Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":65,"issue_id":"bd-3w1.12","author":"Dicklesworthstone","text":"TASK: Implement storage-backed staleness detector integration.\n\nThis bridges the FrankenSQLite storage layer (bd-3w1.11) with the IndexCache staleness detection system (bd-3un.41). Instead of relying solely on file timestamps, the staleness detector queries the document metadata database for precise change detection.\n\nINTEGRATION API:\n\n  pub struct StorageBackedStaleness {\n      storage: Arc<Storage>,\n      config: StalenessConfig,\n  }\n\n  pub struct StalenessConfig {\n      /// Minimum number of new/changed documents before triggering rebuild\n      pub min_change_threshold: usize,       // Default: 10\n      /// Maximum age of index before forced rebuild (even if no changes detected)\n      pub max_index_age_secs: Option<u64>,   // Default: None (no age limit)\n      /// Check model revision changes (embedder update)\n      pub check_model_revision: bool,        // Default: true\n      /// Check schema version changes\n      pub check_schema_version: bool,        // Default: true\n  }\n\n  impl StorageBackedStaleness {\n      /// Comprehensive staleness check combining all signals\n      pub fn check(&self, index_name: &str, current_embedder_rev: Option<&str>, current_schema: Option<&str>) -> SearchResult<StalenessReport>;\n\n      /// Quick check: just count pending embeddings (fast, no full scan)\n      pub fn quick_check(&self, embedder_id: &str) -> SearchResult<QuickStalenessCheck>;\n  }\n\n  pub struct StalenessReport {\n      pub is_stale: bool,\n      pub reasons: Vec<StalenessReason>,\n      pub confidence: f32,                   // 0.0 to 1.0\n      pub recommended_action: RecommendedAction,\n      pub stats: StalenessStats,\n  }\n\n  pub struct StalenessStats {\n      pub total_documents: usize,\n      pub indexed_documents: usize,\n      pub pending_documents: usize,\n      pub failed_documents: usize,\n      pub docs_changed_since_build: usize,\n      pub index_age: Duration,\n      pub last_build_duration: Option<Duration>,\n  }\n\n  pub enum RecommendedAction {\n      NoAction,                              // Index is fresh\n      IncrementalUpdate { doc_count: usize }, // Only embed new/changed docs\n      FullRebuild { reason: String },         // Schema change, model update, etc.\n  }\n\nQUICK CHECK SQL (O(1)):\n  SELECT COUNT(*) as pending FROM embedding_status\n  WHERE embedder_id = ?embedder AND status = 'pending';\n\n  This uses the partial index from bd-3w1.2 and is effectively instant.\n\nINTEGRATION WITH IndexCache (bd-3un.41):\n  The IndexCache holds OnceLock<Option<VectorIndex>> for each tier.\n  The staleness detector is called:\n  1. On cache initialization (first access)\n  2. Periodically (configurable interval)\n  3. After document ingestion (new content triggers check)\n\n  When staleness is detected:\n  - IncrementalUpdate: queue the pending documents for embedding, update vector index in-place\n  - FullRebuild: clear the cache, rebuild all indices from the document store\n\n  The document store (FrankenSQLite) is the SOURCE OF TRUTH. Vector indices and Tantivy\n  indices are derived/cached artifacts that can always be rebuilt from the source.\n\nCONFIDENCE SCORING:\n  Confidence reflects how certain we are that rebuilding would improve results:\n  - 0.0: Index is perfectly fresh (no changes)\n  - 0.3: A few documents changed (< min_threshold)\n  - 0.7: Significant content change (> 10% of documents)\n  - 0.9: Model revision changed (embeddings are from wrong model version)\n  - 1.0: Schema changed or index missing (rebuild is mandatory)\n\nFile: frankensearch-storage/src/staleness.rs\n","created_at":"2026-02-13T20:46:16Z"}]}
{"id":"bd-3w1.13","title":"Wire FrankenSQLite storage into EmbeddingJobRunner pipeline","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:31.798119798Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:45.175782922Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","integration","pipeline","tier1"],"dependencies":[{"issue_id":"bd-3w1.13","depends_on_id":"bd-3un.27","type":"blocks","created_at":"2026-02-13T20:42:30.872447497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T20:46:45.175740433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:31.798119798Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:42:26.073091423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1.3","type":"blocks","created_at":"2026-02-13T20:42:26.195471543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.13","depends_on_id":"bd-3w1.4","type":"blocks","created_at":"2026-02-13T20:46:45.026328741Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":66,"issue_id":"bd-3w1.13","author":"Dicklesworthstone","text":"TASK: Wire FrankenSQLite storage into the EmbeddingJobRunner pipeline.\n\nThis is the integration bead that connects the persistent storage (bd-3w1.1-4) with the embedding pipeline (bd-3un.27, bd-3un.28). The EmbeddingJobRunner's workflow changes from in-memory queue to:\n\nUPDATED PIPELINE:\n\n  1. INGEST: Document arrives via public API\n     -> Canonicalize text (bd-3un.42)\n     -> Compute SHA-256 content hash (bd-3w1.4)\n     -> Upsert into documents table (bd-3w1.2)\n     -> Check dedup: if content unchanged, skip\n     -> If new/changed: enqueue embedding job (bd-3w1.3)\n\n  2. EMBED: EmbeddingJobRunner processes queue\n     -> Claim batch from persistent queue (bd-3w1.3)\n     -> Read canonical text from documents table\n     -> Embed via fast-tier embedder (bd-3un.7)\n     -> Write embedding to FSVI vector index (bd-3un.13)\n     -> Mark job as completed in queue\n     -> If quality-tier available: enqueue quality embedding job\n\n  3. REFRESH: IndexRefreshWorker (bd-3un.28) periodically\n     -> Check staleness via storage (bd-3w1.12)\n     -> If stale: trigger incremental or full rebuild\n     -> Protect new indices with RaptorQ (bd-3w1.7, bd-3w1.8)\n\nINTEGRATION CODE:\n\n  pub struct StorageBackedJobRunner {\n      storage: Arc<Storage>,\n      queue: Arc<PersistentJobQueue>,\n      canonicalizer: Arc<Canonicalizer>,     // From bd-3un.42\n      content_hasher: ContentHasher,         // From bd-3w1.4\n      fast_embedder: Arc<dyn Embedder>,\n      quality_embedder: Option<Arc<dyn Embedder>>,\n      vector_writer: Arc<Mutex<VectorIndexWriter>>,\n      protector: Option<Arc<FileProtector>>, // From bd-3w1.9, if durability enabled\n      metrics: Arc<PipelineMetrics>,\n  }\n\n  impl StorageBackedJobRunner {\n      /// Ingest a document into the full pipeline\n      pub fn ingest(&self, doc_id: &str, text: &str, metadata: Option<serde_json::Value>) -> SearchResult<IngestResult>;\n\n      /// Ingest a batch of documents\n      pub fn ingest_batch(&self, docs: &[IngestRequest]) -> SearchResult<BatchIngestResult>;\n\n      /// Process one batch of embedding jobs from the persistent queue\n      pub fn process_batch(&self) -> SearchResult<BatchProcessResult>;\n\n      /// Run the embedding worker loop (blocks, processes batches continuously)\n      pub fn run_worker(&self, shutdown: Arc<AtomicBool>) -> SearchResult<WorkerReport>;\n  }\n\n  pub struct IngestResult {\n      pub doc_id: String,\n      pub action: IngestAction,\n  }\n\n  pub enum IngestAction {\n      New,                                    // New document, jobs enqueued\n      Updated,                                // Content changed, re-embedding queued\n      Unchanged,                              // Content hash matched, skipped\n      Skipped { reason: String },             // Low-signal content after canonicalization\n  }\n\n  pub struct BatchProcessResult {\n      pub jobs_claimed: usize,\n      pub jobs_completed: usize,\n      pub jobs_failed: usize,\n      pub jobs_skipped: usize,\n      pub embed_time: Duration,\n      pub total_time: Duration,\n  }\n\nHASH-ONLY SKIP (from agent-mail embedding_jobs.rs line 664):\n  When the embedder is the hash embedder (id starts with \"fnv1a-\"), skip writing to\n  the vector index. Hash embeddings are computed on-the-fly during search because:\n  1. They're instant to compute (~0.01ms)\n  2. They change if the hash function changes (no value in storing)\n  3. They save disk space and I/O\n\n  if embedder.id().starts_with(\"fnv1a-\") {\n      queue.skip(job_id, \"hash embeddings computed on-the-fly\")?;\n      continue;\n  }\n\nTWO-TIER EMBEDDING FLOW:\n  For each document, TWO embedding jobs are enqueued:\n  1. Fast tier (potion-128M, priority=1): processed immediately for instant results\n  2. Quality tier (MiniLM-L6-v2, priority=0): processed in background for refinement\n\n  The priority field ensures fast-tier jobs are claimed first, so the search system\n  has fast results available as quickly as possible.\n\nCRASH RECOVERY:\n  On startup, the runner calls queue.reclaim_stale_jobs() to reset any jobs that were\n  being processed when the previous process died. These get requeued automatically.\n  No work is lost.\n\nTRANSACTIONAL CONSISTENCY (MVCC):\n  FrankenSQLite's MVCC ensures:\n  - ingest_batch() runs in a single transaction (all-or-nothing)\n  - process_batch() can run concurrently with ingest() without blocking\n  - Multiple workers can call claim_batch() concurrently (disjoint batches guaranteed)\n  - search queries see a consistent snapshot even during bulk ingestion\n\nFile: frankensearch-storage/src/pipeline.rs (or frankensearch-fusion/src/storage_pipeline.rs)\n","created_at":"2026-02-13T20:46:16Z"}]}
{"id":"bd-3w1.14","title":"Update Cargo feature flags for storage and durability features","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:35.858268398Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:17.109111092Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","features","frankensqlite","raptorq"],"dependencies":[{"issue_id":"bd-3w1.14","depends_on_id":"bd-3un.29","type":"blocks","created_at":"2026-02-13T20:42:31.231418826Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.14","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:35.858268398Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.14","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:30.989149348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.14","depends_on_id":"bd-3w1.5","type":"blocks","created_at":"2026-02-13T20:42:31.111700999Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":67,"issue_id":"bd-3w1.14","author":"Dicklesworthstone","text":"TASK: Update Cargo feature flags for storage and durability features.\n\nThis extends the feature flag system (bd-3un.29) with new features for the FrankenSQLite and RaptorQ integrations.\n\nUPDATED FEATURE MAP:\n\n  [features]\n  default = [\"hash\"]\n\n  # Existing features (unchanged)\n  hash = []\n  model2vec = [\"dep:safetensors\", \"dep:tokenizers\", \"dep:dirs\"]\n  fastembed = [\"dep:fastembed\"]\n  lexical = [\"dep:tantivy\"]\n  rerank = [\"dep:ort\", \"dep:tokenizers\"]\n  ann = [\"dep:hnsw_rs\"]\n  download = [\"dep:reqwest\"]\n\n  # NEW: FrankenSQLite storage features\n  storage = [\"dep:frankensearch-storage\"]     # Document store, job queue, content dedup\n  fts5 = [\"storage\"]                          # FTS5 lexical engine (requires storage)\n\n  # NEW: Durability features\n  durability = [\"dep:frankensearch-durability\"]  # RaptorQ self-healing indices\n\n  # Updated bundles\n  semantic = [\"hash\", \"model2vec\", \"fastembed\"]\n  hybrid = [\"semantic\", \"lexical\"]\n  persistent = [\"hybrid\", \"storage\"]                    # Hybrid search with persistent storage\n  durable = [\"persistent\", \"durability\"]                # Persistent + self-healing\n  full = [\"durable\", \"rerank\", \"ann\", \"download\"]       # Everything\n  full-fts5 = [\"full\", \"fts5\"]                          # Everything + FTS5 alternative\n\nWORKSPACE-LEVEL FEATURE FORWARDING:\n\n  In the facade crate (frankensearch/Cargo.toml):\n  [dependencies]\n  frankensearch-core = { path = \"../crates/frankensearch-core\" }\n  frankensearch-embed = { path = \"../crates/frankensearch-embed\" }\n  frankensearch-index = { path = \"../crates/frankensearch-index\" }\n  frankensearch-lexical = { path = \"../crates/frankensearch-lexical\", optional = true }\n  frankensearch-fusion = { path = \"../crates/frankensearch-fusion\" }\n  frankensearch-rerank = { path = \"../crates/frankensearch-rerank\", optional = true }\n  frankensearch-storage = { path = \"../crates/frankensearch-storage\", optional = true }      # NEW\n  frankensearch-durability = { path = \"../crates/frankensearch-durability\", optional = true } # NEW\n\nCONDITIONAL COMPILATION:\n\n  In frankensearch-storage/Cargo.toml:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite = { path = \"/dp/frankensqlite/fsqlite\" }\n  sha2 = \"0.10\"\n  tracing = \"0.1\"\n  serde = { version = \"1\", features = [\"derive\"] }\n  serde_json = \"1\"\n\n  In frankensearch-durability/Cargo.toml:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite-core = { path = \"/dp/frankensqlite/crates/fsqlite-core\" }\n  xxhash-rust = { version = \"0.8\", features = [\"xxh3\"] }\n  crc32fast = \"1.4\"\n  tracing = \"0.1\"\n  serde = { version = \"1\", features = [\"derive\"] }\n\nDESIGN RATIONALE:\n1. storage and durability are INDEPENDENT features: you can have storage without durability (no .fec files) or durability without storage (protect files without SQL metadata)\n2. fts5 REQUIRES storage because FTS5 runs inside FrankenSQLite\n3. The 'persistent' bundle is the recommended production configuration: hybrid search with crash-safe metadata\n4. The 'durable' bundle adds self-healing on top of persistent\n5. 'full' now includes durability by default (it's the kitchen-sink bundle)\n6. 'full-fts5' adds FTS5 for consumers who want both Tantivy AND FTS5\n\nCONSUMER USAGE:\n  # Minimal (testing): hash embedder only\n  frankensearch = { version = \"0.1\" }\n\n  # Production (typical): hybrid search with persistence\n  frankensearch = { version = \"0.1\", features = [\"persistent\"] }\n\n  # Production (maximum durability): self-healing indices\n  frankensearch = { version = \"0.1\", features = [\"durable\"] }\n\n  # Everything\n  frankensearch = { version = \"0.1\", features = [\"full\"] }\n\nFile: Updates to frankensearch/Cargo.toml and each sub-crate's Cargo.toml\n","created_at":"2026-02-13T20:46:17Z"}]}
{"id":"bd-3w1.15","title":"Write unit tests for FrankenSQLite storage layer","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:37.023828606Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:48.378051930Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","testing","unit-tests"],"dependencies":[{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:37.023828606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.11","type":"blocks","created_at":"2026-02-13T20:46:48.378006324Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:42:32.564674392Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.3","type":"blocks","created_at":"2026-02-13T20:46:48.234412533Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.15","depends_on_id":"bd-3w1.4","type":"blocks","created_at":"2026-02-13T20:42:32.687086602Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":68,"issue_id":"bd-3w1.15","author":"Dicklesworthstone","text":"TASK: Write unit tests for the FrankenSQLite storage layer.\n\nComprehensive test suite covering all storage operations with detailed tracing.\n\nTEST MODULES:\n\n1. schema_tests:\n   - test_create_tables: Verify schema creation on fresh database\n   - test_schema_migration: Verify schema migration when version changes\n   - test_concurrent_schema_init: Multiple threads calling open() simultaneously\n   - test_in_memory_database: Storage::open_in_memory() works correctly\n   - test_wal_mode_enabled: Verify WAL mode is active after open\n\n2. document_tests:\n   - test_insert_document: Basic insert and retrieve\n   - test_upsert_unchanged: Same content_hash returns false (no-op)\n   - test_upsert_changed: Different content_hash updates and resets embedding status\n   - test_delete_document: Cascades to embedding_status\n   - test_batch_upsert: Atomic batch of 100 documents\n   - test_batch_upsert_rollback: If one doc in batch fails, none persist\n   - test_get_nonexistent: Returns None, not error\n   - test_content_preview: First 400 chars stored correctly\n   - test_metadata_json: Arbitrary JSON roundtrips correctly\n   - test_unicode_doc_id: UTF-8 doc IDs work correctly\n\n3. embedding_status_tests:\n   - test_mark_embedded: Status transitions to 'embedded'\n   - test_mark_failed: Records error message, increments retry_count\n   - test_list_pending: Only returns pending items for specified embedder\n   - test_multi_tier_status: Same doc can have different status for fast vs quality\n   - test_count_by_status: Correct counts for each status\n   - test_embedding_status_cascade: Deleting doc removes all embedding status\n\n4. job_queue_tests:\n   - test_enqueue_and_claim: Basic enqueue, claim, complete cycle\n   - test_dedup_same_hash: Second enqueue with same hash is skipped\n   - test_dedup_different_hash: Second enqueue with different hash replaces\n   - test_claim_batch_disjoint: Two workers get disjoint batches (CRITICAL)\n   - test_visibility_timeout: Stale jobs reclaimed after timeout\n   - test_retry_on_failure: Failed job requeued up to max_retries\n   - test_max_retries_exceeded: Job stays 'failed' after max retries\n   - test_priority_ordering: Higher priority jobs claimed first\n   - test_fifo_within_priority: Same priority jobs claimed in submission order\n   - test_backpressure: is_backpressured() returns true above threshold\n   - test_queue_depth: Correct counts by status\n   - test_concurrent_claim: 4 threads claiming simultaneously (no double-claim)\n   - test_hash_only_skip: fnv1a embedder jobs skipped\n   - test_metrics_tracking: All atomic counters increment correctly\n\n5. content_hash_tests:\n   - test_hash_deterministic: Same text always produces same hash\n   - test_hash_differs: Different text produces different hash\n   - test_dedup_new: New doc_id returns DeduplicationDecision::New\n   - test_dedup_unchanged: Same hash returns Skip\n   - test_dedup_changed: Different hash returns Changed\n   - test_batch_dedup: Batch check returns correct decisions for mixed input\n\n6. index_metadata_tests:\n   - test_record_build: Build metadata persisted correctly\n   - test_staleness_no_changes: Fresh index not stale\n   - test_staleness_new_documents: New docs trigger stale\n   - test_staleness_model_change: Changed embedder_revision triggers stale\n   - test_build_history: Multiple builds recorded chronologically\n   - test_verification_recording: Durability verification events logged\n   - test_repair_recording: Repair events logged with details\n\nLOGGING IN TESTS:\n  All tests use tracing-test to capture and verify log output:\n  #[test]\n  fn test_upsert_changed() {\n      let (storage, _guard) = test_storage_with_tracing();\n      // ... test logic ...\n      // Verify specific log lines were emitted\n      assert!(logs_contain(\"document content changed, resetting embedding status\"));\n  }\n\nSHARED TEST FIXTURES:\n  fn test_storage_with_tracing() -> (Storage, tracing::subscriber::DefaultGuard) {\n      let subscriber = tracing_subscriber::fmt().with_test_writer().finish();\n      let guard = tracing::subscriber::set_default(subscriber);\n      let storage = Storage::open_in_memory().unwrap();\n      (storage, guard)\n  }\n\n  fn sample_document(doc_id: &str) -> DocumentRecord {\n      DocumentRecord {\n          doc_id: doc_id.to_string(),\n          source_path: Some(\"/test/path\".into()),\n          content_preview: \"This is test content for unit testing...\".into(),\n          content_hash: ContentHasher::hash(\"This is test content for unit testing...\"),\n          content_length: 42,\n          created_at: 1707840000000,\n          updated_at: 1707840000000,\n          metadata: None,\n      }\n  }\n\nMVCC CONCURRENCY TESTS:\n  - test_concurrent_read_write: Writer inserts while reader queries (no blocking)\n  - test_concurrent_writers: Two writers insert different docs simultaneously\n  - test_snapshot_isolation: Reader sees consistent snapshot even during writes\n\nFile: frankensearch-storage/src/tests/ (inline #[cfg(test)] modules in each source file)\n","created_at":"2026-02-13T20:46:17Z"}]}
{"id":"bd-3w1.16","title":"Write unit tests for RaptorQ durability layer","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:37.892553351Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:53.334855609Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["raptorq","testing","unit-tests"],"dependencies":[{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:37.892553351Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:32.805806450Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.7","type":"blocks","created_at":"2026-02-13T20:42:32.966379927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.8","type":"blocks","created_at":"2026-02-13T20:46:53.246208996Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.16","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:46:53.334816175Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":69,"issue_id":"bd-3w1.16","author":"Dicklesworthstone","text":"TASK: Write unit tests for the RaptorQ durability layer.\n\nComprehensive test suite for the erasure coding, repair trailer, and self-healing pipelines.\n\nTEST MODULES:\n\n1. codec_tests:\n   - test_encode_decode_roundtrip: Encode data, decode from source symbols only\n   - test_repair_from_partial: Remove some source symbols, decode from remaining + repair\n   - test_repair_20pct_corruption: Corrupt 20% of symbols, verify successful repair\n   - test_repair_exceeds_capacity: Corrupt >20% of symbols, verify graceful failure\n   - test_deterministic_symbols: Same input always produces identical repair symbols\n   - test_different_inputs_different_symbols: Different data produces different symbols\n   - test_empty_input: Zero-length source data handled correctly\n   - test_small_input: Source smaller than one symbol (edge case)\n   - test_large_input: 100MB source data (realistic index size)\n   - test_symbol_size_alignment: Non-aligned source padded correctly\n   - test_metrics_increment: Encode/decode counters update correctly\n\n2. repair_trailer_tests:\n   - test_write_read_fec_sidecar: Write .fec, read it back, verify contents\n   - test_fec_header_validation: Corrupt header magic, verify rejection\n   - test_fec_crc_validation: Corrupt CRC footer, verify rejection\n   - test_source_hash_verification: Modified source detected by hash mismatch\n   - test_fec_file_naming: .fsvi -> .fsvi.fec, .idx -> .idx.fec\n   - test_atomic_write: .fec written via temp + rename (crash-safe)\n\n3. file_protector_tests:\n   - test_protect_and_verify_intact: Protect file, verify returns Intact\n   - test_detect_single_bit_flip: Flip one bit in protected file, detect corruption\n   - test_repair_single_bit_flip: Flip one bit, repair successfully\n   - test_detect_zeroed_block: Zero out a 4KB block, detect corruption\n   - test_repair_zeroed_block: Zero out a 4KB block, repair successfully\n   - test_detect_appended_data: Extra bytes appended, detect corruption\n   - test_repair_multiple_blocks: Corrupt 3 non-adjacent blocks, repair all\n   - test_unprotected_file: verify() returns Unprotected when no .fec exists\n   - test_verify_on_open_config: Configurable verify-on-open behavior\n   - test_directory_protection: Protect all files in a directory\n   - test_directory_verification: Verify all protected files in a directory\n\n4. fsvi_protector_tests:\n   - test_protect_real_fsvi: Create a real FSVI file, protect it, verify\n   - test_corrupt_fsvi_header: Corrupt FSVI magic bytes, detect and repair\n   - test_corrupt_fsvi_vectors: Corrupt vector slab, detect and repair\n   - test_corrupt_fsvi_string_table: Corrupt doc IDs, detect and repair\n   - test_fsvi_open_with_auto_repair: VectorIndex::open() repairs corrupted file automatically\n   - test_fsvi_open_unrecoverable: Corruption beyond capacity triggers error\n\n5. tantivy_wrapper_tests:\n   - test_protect_tantivy_segments: Commit documents, verify segments protected\n   - test_corrupt_tantivy_postings: Corrupt .idx file, detect and repair\n   - test_corrupt_tantivy_store: Corrupt .store file, detect and repair\n   - test_post_merge_protection: After merge, new segment protected, old .fec cleaned up\n   - test_segment_health_report: Full report with per-segment status\n\n6. performance_tests:\n   - test_encode_throughput: Measure encode speed (expect > 100MB/s)\n   - test_decode_throughput: Measure decode speed (expect > 100MB/s)\n   - test_verify_fast_path: xxh3_64 verification (expect < 1ms per 100MB)\n   - test_repair_latency: Single-block repair (expect < 10ms)\n\nCORRUPTION SIMULATION UTILITIES:\n  fn corrupt_bytes(data: &mut [u8], offset: usize, count: usize) {\n      for i in offset..offset+count {\n          data[i] ^= 0xFF;  // Flip all bits in range\n      }\n  }\n\n  fn zero_block(data: &mut [u8], block_idx: usize, block_size: usize) {\n      let start = block_idx * block_size;\n      let end = (start + block_size).min(data.len());\n      data[start..end].fill(0);\n  }\n\n  fn random_corruption(data: &mut [u8], percent: f32, rng: &mut impl Rng) {\n      let count = (data.len() as f32 * percent / 100.0) as usize;\n      for _ in 0..count {\n          let idx = rng.gen_range(0..data.len());\n          data[idx] ^= rng.gen::<u8>();\n      }\n  }\n\nTRACING IN TESTS:\n  Every test verifies that appropriate log events are emitted:\n  - Protection: INFO \"file protected\" { path, source_size, repair_size, overhead_ratio }\n  - Verification: DEBUG \"file integrity check\" { path, result }\n  - Corruption: WARN \"corruption detected\" { path, corrupted_symbols, total_symbols }\n  - Repair: INFO \"file repaired\" { path, symbols_repaired, decode_time_ms }\n  - Failure: ERROR \"repair failed\" { path, reason }\n\nFile: frankensearch-durability/src/tests/ (inline #[cfg(test)] modules)\n","created_at":"2026-02-13T20:46:17Z"}]}
{"id":"bd-3w1.17","title":"Write integration tests for storage + search pipeline","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:39.177479366Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:54.264262819Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","integration","raptorq","testing"],"dependencies":[{"issue_id":"bd-3w1.17","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:46:54.264223285Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.17","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:39.177479366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.17","depends_on_id":"bd-3w1.13","type":"blocks","created_at":"2026-02-13T20:42:33.132719447Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.17","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:42:33.276104247Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":70,"issue_id":"bd-3w1.17","author":"Dicklesworthstone","text":"TASK: Write integration tests for the combined storage + search pipeline.\n\nThese tests verify the full end-to-end flow: document ingestion through FrankenSQLite, embedding, vector index creation with RaptorQ protection, and search with results.\n\nINTEGRATION TEST SCENARIOS:\n\n1. full_pipeline_test:\n   - Create Storage (in-memory FrankenSQLite)\n   - Ingest 100 documents via StorageBackedJobRunner\n   - Process all embedding jobs (hash embedder for CI)\n   - Build FSVI vector index from embeddings\n   - Protect index with RaptorQ\n   - Search with TwoTierSearcher\n   - Verify results match expected ground truth\n   - Verify all metrics counters are correct\n\n2. incremental_update_test:\n   - Ingest initial 50 documents\n   - Build index\n   - Ingest 20 more documents\n   - Check staleness -> reports 20 new docs\n   - Process incremental embedding jobs\n   - Update vector index (append new vectors)\n   - Search verifies new documents found\n\n3. content_change_detection_test:\n   - Ingest 50 documents\n   - Build index\n   - Update 10 documents with new content (same doc_ids, different text)\n   - Check dedup -> reports 10 changed\n   - Re-embed changed documents\n   - Search verifies updated content reflected\n\n4. crash_recovery_test:\n   - Ingest 50 documents\n   - Enqueue embedding jobs\n   - Claim a batch of 10 jobs (simulating worker start)\n   - \"Crash\" (drop the runner without completing)\n   - Create new runner with same Storage\n   - Call reclaim_stale_jobs()\n   - Verify 10 jobs reclaimed and reprocessed\n   - Verify no duplicate embeddings\n\n5. corruption_and_repair_test:\n   - Build FSVI index from 100 documents\n   - Protect with RaptorQ\n   - Corrupt 5% of the index file (random byte flips)\n   - Open the index (should detect corruption and auto-repair)\n   - Search produces correct results (identical to pre-corruption)\n\n6. fts5_and_tantivy_comparison_test:\n   - Index same 100 documents with both Tantivy AND FTS5\n   - Run identical queries against both\n   - Compare result sets (should be similar, not necessarily identical due to different tokenizers)\n   - Verify RRF fusion works with either lexical backend\n   - Log score distributions for analysis\n\n7. concurrent_ingest_and_search_test:\n   - Spawn 2 threads: one ingesting documents, one searching\n   - Verify search never blocks on ingest (MVCC)\n   - Verify search sees progressively more results as ingest proceeds\n   - No panics, no deadlocks, no data corruption\n\n8. two_tier_with_storage_test:\n   - Ingest 100 documents\n   - Build fast-tier index (hash embedder, 384d)\n   - Build quality-tier index (hash embedder, 384d -- same in CI, different in prod)\n   - Search via TwoTierSearcher\n   - Verify SearchPhase::Initial returns fast results\n   - Verify SearchPhase::Refined returns blended results\n   - Verify metrics (TwoTierMetrics) populated correctly\n\n9. durability_full_cycle_test:\n   - Ingest, embed, build all indices\n   - Protect all indices\n   - Verify all indices intact\n   - Corrupt vector index -> repair -> verify\n   - Corrupt Tantivy segment -> repair -> verify\n   - Search still produces correct results\n   - Verify repair events logged in index_metadata table\n\n10. storage_metrics_test:\n    - Run full pipeline\n    - Check all atomic counters: enqueued, completed, failed, skipped\n    - Check queue depth at various stages\n    - Check index build history\n    - Verify no metrics counter is zero (all paths exercised)\n\nSHARED TEST INFRASTRUCTURE:\n\n  /// Create a full test pipeline with in-memory storage and hash embedders\n  fn test_pipeline() -> (StorageBackedJobRunner, Storage, TwoTierSearcher) {\n      let storage = Storage::open_in_memory().unwrap();\n      let fast_embedder = Arc::new(HashEmbedder::default_256());\n      let quality_embedder = Arc::new(HashEmbedder::default_384());\n      // ... wire everything together ...\n  }\n\n  /// Generate test corpus: 100 documents with 5 clusters and known ground truth\n  fn test_corpus() -> Vec<(String, String)> {\n      // Reuses bd-3un.38 test fixture corpus\n      frankensearch_test_fixtures::corpus_100_5cluster()\n  }\n\nLOGGING:\n  All integration tests use tracing-subscriber with RUST_LOG=debug.\n  Key events logged:\n  - Pipeline stages: \"ingest_batch\" -> \"canonicalize\" -> \"hash_content\" -> \"enqueue_jobs\"\n  - Embedding: \"claim_batch\" -> \"embed\" -> \"write_vector\" -> \"complete_job\"\n  - Search: \"search_fast\" -> \"search_quality\" -> \"rrf_fuse\" -> \"return_results\"\n  - Durability: \"protect_index\" -> \"verify_index\" -> \"repair_index\"\n\nFile: tests/integration/storage_pipeline_test.rs\n","created_at":"2026-02-13T20:46:18Z"}]}
{"id":"bd-3w1.18","title":"Write e2e corruption-and-recovery test suite","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:44.025461125Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:55.666262621Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","e2e","raptorq","testing"],"dependencies":[{"issue_id":"bd-3w1.18","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:44.025461125Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.18","depends_on_id":"bd-3w1.17","type":"blocks","created_at":"2026-02-13T20:46:55.666225301Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.18","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:42:33.399117653Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":71,"issue_id":"bd-3w1.18","author":"Dicklesworthstone","text":"TASK: Write e2e corruption-and-recovery test suite.\n\nDedicated end-to-end tests that simulate real-world corruption scenarios and verify the self-healing pipeline recovers correctly. These tests are the \"proof that pervasive RaptorQ works.\"\n\nTEST SCENARIOS:\n\n1. power_loss_during_index_write:\n   - Begin writing FSVI vector index\n   - Simulate power loss mid-write (truncate file at random offset)\n   - Attempt to open the truncated file\n   - If .fec sidecar exists from previous build: repair and open\n   - If no .fec: detect corruption, trigger full rebuild from FrankenSQLite\n\n2. bit_rot_simulation:\n   - Create and protect a complete search index (FSVI + Tantivy)\n   - Simulate gradual bit rot: flip 1 random bit every \"day\" for 100 \"days\"\n   - After each day, verify and repair if needed\n   - Count how many days of bit rot the index survives (should be >95 with 20% overhead)\n   - Log repair events with timestamps\n\n3. storage_medium_failure:\n   - Create indices on disk\n   - Zero out a random 4KB block (simulating a bad sector)\n   - Verify detection and repair\n   - Zero out 10 random 4KB blocks (simulating multiple bad sectors)\n   - Verify detection and repair (should succeed up to 20% of file)\n   - Zero out 25% of file (exceeds repair capacity)\n   - Verify graceful failure with clear error message\n\n4. concurrent_corruption_and_search:\n   - Start search workload (continuous queries)\n   - In background, corrupt 1 vector index block\n   - Verify search detects corruption, repairs, and retries automatically\n   - Verify search results are correct after repair\n   - Verify no search queries return wrong results during corruption\n\n5. cascading_corruption:\n   - Corrupt FSVI vector index (self-heals from .fec sidecar)\n   - Then corrupt the .fec sidecar itself\n   - Then corrupt the FrankenSQLite WAL\n   - Verify FrankenSQLite's own WAL-FEC repairs the WAL\n   - Verify the document store (FrankenSQLite) can rebuild the vector index\n   - This tests the \"defense in depth\" property of pervasive RaptorQ\n\n6. full_rebuild_from_storage:\n   - Create complete search setup (storage + indices)\n   - Delete ALL index files (FSVI, Tantivy, .fec sidecars)\n   - Verify the system detects missing indices\n   - Trigger full rebuild from FrankenSQLite document store\n   - Verify rebuilt indices produce identical search results\n   - This proves FrankenSQLite IS the source of truth\n\n7. fec_sidecar_corruption:\n   - Protect an index, producing .fec sidecar\n   - Corrupt the .fec sidecar (not the index)\n   - Verify that verification still detects this (CRC footer check)\n   - Regenerate .fec from the intact index\n   - Verify new .fec is identical to original (deterministic repair symbols)\n\n8. partial_index_recovery:\n   - Write 200 vectors to FSVI, protect\n   - Corrupt the last 50 vectors (tail of file)\n   - Repair: only the corrupted portion is regenerated\n   - Verify: first 150 vectors unchanged, last 50 recovered\n   - This tests that repair is surgical, not a full rewrite\n\nVALIDATION FRAMEWORK:\n\n  /// Verify that a search index produces correct results after repair\n  fn verify_search_integrity(\n      searcher: &TwoTierSearcher,\n      queries: &[(String, Vec<String>)],  // (query, expected_doc_ids)\n  ) -> Vec<IntegrityCheckResult> {\n      queries.iter().map(|(query, expected)| {\n          let results = searcher.search_fast_only(query, expected.len());\n          let actual_ids: Vec<_> = results.iter().map(|r| r.doc_id.clone()).collect();\n          IntegrityCheckResult {\n              query: query.clone(),\n              expected_count: expected.len(),\n              actual_count: actual_ids.len(),\n              recall: compute_recall(&actual_ids, expected),\n              passed: compute_recall(&actual_ids, expected) >= 0.9,\n          }\n      }).collect()\n  }\n\nLOGGING (COLORIZED):\n  Every test phase logs with clear headers:\n  tracing::info!(\"=== PHASE 1: Create and protect indices ===\");\n  tracing::info!(\"=== PHASE 2: Simulate corruption ===\");\n  tracing::info!(\"=== PHASE 3: Detect and repair ===\");\n  tracing::info!(\"=== PHASE 4: Verify search integrity ===\");\n\n  Each phase includes timing and byte-level details:\n  tracing::info!(\n      phase = \"corruption\",\n      file = %path.display(),\n      offset = corrupted_offset,\n      bytes = corrupted_bytes,\n      \"simulated corruption injected\"\n  );\n\nFile: tests/e2e/corruption_recovery_test.rs\n","created_at":"2026-02-13T20:46:18Z"}]}
{"id":"bd-3w1.19","title":"Design Native Mode integration for distributed search (future)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:37:44.997509958Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:18.724629452Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","future","native-mode","tier4"],"dependencies":[{"issue_id":"bd-3w1.19","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:44.997509958Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":72,"issue_id":"bd-3w1.19","author":"Dicklesworthstone","text":"TASK: Design the Native Mode integration for distributed search (future architecture).\n\nThis is a DESIGN-ONLY bead -- no implementation, just architecture documentation for how frankensearch could leverage FrankenSQLite's Native Mode in the future.\n\nNATIVE MODE OVERVIEW:\nFrankenSQLite's Native Mode replaces the traditional SQLite file with an append-only ECS (Erasure-Coded Stream) commit stream. Every mutation is a CommitCapsule: a content-addressed, erasure-coded object identified by a BLAKE3-derived ObjectId.\n\nDISTRIBUTED SEARCH ARCHITECTURE:\n\n  1. ECS Commit Stream for Index Updates:\n     - Each document ingestion becomes a CommitCapsule\n     - CommitCapsules replicate across nodes via rateless coding\n     - Each node rebuilds its local search indices from the commit stream\n     - Index state is fully deterministic from the commit history\n\n  2. Snapshot Shipping for New Nodes:\n     - New search nodes receive a snapshot via RaptorQ-encoded transfer\n     - Bandwidth-optimal: any K of N received symbols suffice for reconstruction\n     - No coordination required: sender transmits fountain of symbols, receiver collects K\n     - Over lossy networks (unreliable UDP), this is dramatically more efficient than TCP\n\n  3. Time-Travel Queries:\n     - Query the search index as it existed at any historical commit point\n     - Use case: \"what would this query have returned yesterday?\"\n     - Implementation: maintain commit-indexed checkpoints of the search state\n     - FrankenSQLite's MVCC visibility check: V.commit_seq <= S.high\n\n  4. Quorum Durability for Search Indices:\n     - PRAGMA durability = quorum(M): index data durable across M of N replicas\n     - Each replica maintains its own local FSVI + Tantivy indices\n     - CommitMarkers (not capsules) determine commit finality\n     - Replicas can independently verify each other's indices via deterministic repair symbols\n\nEXAMPLE: 3-Node Search Cluster\n\n  Node A: Primary writer, ingests documents\n    -> CommitCapsule{doc_id: \"d1\", text: \"hello world\"} with ObjectId=BLAKE3(...)\n    -> Encodes capsule into K source symbols + R repair symbols\n    -> Streams symbols to Nodes B and C\n\n  Node B: Replica\n    -> Receives symbols, decodes CommitCapsule\n    -> Applies capsule: inserts doc into local FrankenSQLite\n    -> Triggers embedding pipeline (local embedder)\n    -> Updates local FSVI + Tantivy indices\n    -> Search queries served from local indices (zero network latency)\n\n  Node C: Replica (same as B)\n\n  Result: Each node has identical search indices, built from the same commit stream,\n  with independent RaptorQ protection. If any single node's storage fails, the other\n  two can reconstruct it from their repair symbols + commit stream.\n\nINCREMENTAL INDEX REPLICATION (ALTERNATIVE):\n  Instead of each node rebuilding indices from raw documents, share the indices directly:\n  - After index build on primary, encode the FSVI file as RaptorQ symbols\n  - Ship symbols to replicas (rateless: any K symbols suffice)\n  - Replicas decode to get identical FSVI files\n  - This is faster than re-embedding (avoids ML inference on each node)\n  - Trade-off: requires more network bandwidth but saves compute\n\nWHY THIS IS TIER 4 (FUTURE):\n  1. FrankenSQLite Native Mode is Phase 6+ (not yet wired to Connection)\n  2. Distributed search requires network protocol design\n  3. The commit stream -> index rebuild pipeline needs careful ordering guarantees\n  4. Single-node frankensearch (Tiers 1-3) is the priority\n\n  But the architecture is designed FROM THE START to be distribution-ready:\n  - FrankenSQLite as source of truth (not the index files)\n  - Deterministic repair symbols (replicas can cross-verify)\n  - Content-addressed objects (ObjectId for cache coherence)\n  - Transactional consistency (MVCC snapshots for consistent reads)\n\nNO DEPENDENCIES: This bead blocks nothing and is blocked by everything.\nIt's a north-star design document for the project's long-term vision.\n\nFile: docs/architecture/native-mode-distributed-search.md (design doc, not code)\n","created_at":"2026-02-13T20:46:18Z"}]}
{"id":"bd-3w1.2","title":"Implement document metadata schema and CRUD operations","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:12.542167228Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:08.997593397Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","schema","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.2","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:12.542167228Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.2","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.599373073Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":55,"issue_id":"bd-3w1.2","author":"Dicklesworthstone","text":"TASK: Implement the SQL schema and CRUD operations for document metadata storage.\n\nThis is the core of Tier 1 integration. Every document indexed by frankensearch gets a row in FrankenSQLite that tracks its metadata, embedding status, and content fingerprint.\n\nSQL SCHEMA:\n\n  CREATE TABLE IF NOT EXISTS documents (\n      doc_id TEXT PRIMARY KEY,          -- Caller-provided unique ID\n      source_path TEXT,                 -- Optional filesystem path or URL\n      content_preview TEXT,             -- First 400 chars (for result snippets without re-reading source)\n      content_hash BLOB NOT NULL,       -- SHA-256 of canonicalized text (32 bytes)\n      content_length INTEGER NOT NULL,  -- Original text length in bytes\n      created_at INTEGER NOT NULL,      -- Unix timestamp millis\n      updated_at INTEGER NOT NULL,      -- Unix timestamp millis (last content change)\n      metadata_json TEXT                -- Arbitrary JSON metadata from caller\n  );\n\n  CREATE TABLE IF NOT EXISTS embedding_status (\n      doc_id TEXT NOT NULL REFERENCES documents(doc_id) ON DELETE CASCADE,\n      embedder_id TEXT NOT NULL,        -- e.g., \"potion-multilingual-128M\" or \"all-MiniLM-L6-v2\"\n      embedder_revision TEXT,           -- Model commit SHA for staleness detection\n      status TEXT NOT NULL DEFAULT 'pending',  -- pending | embedded | failed | skipped\n      embedded_at INTEGER,             -- When embedding was computed\n      error_message TEXT,              -- If status='failed', why\n      retry_count INTEGER DEFAULT 0,\n      PRIMARY KEY (doc_id, embedder_id)\n  );\n\n  CREATE INDEX idx_embedding_status_pending\n      ON embedding_status(status) WHERE status = 'pending';\n  CREATE INDEX idx_documents_content_hash\n      ON documents(content_hash);\n  CREATE INDEX idx_documents_updated_at\n      ON documents(updated_at);\n\nCRUD API:\n\n  pub struct DocumentRecord {\n      pub doc_id: String,\n      pub source_path: Option<String>,\n      pub content_preview: String,\n      pub content_hash: [u8; 32],\n      pub content_length: usize,\n      pub created_at: i64,\n      pub updated_at: i64,\n      pub metadata: Option<serde_json::Value>,\n  }\n\n  impl Storage {\n      /// Upsert a document. If doc_id exists with same content_hash, no-op (returns false).\n      /// If doc_id exists with different content_hash, updates and resets embedding status.\n      pub fn upsert_document(&self, doc: &DocumentRecord) -> SearchResult<bool>;\n\n      /// Get document by ID\n      pub fn get_document(&self, doc_id: &str) -> SearchResult<Option<DocumentRecord>>;\n\n      /// List documents that need embedding for a given embedder\n      pub fn list_pending_embeddings(&self, embedder_id: &str, limit: usize) -> SearchResult<Vec<String>>;\n\n      /// Mark embedding as completed for a doc/embedder pair\n      pub fn mark_embedded(&self, doc_id: &str, embedder_id: &str) -> SearchResult<()>;\n\n      /// Mark embedding as failed with error message\n      pub fn mark_failed(&self, doc_id: &str, embedder_id: &str, error: &str) -> SearchResult<()>;\n\n      /// Count documents by embedding status\n      pub fn count_by_status(&self, embedder_id: &str) -> SearchResult<StatusCounts>;\n\n      /// Delete document and cascade to embedding_status\n      pub fn delete_document(&self, doc_id: &str) -> SearchResult<bool>;\n\n      /// Batch upsert (uses FrankenSQLite transaction for atomicity)\n      pub fn upsert_batch(&self, docs: &[DocumentRecord]) -> SearchResult<BatchResult>;\n  }\n\nDESIGN RATIONALE:\n1. content_hash as BLOB (not TEXT) -- 32 bytes raw vs 64 hex chars, faster comparison\n2. Separate embedding_status table -- supports multi-tier embedding (fast + quality have different status)\n3. Partial index on status='pending' -- only pending items need scanning, keeps index small\n4. content_preview stored here -- enables result snippets without re-reading full content from external source\n5. CASCADE delete -- removing a document auto-removes all embedding status rows\n6. FrankenSQLite MVCC -- upsert_batch can run in a transaction while search queries read concurrently\n\nWHY THIS MATTERS:\nWithout persistent metadata, frankensearch has no way to:\n- Know which documents have been embedded by which tier\n- Detect content changes (content_hash comparison)\n- Resume after crash (all in-memory state lost)\n- Track embedding failures for retry\nThis table is the single source of truth for the indexing pipeline.\n\nFile: frankensearch-storage/src/document.rs\n","created_at":"2026-02-13T20:46:08Z"}]}
{"id":"bd-3w1.20","title":"Add durability benchmarks (encode/decode throughput, repair latency)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:37:45.981449400Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:20.456127631Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["benchmarks","performance","raptorq"],"dependencies":[{"issue_id":"bd-3w1.20","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:45.981449400Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.20","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:34.612703593Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.20","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:42:34.734313421Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":73,"issue_id":"bd-3w1.20","author":"Dicklesworthstone","text":"TASK: Add benchmarks for RaptorQ durability operations.\n\nMeasure encode/decode throughput, repair latency, and overhead for realistic index sizes.\n\nBENCHMARK SCENARIOS:\n\n  1. encode_throughput:\n     - Source sizes: 1MB, 10MB, 50MB, 100MB, 500MB\n     - Symbol size: 4KB (default)\n     - Overhead: 20% (default)\n     - Measure: MB/s encoding throughput\n     - Expected: > 200MB/s (RaptorQ is compute-bound, scales with CPU)\n\n  2. decode_throughput:\n     - Same sizes as encode\n     - Decode from source symbols only (no corruption, fast path)\n     - Decode from source + repair symbols (simulated corruption)\n     - Measure: MB/s decoding throughput\n     - Expected: > 150MB/s for clean decode, > 100MB/s for repair decode\n\n  3. verify_fast_path:\n     - Source sizes: 1MB, 10MB, 50MB, 100MB, 500MB\n     - Measure: xxh3_64 verification time (the fast path before RaptorQ)\n     - Expected: > 5GB/s (xxh3 is extremely fast, memory-bandwidth-limited)\n\n  4. repair_latency_by_corruption:\n     - 50MB source, 20% overhead\n     - Corruption levels: 1 block, 5 blocks, 1%, 5%, 10%, 20%\n     - Measure: repair latency for each corruption level\n     - Expected: < 50ms for 1 block, < 500ms for 20% corruption\n\n  5. overhead_vs_protection:\n     - 50MB source\n     - Overhead levels: 5%, 10%, 20%, 30%, 50%\n     - Measure: .fec file size, encode time, max repairable corruption\n     - This shows the trade-off curve: more overhead = more protection = larger .fec\n\n  6. realistic_index_encode:\n     - Create a real FSVI index with 10K, 50K, 100K documents (384 dimensions, f16)\n     - Measure: time to protect the index (encode + write .fec)\n     - This benchmarks the actual production workload\n     - Expected: < 1s for 10K docs, < 5s for 100K docs\n\n  7. concurrent_verify:\n     - 4 threads verifying 4 different protected files simultaneously\n     - Measure: per-thread throughput vs single-thread\n     - Expected: near-linear scaling (verification is memory-bandwidth-bound, not CPU-bound)\n\nBENCHMARK INFRASTRUCTURE:\n\n  Use criterion for statistical benchmarks:\n\n  fn bench_encode(c: &mut Criterion) {\n      let codec = RepairCodec::new(RepairCodecConfig::default()).unwrap();\n      let data = vec![42u8; 50 * 1024 * 1024]; // 50MB\n\n      c.bench_function(\"encode_50mb\", |b| {\n          b.iter(|| codec.encode(black_box(&data)))\n      });\n  }\n\n  fn bench_verify_fast_path(c: &mut Criterion) {\n      let data = vec![42u8; 100 * 1024 * 1024]; // 100MB\n      let expected_hash = xxh3_64(&data);\n\n      c.bench_function(\"verify_fast_100mb\", |b| {\n          b.iter(|| {\n              let hash = xxh3_64(black_box(&data));\n              assert_eq!(hash, expected_hash);\n          })\n      });\n  }\n\nPERFORMANCE BUDGET (HARD LIMITS):\n  These are budgets that should cause CI failure if exceeded:\n  - Encode 50MB: < 2 seconds (25 MB/s minimum)\n  - Decode 50MB (clean): < 2 seconds\n  - Verify 100MB (fast path): < 50ms\n  - Repair 1 block (4KB): < 10ms\n  - .fec overhead at 20%: within [19%, 21%] of source size\n\nFile: benches/durability_bench.rs\n","created_at":"2026-02-13T20:46:20Z"}]}
{"id":"bd-3w1.21","title":"Update facade crate re-exports for storage and durability APIs","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:47.199892592Z","created_by":"ubuntu","updated_at":"2026-02-13T20:47:01.839374836Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","facade","frankensqlite","raptorq"],"dependencies":[{"issue_id":"bd-3w1.21","depends_on_id":"bd-3un.30","type":"blocks","created_at":"2026-02-13T20:42:34.980116503Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:47.199892592Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1.13","type":"blocks","created_at":"2026-02-13T20:47:01.752611149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1.14","type":"blocks","created_at":"2026-02-13T20:42:34.857532702Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.21","depends_on_id":"bd-3w1.9","type":"blocks","created_at":"2026-02-13T20:47:01.839319603Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":74,"issue_id":"bd-3w1.21","author":"Dicklesworthstone","text":"TASK: Update the facade crate re-exports for storage and durability APIs.\n\nExtend the frankensearch facade crate (bd-3un.30) to re-export the new storage and durability public APIs, so consumers can access everything through a single import.\n\nUPDATED RE-EXPORTS:\n\n  // frankensearch/src/lib.rs\n\n  // ... existing re-exports from bd-3un.30 ...\n\n  // Storage (feature-gated)\n  #[cfg(feature = \"storage\")]\n  pub use frankensearch_storage::{\n      Storage, StorageConfig,\n      DocumentRecord, BatchResult,\n      PersistentJobQueue, JobQueueConfig, JobQueueMetrics,\n      ContentHasher, DeduplicationDecision,\n      IndexMetadata, StalenessCheck, StalenessReason,\n      StorageBackedJobRunner, IngestResult, IngestAction,\n  };\n\n  // FTS5 (feature-gated, requires storage)\n  #[cfg(feature = \"fts5\")]\n  pub use frankensearch_storage::{\n      Fts5LexicalIndex, Fts5Config, Fts5Tokenizer, Fts5ContentMode,\n  };\n\n  // Durability (feature-gated)\n  #[cfg(feature = \"durability\")]\n  pub use frankensearch_durability::{\n      RepairCodec, RepairCodecConfig, DurabilityMetrics,\n      FileProtector, FileProtectorConfig,\n      FsviProtector, ProtectionResult, RepairResult,\n      VerifyResult, FileHealth,\n  };\n\nERGONOMIC AUTO-CONFIGURATION:\n\n  The TwoTierSearcher::auto() constructor should detect available features:\n\n  impl TwoTierSearcher {\n      pub fn auto(data_dir: &Path) -> SearchResult<Self> {\n          // If 'storage' feature enabled:\n          //   Open FrankenSQLite database at data_dir/frankensearch.db\n          //   Use persistent job queue\n          //   Use storage-backed staleness detection\n          //\n          // If 'durability' feature enabled:\n          //   Create FileProtector with default config\n          //   Verify indices on load\n          //   Protect indices after build\n          //\n          // If 'fts5' feature enabled AND 'lexical' not enabled:\n          //   Use FTS5 as the lexical engine\n          //\n          // If both 'fts5' and 'lexical' enabled:\n          //   Use Tantivy as primary, FTS5 as fallback\n\n          // ... auto-detect embedders, build config ...\n      }\n  }\n\nDOCUMENTATION ADDITIONS:\n  The facade crate's doc comments should explain:\n  1. Feature flag selection guide (which features for which use case)\n  2. Storage: what it provides and when to use it\n  3. Durability: what it protects and the overhead cost\n  4. FTS5 vs Tantivy: trade-offs and when to use each\n\nFile: frankensearch/src/lib.rs (update to bd-3un.30)\n","created_at":"2026-02-13T20:46:20Z"}]}
{"id":"bd-3w1.3","title":"Implement persistent embedding job queue in FrankenSQLite","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:14.063817140Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:28.987143739Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["frankensqlite","queue","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.3","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:14.063817140Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.3","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.716973335Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.3","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:28.700934728Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":56,"issue_id":"bd-3w1.3","author":"Dicklesworthstone","text":"TASK: Implement a persistent, crash-safe embedding job queue backed by FrankenSQLite.\n\nThis replaces the in-memory Mutex-based EmbeddingQueue (bd-3un.27) with a durable queue that survives process restarts and crashes. The in-memory queue from bd-3un.27 becomes a thin wrapper that delegates to this persistent backend.\n\nSQL SCHEMA:\n\n  CREATE TABLE IF NOT EXISTS embedding_jobs (\n      job_id INTEGER PRIMARY KEY AUTOINCREMENT,\n      doc_id TEXT NOT NULL,\n      embedder_id TEXT NOT NULL,        -- Which embedder to use\n      priority INTEGER NOT NULL DEFAULT 0,  -- Higher = process first\n      submitted_at INTEGER NOT NULL,    -- Unix timestamp millis\n      started_at INTEGER,               -- When a worker picked it up\n      completed_at INTEGER,             -- When processing finished\n      status TEXT NOT NULL DEFAULT 'pending',  -- pending | processing | completed | failed | skipped\n      retry_count INTEGER NOT NULL DEFAULT 0,\n      max_retries INTEGER NOT NULL DEFAULT 3,\n      error_message TEXT,\n      content_hash BLOB,               -- SHA-256 for dedup check\n      worker_id TEXT,                   -- Which worker claimed this job (for deadlock detection)\n      UNIQUE(doc_id, embedder_id, status)  -- Prevent duplicate pending jobs\n  );\n\n  CREATE INDEX idx_jobs_pending\n      ON embedding_jobs(status, priority DESC, submitted_at ASC)\n      WHERE status = 'pending';\n  CREATE INDEX idx_jobs_processing\n      ON embedding_jobs(status, started_at)\n      WHERE status = 'processing';\n\nQUEUE API:\n\n  pub struct PersistentJobQueue {\n      storage: Arc<Storage>,\n      config: JobQueueConfig,\n      metrics: Arc<JobQueueMetrics>,\n  }\n\n  pub struct JobQueueConfig {\n      pub batch_size: usize,                  // Default: 32\n      pub visibility_timeout_ms: u64,         // Default: 30_000 (30s)\n      pub max_retries: u32,                   // Default: 3\n      pub retry_base_delay_ms: u64,           // Default: 100\n      pub stale_job_threshold_ms: u64,        // Default: 300_000 (5min)\n      pub backpressure_threshold: usize,      // Default: 10_000 pending jobs\n  }\n\n  pub struct JobQueueMetrics {\n      pub total_enqueued: AtomicU64,\n      pub total_completed: AtomicU64,\n      pub total_failed: AtomicU64,\n      pub total_skipped: AtomicU64,\n      pub total_retried: AtomicU64,\n      pub total_deduplicated: AtomicU64,\n      pub total_batches_processed: AtomicU64,\n      pub total_embed_time_us: AtomicU64,\n  }\n\n  impl PersistentJobQueue {\n      /// Enqueue a document for embedding. Deduplicates by (doc_id, embedder_id).\n      /// If a pending job exists with same content_hash, returns Ok(false) (skipped).\n      /// If a pending job exists with different content_hash, replaces it.\n      pub fn enqueue(&self, doc_id: &str, embedder_id: &str, content_hash: &[u8; 32], priority: i32) -> SearchResult<bool>;\n\n      /// Enqueue a batch atomically (single transaction)\n      pub fn enqueue_batch(&self, jobs: &[EnqueueRequest]) -> SearchResult<BatchEnqueueResult>;\n\n      /// Claim a batch of pending jobs (atomic: sets status='processing', records worker_id)\n      /// Uses SELECT ... LIMIT batch_size with immediate status update in same transaction\n      pub fn claim_batch(&self, worker_id: &str, batch_size: usize) -> SearchResult<Vec<ClaimedJob>>;\n\n      /// Mark job as completed (sets status, completed_at)\n      pub fn complete(&self, job_id: i64) -> SearchResult<()>;\n\n      /// Mark job as failed (increments retry_count, requeues if under max_retries)\n      pub fn fail(&self, job_id: i64, error: &str) -> SearchResult<FailResult>;\n\n      /// Mark job as skipped (low-signal content after canonicalization)\n      pub fn skip(&self, job_id: i64, reason: &str) -> SearchResult<()>;\n\n      /// Reclaim stale 'processing' jobs (worker died without completing)\n      /// Jobs processing for > visibility_timeout_ms get reset to 'pending'\n      pub fn reclaim_stale_jobs(&self) -> SearchResult<usize>;\n\n      /// Check backpressure (returns true if queue depth exceeds threshold)\n      pub fn is_backpressured(&self) -> SearchResult<bool>;\n\n      /// Get queue depth by status\n      pub fn queue_depth(&self) -> SearchResult<QueueDepth>;\n\n      /// Get metrics snapshot\n      pub fn metrics(&self) -> &JobQueueMetrics;\n  }\n\nVISIBILITY TIMEOUT PATTERN (from SQS/cloud queues):\nWhen a worker claims a job, it becomes invisible to other workers for visibility_timeout_ms.\nIf the worker dies without completing/failing the job, reclaim_stale_jobs() resets it to pending.\nThis prevents lost work without requiring distributed locks.\n\nclaim_batch() SQL:\n  UPDATE embedding_jobs\n  SET status = 'processing', started_at = ?now, worker_id = ?worker\n  WHERE job_id IN (\n      SELECT job_id FROM embedding_jobs\n      WHERE status = 'pending'\n      ORDER BY priority DESC, submitted_at ASC\n      LIMIT ?batch_size\n  )\n  RETURNING job_id, doc_id, embedder_id;\n\nThis is atomic in FrankenSQLite -- MVCC ensures concurrent workers get disjoint batches.\n\nEXPONENTIAL BACKOFF ON FAILURE:\n  next_retry_delay = min(retry_base_delay_ms * 2^retry_count, 30_000ms)\n  Jobs that exceed max_retries stay in 'failed' status for manual inspection.\n  From agent-mail embedding_jobs.rs: backoff shift cap at 20 (2^20 * 100ms = ~105s max).\n\nHASH-ONLY SKIP (from agent-mail):\n  When embedder_id is the hash embedder (\"fnv1a-*\"), skip the job entirely.\n  Hash embeddings are computed on-the-fly during search and don't need to be stored.\n  This prevents wasted I/O for the always-available fallback embedder.\n\nDEDUP LOGIC:\n  On enqueue, check if (doc_id, embedder_id) already has a pending/processing job:\n  - Same content_hash: skip (content hasn't changed)\n  - Different content_hash: cancel old job, create new one\n  - No existing job: create new one\n  This integrates with the content_hash dedup from bd-3w1.4.\n\nWHY PERSISTENT QUEUE OVER IN-MEMORY:\n1. Crash recovery: pending jobs survive process restart\n2. Multi-process: multiple workers can claim from same queue via MVCC\n3. Observability: queue depth, failure rates queryable via SQL\n4. Backpressure: count(*) where status='pending' is O(1) with partial index\n5. Dedup: UNIQUE constraint prevents duplicate work at the database level\n\nFile: frankensearch-storage/src/job_queue.rs\n","created_at":"2026-02-13T20:46:09Z"},{"id":77,"issue_id":"bd-3w1.3","author":"Dicklesworthstone","text":"RELATIONSHIP NOTE: Persistent vs In-Memory Job Queue\n\nThis bead (bd-3w1.3) extends the in-memory embedding job queue from bd-3un.27\nwith FrankenSQLite-backed persistence. The relationship is:\n\n- bd-3un.27: In-memory queue with crossbeam channels, backpressure, AIMD rate control\n  - Jobs lost on process crash\n  - Sufficient for real-time indexing of small batches\n\n- bd-3w1.3: Persistent queue backed by FrankenSQLite\n  - Jobs survive process restarts\n  - Supports bulk import (100K+ docs)\n  - Crash recovery: resume from last committed position\n  - Required for production deployments\n\nImplementation approach: bd-3w1.3 should implement the SAME trait interface as\nbd-3un.27 (trait EmbeddingJobQueue) but with SQLite storage. The refresh worker\n(bd-3un.28) should accept any impl EmbeddingJobQueue, making storage backend\nswappable via feature flags.\n","created_at":"2026-02-13T20:46:28Z"}]}
{"id":"bd-3w1.4","title":"Implement content-hash deduplication layer","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:15.412628486Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:28.895908811Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["dedup","frankensqlite","storage","tier1"],"dependencies":[{"issue_id":"bd-3w1.4","depends_on_id":"bd-3un.42","type":"blocks","created_at":"2026-02-13T20:46:27.305310381Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.4","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:15.412628486Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.4","depends_on_id":"bd-3w1.1","type":"blocks","created_at":"2026-02-13T20:42:25.835698874Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.4","depends_on_id":"bd-3w1.2","type":"blocks","created_at":"2026-02-13T20:46:28.773830125Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":57,"issue_id":"bd-3w1.4","author":"Dicklesworthstone","text":"TASK: Implement content-hash based deduplication for embedding pipeline.\n\nDocuments are identified by (doc_id, content_hash) where content_hash = SHA-256 of the canonicalized text. This layer prevents re-embedding when content hasn't changed, and detects when content has changed and needs re-embedding.\n\nDESIGN:\n\n  pub struct ContentHasher {\n      // Stateless -- just wraps SHA-256\n  }\n\n  impl ContentHasher {\n      /// Compute SHA-256 of canonicalized text\n      pub fn hash(canonical_text: &str) -> [u8; 32];\n\n      /// Compare two hashes\n      pub fn matches(a: &[u8; 32], b: &[u8; 32]) -> bool;\n  }\n\n  pub enum DeduplicationDecision {\n      /// Content unchanged, skip embedding entirely\n      Skip { doc_id: String, reason: &'static str },\n      /// New document, needs embedding\n      New { doc_id: String },\n      /// Content changed, needs re-embedding (old embedding invalidated)\n      Changed { doc_id: String, old_hash: [u8; 32], new_hash: [u8; 32] },\n  }\n\n  impl Storage {\n      /// Check if a document needs (re-)embedding by comparing content hashes\n      pub fn check_dedup(&self, doc_id: &str, new_hash: &[u8; 32], embedder_id: &str) -> SearchResult<DeduplicationDecision>;\n\n      /// Batch check for dedup (single query, much faster than N individual checks)\n      pub fn check_dedup_batch(&self, items: &[(String, [u8; 32])], embedder_id: &str) -> SearchResult<Vec<DeduplicationDecision>>;\n  }\n\nINTEGRATION WITH CANONICALIZATION (bd-3un.42):\n  The dedup pipeline is:\n  1. Canonicalize raw text (NFC, markdown strip, code collapse, whitespace normalize)\n  2. If canonical text is empty -> skip (low-signal content)\n  3. Compute SHA-256 of canonical text\n  4. Check dedup against storage: skip / new / changed\n  5. Only if new/changed: actually embed the text\n\n  This means content_hash is computed AFTER canonicalization but BEFORE embedding.\n  The hash represents the exact text that was embedded, so identical canonical forms\n  always produce the same hash regardless of original formatting.\n\nBATCH DEDUP SQL:\n  -- Check which doc_ids need embedding\n  SELECT d.doc_id, d.content_hash, es.status\n  FROM documents d\n  LEFT JOIN embedding_status es ON d.doc_id = es.doc_id AND es.embedder_id = ?embedder\n  WHERE d.doc_id IN (?, ?, ?, ...)\n  -- Then compare content_hash in application code\n\nPERFORMANCE:\n  - SHA-256: ~400MB/s on modern CPUs (sha2 crate with hardware acceleration)\n  - For 10K documents with 2KB average canonical text: ~50ms total hash time\n  - Batch SQL query: single round-trip regardless of batch size\n  - Net savings: skip re-embedding unchanged docs (128ms per MiniLM embed)\n\nWHY SHA-256 (not xxhash or FNV):\n  - Collision resistance matters here: two different texts producing the same hash\n    means we'd skip embedding, silently serving stale results\n  - SHA-256 has 128-bit collision resistance (birthday bound), vs 32-bit for CRC32\n  - Performance is not the bottleneck: hashing is 1000x faster than embedding\n  - Consistency with FrankenSQLite: ECS objects use BLAKE3 for content addressing,\n    SHA-256 is similarly cryptographic-grade\n\nFile: frankensearch-storage/src/content_hash.rs\n","created_at":"2026-02-13T20:46:09Z"},{"id":76,"issue_id":"bd-3w1.4","author":"Dicklesworthstone","text":"DEPENDENCY NOTE: Content Hash and Canonicalization\n\nbd-3w1.4 (content-hash deduplication) should use the same SHA-256 content hashing\nas bd-3un.42 (text canonicalization pipeline). The canonicalization must happen\nBEFORE hashing for dedup to work correctly:\n\n  canonical_text = canonicalize(raw_text)\n  content_hash = sha256(canonical_text)\n\nWithout canonicalization first, trivially different versions of the same content\n(different whitespace, markdown formatting, etc.) would produce different hashes\nand bypass dedup.\n\nThe dependency bd-3w1.4 -> bd-3un.42 ensures the canonicalization pipeline is\navailable when the dedup layer is implemented.\n","created_at":"2026-02-13T20:46:28Z"}]}
{"id":"bd-3w1.5","title":"Add frankensearch-durability crate for RaptorQ integration","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:16.305906544Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:30.156967859Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","scaffold","tier2"],"dependencies":[{"issue_id":"bd-3w1.5","depends_on_id":"bd-3un.1","type":"blocks","created_at":"2026-02-13T20:46:30.065403534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.5","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:46:30.156928715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.5","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:16.305906544Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":58,"issue_id":"bd-3w1.5","author":"Dicklesworthstone","text":"TASK: Create the frankensearch-durability sub-crate.\n\nThis crate provides the RaptorQ integration layer for self-healing search indices. It wraps FrankenSQLite's SymbolCodec trait and provides high-level APIs for adding repair symbols to arbitrary binary files (FSVI, Tantivy segments, etc.).\n\nCRATE STRUCTURE:\n  crates/frankensearch-durability/\n    Cargo.toml\n    src/\n      lib.rs              -- Public API re-exports\n      codec.rs            -- RaptorQ codec wrapper (wraps fsqlite-core SymbolCodec)\n      repair_trailer.rs   -- Binary trailer format for repair symbols appended to files\n      file_protector.rs   -- High-level API: protect/verify/repair a file\n      tantivy_wrapper.rs  -- Tantivy-specific segment protection\n      metrics.rs          -- Durability metrics (encode/decode counts, repair events)\n      config.rs           -- Durability configuration\n\nCARGO.TOML DEPENDENCIES:\n  [dependencies]\n  frankensearch-core = { path = \"../frankensearch-core\" }\n  fsqlite-core = { path = \"/dp/frankensqlite/crates/fsqlite-core\" }  # For SymbolCodec, RaptorQMetrics\n  xxhash-rust = { version = \"0.8\", features = [\"xxh3\"] }  # Fast checksums for corruption detection\n  crc32fast = \"1.4\"                     # CRC-32 for header validation\n  tracing = \"0.1\"\n  serde = { version = \"1\", features = [\"derive\"] }\n\nFEATURE FLAG:\n  The durability crate is feature-gated:\n  [features]\n  durability = [\"dep:frankensearch-durability\"]\n\n  Consumers who don't want erasure coding overhead can omit this feature.\n  When disabled, all durability operations become no-ops (trait provides defaults).\n\nDESIGN RATIONALE:\n  We reuse FrankenSQLite's SymbolCodec trait rather than implementing our own RaptorQ.\n  This gives us:\n  1. Battle-tested encode/decode implementation\n  2. Deterministic repair symbols (same seed = same symbols)\n  3. Configurable overhead (DEFAULT_OVERHEAD_PERCENT = 20%)\n  4. Metrics integration (RaptorQMetrics with atomic counters)\n  5. Proper error handling (DecodeFailureReason enum)\n\n  The durability crate adds a layer ON TOP of SymbolCodec:\n  - RepairTrailer: binary format for appending repair symbols to existing files\n  - FileProtector: high-level protect/verify/repair workflow\n  - TantivyWrapper: hooks into Tantivy's segment lifecycle\n\nKEY INSIGHT -- WHY THIS IS DIFFERENT FROM TRADITIONAL CHECKSUMS:\n  CRC32/SHA-256 can DETECT corruption but cannot REPAIR it.\n  RaptorQ repair symbols can both detect AND repair:\n  - With R=2 repair symbols and K source symbols, any 2 corrupted source symbols can be recovered\n  - With 20% overhead (R = ceil(K * 0.2)), up to 20% of the file can be corrupted and recovered\n  - Recovery is information-theoretically optimal (approaches Shannon limit)\n  - No coordination needed: repair symbols are deterministic from the source data\n\n  For a 73MB vector index (100K docs x 384d x f16), 20% overhead = 14.6MB of repair symbols.\n  This is stored in a sidecar file (.fsvi.fec) to avoid modifying the main index format.\n\nFile: crates/frankensearch-durability/src/lib.rs\n","created_at":"2026-02-13T20:46:10Z"}]}
{"id":"bd-3w1.6","title":"Implement RaptorQ repair symbol codec wrapper","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:22.195438810Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:10.146861857Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["codec","durability","raptorq","tier2"],"dependencies":[{"issue_id":"bd-3w1.6","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:22.195438810Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.6","depends_on_id":"bd-3w1.5","type":"blocks","created_at":"2026-02-13T20:42:27.509866322Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":59,"issue_id":"bd-3w1.6","author":"Dicklesworthstone","text":"TASK: Implement the RaptorQ repair symbol codec wrapper.\n\nThis wraps FrankenSQLite's SymbolCodec trait with a frankensearch-specific API that's optimized for our use case: protecting binary files (vector indices, segment files) rather than database pages.\n\nCODEC WRAPPER API:\n\n  pub struct RepairCodecConfig {\n      pub symbol_size: u32,            // Default: 4096 (4KB, matching typical page size)\n      pub overhead_percent: u32,       // Default: 20 (20% extra repair symbols)\n      pub max_repair_symbols: u32,     // Default: 250_000 (anti-footgun guardrail)\n      pub slack_decode: u32,           // Default: 2 (RFC 6330 Annex B: K+2 for negligible failure)\n  }\n\n  pub struct RepairCodec {\n      inner: Box<dyn SymbolCodec>,     // FrankenSQLite's codec implementation\n      config: RepairCodecConfig,\n      metrics: Arc<DurabilityMetrics>,\n  }\n\n  pub struct DurabilityMetrics {\n      pub files_protected: AtomicU64,\n      pub files_verified: AtomicU64,\n      pub files_repaired: AtomicU64,\n      pub repair_failures: AtomicU64,\n      pub total_source_bytes: AtomicU64,\n      pub total_repair_bytes: AtomicU64,\n      pub total_encode_time_us: AtomicU64,\n      pub total_decode_time_us: AtomicU64,\n      pub corruption_events_detected: AtomicU64,\n  }\n\n  impl RepairCodec {\n      pub fn new(config: RepairCodecConfig) -> SearchResult<Self>;\n\n      /// Encode source data into source + repair symbols\n      pub fn encode(&self, source_data: &[u8]) -> SearchResult<EncodedData>;\n\n      /// Verify source data integrity using repair symbols\n      /// Returns Ok(true) if intact, Ok(false) if corrupted but repairable\n      pub fn verify(&self, source_data: &[u8], repair_data: &RepairData) -> SearchResult<VerifyResult>;\n\n      /// Attempt to repair corrupted source data using repair symbols\n      pub fn repair(&self, corrupted_data: &[u8], repair_data: &RepairData) -> SearchResult<Vec<u8>>;\n\n      /// Compute deterministic repair symbols for a given source\n      /// Determinism: same source_data + same config = identical repair symbols every time\n      /// Seed derivation: xxh3_64(source_data) (matching FrankenSQLite's approach)\n      pub fn compute_repair_symbols(&self, source_data: &[u8]) -> SearchResult<RepairData>;\n  }\n\n  pub struct EncodedData {\n      pub source_symbols: Vec<(u32, Vec<u8>)>,  // (ESI, data)\n      pub repair_symbols: Vec<(u32, Vec<u8>)>,  // (ESI, data)\n      pub k_source: u32,\n      pub symbol_size: u32,\n  }\n\n  pub struct RepairData {\n      pub repair_symbols: Vec<(u32, Vec<u8>)>,\n      pub k_source: u32,\n      pub symbol_size: u32,\n      pub source_hash: [u8; 8],         // xxh3_64 of original source (for verification)\n  }\n\n  pub enum VerifyResult {\n      Intact,\n      Corrupted { corrupted_symbols: usize, repairable: bool },\n  }\n\nREPAIR BUDGET FORMULA (from FrankenSQLite repair_symbols.rs):\n  slack_decode = 2\n  R_formula = max(slack_decode, ceil(K_source * overhead_percent / 100))\n  R = min(max_repair_symbols, R_formula)\n\n  For a 73MB FSVI index with 4KB symbols:\n  K_source = 73MB / 4KB = 18,688 source symbols\n  R = max(2, ceil(18688 * 0.20)) = 3,738 repair symbols\n  Repair data size = 3,738 * 4KB = ~14.6MB\n\nDETERMINISTIC REPAIR (from FrankenSQLite):\n  The repair symbols are deterministic: given the same source data and config, the same\n  repair symbols are always generated. This is achieved by deriving the RaptorQ random seed\n  from xxh3_64(source_data). Benefits:\n  1. Verification without original: compare generated repair symbols against stored ones\n  2. Incremental repair: regenerate specific missing symbols on demand\n  3. Idempotent writes: writing the same repair symbols twice is harmless\n  4. Cross-replica consistency: any node generates identical symbols\n\nCORRUPTION DETECTION:\n  Before invoking the full decode pipeline (expensive), do a quick integrity check:\n  1. xxh3_64 of source data vs stored hash: if match, file is intact (fast path)\n  2. Per-symbol CRC32: identify which symbols are corrupted (medium path)\n  3. Full RaptorQ decode: reconstruct from surviving + repair symbols (slow path)\n\n  The fast path (xxh3_64 comparison) takes < 1ms for a 73MB file.\n  Full decode only happens when corruption is detected.\n\nFile: frankensearch-durability/src/codec.rs\n","created_at":"2026-02-13T20:46:10Z"}]}
{"id":"bd-3w1.7","title":"Add RaptorQ repair trailer to FSVI vector index format","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:23.247430442Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:10.234057814Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","tier2","vector-index"],"dependencies":[{"issue_id":"bd-3w1.7","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T20:42:27.748405998Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.7","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:23.247430442Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.7","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:27.630468374Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":60,"issue_id":"bd-3w1.7","author":"Dicklesworthstone","text":"TASK: Add RaptorQ repair trailer format to FSVI vector index files.\n\nThis extends the FSVI format (bd-3un.13) with an optional sidecar file containing RaptorQ repair symbols. The main FSVI file is unchanged (backwards compatible), and the repair data lives in a parallel .fsvi.fec file.\n\nSIDECAR FILE FORMAT (.fsvi.fec):\n\n  Header (32 bytes, little-endian):\n    Offset  Size  Field\n    0       4     magic: \"FECR\" (FrankenSearch Erasure Code Repair)\n    4       2     version: u16 (start at 1)\n    6       2     reserved: u16\n    8       4     symbol_size: u32 (bytes per symbol, e.g., 4096)\n    12      4     k_source: u32 (number of source symbols)\n    16      4     r_repair: u32 (number of repair symbols)\n    20      4     overhead_percent: u32 (for documentation/verification)\n    24      8     source_hash: u64 (xxh3_64 of the protected file)\n\n  Repair Symbol Table (r_repair entries, each symbol_size bytes):\n    [symbol_0: symbol_size bytes]\n    [symbol_1: symbol_size bytes]\n    ...\n    [symbol_{r_repair-1}: symbol_size bytes]\n\n  Footer (8 bytes):\n    0       4     trailer_crc32: u32 (CRC32 of entire file excluding this footer)\n    4       4     magic_end: \"RCEF\" (reverse magic, end-of-file marker)\n\nFILE NAMING CONVENTION:\n  vector.fast.fsvi     -> vector.fast.fsvi.fec\n  vector.quality.fsvi  -> vector.quality.fsvi.fec\n\nPROTECTION WORKFLOW:\n\n  pub struct FsviProtector {\n      codec: RepairCodec,\n  }\n\n  impl FsviProtector {\n      /// Generate repair symbols for an FSVI file and write the .fec sidecar\n      pub fn protect(&self, fsvi_path: &Path) -> SearchResult<ProtectionResult>;\n\n      /// Verify FSVI file integrity using the .fec sidecar\n      /// Returns VerifyResult::Intact if file matches source_hash\n      /// Returns VerifyResult::Corrupted with repair info if damaged\n      pub fn verify(&self, fsvi_path: &Path) -> SearchResult<VerifyResult>;\n\n      /// Attempt to repair a corrupted FSVI file using the .fec sidecar\n      /// On success, overwrites the corrupted file with repaired data\n      /// On failure, returns error (corruption exceeds repair capacity)\n      pub fn repair(&self, fsvi_path: &Path) -> SearchResult<RepairResult>;\n\n      /// Atomic protect: write .fec to temp file, then rename (crash-safe)\n      pub fn protect_atomic(&self, fsvi_path: &Path) -> SearchResult<ProtectionResult>;\n  }\n\n  pub struct ProtectionResult {\n      pub source_size: u64,\n      pub repair_size: u64,\n      pub overhead_ratio: f32,\n      pub k_source: u32,\n      pub r_repair: u32,\n      pub encode_time: Duration,\n  }\n\n  pub struct RepairResult {\n      pub bytes_corrupted: usize,\n      pub symbols_repaired: usize,\n      pub decode_time: Duration,\n      pub source_hash_before: u64,  // xxh3_64 of corrupted data\n      pub source_hash_after: u64,   // xxh3_64 of repaired data (should match original)\n  }\n\nINTEGRATION WITH VECTOR INDEX WRITER (bd-3un.13):\n  VectorIndexWriter::finish() should optionally call FsviProtector::protect_atomic()\n  when the 'durability' feature is enabled. The protection happens AFTER fsync of the\n  main index file, ensuring the repair symbols cover the durable version.\n\n  Updated finish() flow:\n  1. Write all records to FSVI file\n  2. fsync the FSVI file\n  3. fsync the parent directory\n  4. If durability feature enabled:\n     a. Compute repair symbols from the fsynced file\n     b. Write .fec sidecar (atomic: temp + rename)\n     c. fsync the .fec file and parent directory\n     d. Log protection result\n\nAUTOMATIC REPAIR ON LOAD:\n  VectorIndex::open() should optionally verify integrity and attempt repair:\n\n  impl VectorIndex {\n      pub fn open(path: &Path) -> SearchResult<Self> {\n          // ... existing load logic ...\n\n          #[cfg(feature = \"durability\")]\n          if let Some(protector) = FsviProtector::try_new() {\n              match protector.verify(path)? {\n                  VerifyResult::Intact => { /* fast path, < 1ms */ }\n                  VerifyResult::Corrupted { repairable: true, .. } => {\n                      tracing::warn!(\"vector index corrupted, attempting repair\");\n                      protector.repair(path)?;\n                      tracing::info!(\"vector index repaired successfully\");\n                      // Re-open from repaired file\n                  }\n                  VerifyResult::Corrupted { repairable: false, .. } => {\n                      tracing::error!(\"vector index corrupted beyond repair capacity\");\n                      return Err(SearchError::IndexCorrupted { path: path.to_owned() });\n                  }\n              }\n          }\n      }\n  }\n\nEXAMPLE SIZES:\n  | Index Size | Symbols (4KB) | Repair (20%) | .fec Size | Overhead |\n  |-----------|---------------|--------------|-----------|----------|\n  | 7.3MB     | 1,869         | 374          | 1.5MB     | 20.5%    |\n  | 73MB      | 18,688        | 3,738        | 14.6MB    | 20.0%    |\n  | 730MB     | 186,880       | 37,376       | 146MB     | 20.0%    |\n\nFile: frankensearch-durability/src/repair_trailer.rs + integration in frankensearch-index\n","created_at":"2026-02-13T20:46:10Z"}]}
{"id":"bd-3w1.8","title":"Implement self-healing Tantivy segment wrapper with RaptorQ","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:23.916608544Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:10.934466498Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","tantivy","tier2"],"dependencies":[{"issue_id":"bd-3w1.8","depends_on_id":"bd-3un.17","type":"blocks","created_at":"2026-02-13T20:42:27.986993183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.8","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:23.916608544Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.8","depends_on_id":"bd-3w1.6","type":"blocks","created_at":"2026-02-13T20:42:27.866126756Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":61,"issue_id":"bd-3w1.8","author":"Dicklesworthstone","text":"TASK: Implement self-healing Tantivy segment wrapper with RaptorQ repair symbols.\n\nTantivy stores its index as multiple segment files. Each segment is a set of files (postings, positions, terms, store, fast fields). This task wraps Tantivy's segment lifecycle to add RaptorQ protection.\n\nCHALLENGE:\n  Tantivy manages its own files via the Directory trait. We can't simply append repair\n  symbols to Tantivy's files because Tantivy's merge process creates/deletes segments.\n  Instead, we hook into the segment lifecycle and protect completed segments.\n\nAPPROACH -- SEGMENT LIFECYCLE HOOKS:\n\n  pub struct DurableTantivyIndex {\n      inner: tantivy::Index,\n      protector: Arc<FileProtector>,\n      data_dir: PathBuf,\n  }\n\n  impl DurableTantivyIndex {\n      /// Open a Tantivy index with durability protection\n      pub fn open(data_dir: &Path, schema: Schema, config: RepairCodecConfig) -> SearchResult<Self>;\n\n      /// After a commit, protect any new/changed segment files\n      /// Called automatically after writer.commit() via a post-commit hook\n      pub fn protect_segments(&self) -> SearchResult<SegmentProtectionReport>;\n\n      /// Verify all segment files, attempt repair if corrupted\n      /// Called automatically on open()\n      pub fn verify_and_repair(&self) -> SearchResult<SegmentHealthReport>;\n\n      /// Get the inner Tantivy index for search operations\n      pub fn index(&self) -> &tantivy::Index;\n  }\n\n  pub struct SegmentProtectionReport {\n      pub segments_protected: usize,\n      pub segments_already_protected: usize,\n      pub total_source_bytes: u64,\n      pub total_repair_bytes: u64,\n      pub encode_time: Duration,\n  }\n\n  pub struct SegmentHealthReport {\n      pub segments_checked: usize,\n      pub segments_intact: usize,\n      pub segments_repaired: usize,\n      pub segments_unrecoverable: usize,\n      pub verify_time: Duration,\n      pub repair_time: Duration,\n  }\n\nTANTIVY SEGMENT FILE PROTECTION:\n  Each Tantivy segment has an ID (UUID) and multiple component files:\n    - {segment_id}.pos   (positions)\n    - {segment_id}.idx   (postings)\n    - {segment_id}.term  (term dictionary)\n    - {segment_id}.store (document store)\n    - {segment_id}.fast  (fast fields)\n    - {segment_id}.fieldnorm (field norms)\n\n  For each segment, we create a single .seg.fec sidecar that contains repair symbols\n  for ALL component files concatenated. The sidecar header maps component files to\n  their byte ranges within the concatenated source:\n\n  .seg.fec Header Extension:\n    component_count: u32\n    For each component:\n      filename_len: u16\n      filename: bytes\n      offset: u64 (within concatenated source)\n      size: u64\n\n  This means one .fec file per segment, not one per component file.\n\nLIFECYCLE INTEGRATION:\n  1. ON COMMIT: After writer.commit(), enumerate new segments and protect them\n  2. ON MERGE: After merge completes, protect the merged segment, remove .fec for merged-away segments\n  3. ON OPEN: Verify all segments, repair any corrupted ones before search\n  4. ON DELETE: When segments are garbage-collected, remove their .fec sidecars too\n\n  The protect step is asynchronous (doesn't block the commit path) but must complete\n  before the segment is considered durable.\n\nCORRUPTED INDEX RECOVERY (from cass tantivy.rs lines 110-121):\n  If a segment is corrupted beyond repair (RaptorQ fails), fall back to:\n  1. Log WARN with corruption details\n  2. Attempt Tantivy's built-in recovery (open with FORCE flag)\n  3. If that fails, trigger a full index rebuild from the document store (FrankenSQLite)\n  4. This is the fallback of last resort -- the document store IS the source of truth\n\n  This integrates with the schema versioning from bd-3un.17: if schema_hash mismatches,\n  the index is rebuilt anyway, so corruption during a schema migration is handled.\n\nPERFORMANCE CONSIDERATIONS:\n  - Tantivy indices are typically 10-100MB (much larger with store enabled)\n  - Encoding a 50MB segment at 20% overhead: ~200ms (4KB symbols, parallelizable)\n  - Verification (xxh3_64 fast path): ~10ms for 50MB\n  - This is acceptable because it happens at COMMIT time, not QUERY time\n\nFile: frankensearch-durability/src/tantivy_wrapper.rs\n","created_at":"2026-02-13T20:46:10Z"}]}
{"id":"bd-3w1.9","title":"Implement corruption detection and automatic repair pipeline","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-13T20:37:24.721964896Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:11.026546939Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq","repair","tier2"],"dependencies":[{"issue_id":"bd-3w1.9","depends_on_id":"bd-3w1","type":"parent-child","created_at":"2026-02-13T20:37:24.721964896Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.9","depends_on_id":"bd-3w1.7","type":"blocks","created_at":"2026-02-13T20:42:28.111622734Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3w1.9","depends_on_id":"bd-3w1.8","type":"blocks","created_at":"2026-02-13T20:42:28.231363504Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":62,"issue_id":"bd-3w1.9","author":"Dicklesworthstone","text":"TASK: Implement the corruption detection and automatic repair pipeline.\n\nThis is the high-level orchestrator that ties together the codec (bd-3w1.6), FSVI protection (bd-3w1.7), and Tantivy protection (bd-3w1.8) into a single coherent repair pipeline.\n\nFILE PROTECTOR API:\n\n  pub struct FileProtector {\n      codec: RepairCodec,\n      config: FileProtectorConfig,\n      metrics: Arc<DurabilityMetrics>,\n  }\n\n  pub struct FileProtectorConfig {\n      pub verify_on_open: bool,           // Default: true (verify indices on load)\n      pub protect_on_write: bool,         // Default: true (generate .fec after index write)\n      pub auto_repair: bool,              // Default: true (attempt repair when corruption detected)\n      pub repair_log_dir: Option<PathBuf>, // Optional directory for repair event logs\n      pub verify_interval_secs: Option<u64>, // Optional periodic verification (e.g., every 3600s)\n  }\n\n  impl FileProtector {\n      /// Protect a file: compute repair symbols and write .fec sidecar\n      pub fn protect(&self, path: &Path) -> SearchResult<ProtectionResult>;\n\n      /// Verify a file's integrity using its .fec sidecar\n      pub fn verify(&self, path: &Path) -> SearchResult<VerifyResult>;\n\n      /// Attempt to repair a corrupted file\n      pub fn repair(&self, path: &Path) -> SearchResult<RepairResult>;\n\n      /// Full pipeline: verify, repair if needed, report\n      pub fn verify_and_repair(&self, path: &Path) -> SearchResult<HealthCheckResult>;\n\n      /// Protect all index files in a directory (FSVI + Tantivy segments)\n      pub fn protect_directory(&self, dir: &Path) -> SearchResult<DirectoryProtectionReport>;\n\n      /// Verify all protected files in a directory\n      pub fn verify_directory(&self, dir: &Path) -> SearchResult<DirectoryHealthReport>;\n  }\n\n  pub struct HealthCheckResult {\n      pub path: PathBuf,\n      pub status: FileHealth,\n      pub details: String,\n  }\n\n  pub enum FileHealth {\n      Intact,\n      Repaired { corrupted_bytes: usize, repair_time: Duration },\n      Unrecoverable { reason: String },\n      Unprotected,  // No .fec sidecar found\n  }\n\nAUTOMATIC REPAIR PIPELINE:\n\n  The repair pipeline is invoked:\n  1. On index open (if verify_on_open = true)\n  2. Periodically (if verify_interval_secs is set)\n  3. When a search returns unexpected errors (SearchError::IndexCorrupted)\n\n  Pipeline steps:\n  a. FAST CHECK: xxh3_64(file) vs stored source_hash (~1ms per 100MB)\n     - If match: file is intact, return immediately\n  b. SYMBOL-LEVEL CHECK: Compare individual source symbols against stored checksums\n     - Identifies which specific 4KB blocks are corrupted\n  c. REPAIR ATTEMPT: Feed surviving + repair symbols into RaptorQ decoder\n     - If decode succeeds: overwrite corrupted file, verify again, log event\n     - If decode fails: log error, return Unrecoverable\n\n  All repair events are logged with full context:\n    tracing::warn!(\n        path = %path.display(),\n        corrupted_bytes = corrupted,\n        repair_symbols_used = used,\n        decode_time_ms = time.as_millis(),\n        \"index file repaired after corruption detected\"\n    );\n\nREPAIR EVENT LOG:\n  When repair_log_dir is set, each repair event is appended as a JSONL record:\n  {\n      \"timestamp\": \"2026-02-13T20:30:00Z\",\n      \"path\": \"/data/search/vector.fast.fsvi\",\n      \"corrupted_symbols\": 3,\n      \"total_symbols\": 18688,\n      \"repair_succeeded\": true,\n      \"decode_time_ms\": 450,\n      \"source_hash_before\": \"0x1234abcd\",\n      \"source_hash_after\": \"0x5678ef01\"\n  }\n\n  This provides an audit trail for understanding corruption patterns\n  (hardware issues, filesystem bugs, etc.).\n\nGRACEFUL DEGRADATION:\n  If the durability feature is disabled at compile time, the FileProtector becomes a no-op:\n  - protect() returns Ok immediately\n  - verify() returns VerifyResult::Unprotected\n  - repair() returns error \"durability feature not enabled\"\n\n  This is implemented via a trait with a default no-op implementation and a\n  #[cfg(feature = \"durability\")] real implementation.\n\nINTEGRATION WITH SEARCH PIPELINE:\n  The TwoTierSearcher (bd-3un.24) should:\n  1. Hold an optional Arc<FileProtector>\n  2. On SearchPhase::RefinementFailed, check if failure was due to index corruption\n  3. If so, attempt repair and retry the search once\n  4. Log the entire sequence (corruption detected -> repair -> retry -> success/failure)\n\nFile: frankensearch-durability/src/file_protector.rs\n","created_at":"2026-02-13T20:46:11Z"}]}
{"id":"bd-6sj","title":"Implement off-policy evaluation for safe ranking changes","description":"Implement offline ranking evaluation infrastructure using Inverse Propensity Scoring (IPS) and Doubly Robust (DR) estimators. This enables estimating the impact of ranking algorithm changes (new K, new blend_factor, new reranker model) using historical search logs BEFORE deploying online.\n\nGraveyard entry: §12.12 Off-Policy Evaluation (IPS/DR)\nEV score: 6.0 (Impact=3, Confidence=3, Reuse=4, Effort=3, Friction=2)\nPriority tier: B (requires evidence ledger from bd-3un.39 first)\n\nArchitecture:\npub struct OffPolicyEvaluator {\n    logging_policy: Box<dyn RankingPolicy>,   // The policy that generated the logs\n    target_policy: Box<dyn RankingPolicy>,    // The proposed new policy\n    clipping_threshold: f64,                   // Max IPS weight (default 100.0)\n}\n\npub trait RankingPolicy: Send + Sync {\n    fn score(&self, query: &str, doc_id: &str) -> f64;\n    fn propensity(&self, query: &str, doc_id: &str, rank: usize) -> f64;\n}\n\nEstimators:\n1. IPS (Inverse Propensity Scoring):\n   estimated_reward = (1/N) * sum(reward_i * target_propensity_i / logging_propensity_i)\n   - Unbiased but high variance\n   - Clipping: cap importance weight at threshold to reduce variance\n\n2. DR (Doubly Robust):\n   estimated_reward = IPS_term + control_variate_from_reward_model\n   - Variance <= IPS variance (provable)\n   - Requires reward model (can use NDCG@10 from golden corpus)\n\n3. Effective Sample Size (ESS):\n   ESS = (sum(w_i))^2 / sum(w_i^2)\n   - Reject estimate if ESS < 100 (insufficient overlap)\n\nIntegration:\n- Consumes evidence ledger (bd-3un.39) as log source\n- Consumes test fixture corpus (bd-3un.38) for reward labels\n- Produces: estimated NDCG@10 delta, confidence interval, ESS\n\nBudgeted mode: Offline only (no runtime cost). Memory proportional to log size.\n\nFallback: Don't use OPE estimates; rely on golden-check corpus only.\n\nFile: frankensearch-fusion/src/ope.rs (offline evaluation module)\n\nReference: Dudik et al. (2011) \"Doubly Robust Policy Evaluation\", Swaminathan & Joachims (2015) \"Batch Learning from Logged Bandit Feedback\"\nBaseline comparator: Golden-check-only validation (current bd-3un.38)","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:45:43.570858954Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:23.437527821Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["evaluation","graveyard","offline","phase11"],"dependencies":[{"issue_id":"bd-6sj","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.437480493Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6sj","depends_on_id":"bd-3un.38","type":"blocks","created_at":"2026-02-13T20:46:12.983731002Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6sj","depends_on_id":"bd-3un.39","type":"blocks","created_at":"2026-02-13T20:46:12.896196972Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-l7v","title":"Implement S3-FIFO lock-free cache eviction","description":"Implement S3-FIFO (Yang et al. 2023) three-queue cache eviction for embedding vectors, search results, and index segments. S3-FIFO uses Small/Main/Ghost FIFO queues with no per-access list manipulation (unlike LRU), achieving higher hit rates and lock-free operation.\n\nGraveyard entry: §15.1 S3-FIFO Cache Eviction\nEV score: 50 (Impact=4, Confidence=5, Reuse=5, Effort=2, Friction=1)\nPriority tier: A\n\nArchitecture:\npub struct S3FifoCache<K, V> {\n    small: FifoQueue<K, V>,    // New entries land here (10% capacity)\n    main: FifoQueue<K, V>,     // Promoted entries (90% capacity)\n    ghost: GhostQueue<K>,      // Evicted keys (metadata only, 2x main capacity)\n    max_bytes: usize,          // Configurable memory budget (default 256MB)\n    freq_threshold: u8,        // Promotion threshold (default 1)\n}\n\nEviction policy:\n1. New items enter Small queue\n2. On Small eviction: if accessed >= freq_threshold times, promote to Main; else evict\n3. On Main eviction: evict (FIFO order)\n4. Ghost tracks recently evicted keys (metadata only); re-access of ghost key → direct Main insert\n5. All queues are FIFO (no list manipulation per access)\n\nCache targets in frankensearch:\n- Embedding vectors: Key=(doc_id, embedder_id), Value=Vec<f32> (256-768 bytes)\n- Quality model inference: Key=content_hash, Value=Vec<f32> (avoids re-embedding)\n- Tantivy search results: Key=(query_hash, k), Value=Vec<ScoredResult>\n\nTrait interface:\npub trait CachePolicy<K: Hash + Eq, V>: Send + Sync {\n    fn get(&self, key: &K) -> Option<&V>;\n    fn insert(&self, key: K, value: V, size_bytes: usize);\n    fn hit_rate(&self) -> f64;\n    fn memory_used(&self) -> usize;\n}\n\nImplementations: S3Fifo (default), Unbounded (HashMap, for tests), NoCache (passthrough).\n\nBudgeted mode: max_bytes configurable via FRANKENSEARCH_CACHE_MB env var. On exhaustion: evict Small first. Hit rate < 30% for 100 queries → bypass cache + log WARN.\n\nFallback: CachePolicy::noop() returns None for all gets (zero-cost passthrough).\n\nIsomorphism proof: Cache is transparent — same query produces identical rankings with/without cache. Verify via golden-check corpus (bd-3un.38).\n\nFile: frankensearch-core/src/cache.rs (trait + S3Fifo impl)\n\nReference: Yang et al. \"FIFO Queues are All You Need for Cache Eviction\" (SOSP 2023)\nBaseline comparator: OnceLock (never evicts), HashMap (unbounded growth), LRU (lock per access)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-13T20:45:31.505125880Z","created_by":"ubuntu","updated_at":"2026-02-13T20:46:23.178957469Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cache","graveyard","phase7"],"dependencies":[{"issue_id":"bd-l7v","depends_on_id":"bd-3un","type":"parent-child","created_at":"2026-02-13T20:46:23.178896495Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-l7v","depends_on_id":"bd-3un.2","type":"blocks","created_at":"2026-02-13T20:45:55.877271569Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-l7v","depends_on_id":"bd-3un.5","type":"blocks","created_at":"2026-02-13T20:46:00.462669693Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-qtx","title":"Implement quantization ladder (f32/f16/int8/int4) with formal quality bounds","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T20:31:23.285837206Z","created_by":"ubuntu","updated_at":"2026-02-13T20:31:46.435180900Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["optimization","phase4","quantization","vector-index"],"dependencies":[{"issue_id":"bd-qtx","depends_on_id":"bd-3un","type":"blocks","created_at":"2026-02-13T20:31:46.435141937Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.13","type":"blocks","created_at":"2026-02-13T20:31:39.251284448Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qtx","depends_on_id":"bd-3un.14","type":"blocks","created_at":"2026-02-13T20:31:39.331697702Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":37,"issue_id":"bd-qtx","author":"Dicklesworthstone","text":"Implement a quantization ladder (f32/f16/int8/int4) with formal quality-memory tradeoff characterization for vector indices. Enables extreme memory savings for large-scale deployments.\n\nOPTIMIZATION RATIONALE:\n\nCurrent FSVI format supports f32 and f16. For large indices (100K+ docs), memory is the bottleneck:\n- 100K docs x 384 dims x f32 = 147 MB\n- 100K docs x 384 dims x f16 = 73 MB   (current default)\n- 100K docs x 384 dims x int8 = 37 MB   (THIS BEAD)\n- 100K docs x 384 dims x int4 = 18 MB   (THIS BEAD)\n\nOpportunity Matrix:\n| Quantization | Memory | Quality Loss | Effort | Score (Impact x Conf / Effort) |\n|-------------|--------|-------------|--------|-------------------------------|\n| int8 scalar | 4x vs f32 | ~1-2%  | 2      | 10.0                          |\n| int4 packed | 8x vs f32 | ~3-5%  | 4      | 5.0                           |\n\nIMPLEMENTATION:\n\n1. Scalar Quantization (int8):\n   - Per-dimension min/max computed during indexing\n   - Quantize: q = round((x - min) / (max - min) * 255)\n   - Dequantize: x' = q / 255 * (max - min) + min\n   - Store quantization parameters in FSVI header (2 floats per dimension)\n   - SIMD dot product on int8: use i16 multiply-accumulate via wide crate\n\npub struct ScalarQuantizer {\n    mins: Vec<f32>,     // Per-dimension minimum\n    scales: Vec<f32>,   // Per-dimension (max - min) / 255\n}\n\nimpl ScalarQuantizer {\n    pub fn fit(vectors: &[Vec<f32>]) -> Self;\n    pub fn quantize(&self, vector: &[f32]) -> Vec<u8>;\n    pub fn dot_product_quantized(&self, stored: &[u8], query: &[f32]) -> f32;\n}\n\n2. Product Quantization (optional, for int4-equivalent compression):\n   - Split 384-dim vector into 48 sub-vectors of 8 dims each\n   - K-means cluster each sub-space into 256 centroids\n   - Store 1 byte per sub-vector (centroid index)\n   - Asymmetric distance computation (ADC): query in full precision, database quantized\n   - Compression: 384 dims -> 48 bytes (8x vs f32, 4x vs f16)\n\n3. FSVI Format Extension:\n   - quantization field in header: 0=f32, 1=f16, 2=int8, 3=int4/PQ\n   - For int8: append quantization parameters after header (mins + scales)\n   - For PQ: append codebook after header\n\n4. Quality Characterization (Alien-Artifact):\n   - For each quantization level, compute NDCG@10 on the test fixture corpus\n   - Provide formal bounds on maximum quality loss:\n     For int8: |cos_sim(q, x) - cos_sim(q, x')| <= epsilon\n     where epsilon = max_dim(scale_i / 255) * sqrt(d) / ||q|| / ||x||\n   - This is a PROVABLE bound on the worst-case quality degradation\n\n5. Automatic Selection:\n   pub fn recommended_quantization(index_size: usize, available_memory: usize) -> Quantization {\n       let f16_size = index_size * dimension * 2;\n       let int8_size = index_size * dimension;\n       if f16_size <= available_memory { Quantization::F16 }\n       else if int8_size <= available_memory { Quantization::Int8 }\n       else { Quantization::PQ }\n   }\n\nFile: frankensearch-index/src/quantization.rs\nDependencies: bd-3un.13 (FSVI format), bd-3un.14 (SIMD dot product)\n\nIsomorphism: Rankings may change slightly due to quantization error. Provide formal epsilon bounds and NDCG regression test: NDCG@10(int8) >= 0.95 * NDCG@10(f16) on test corpus.\n","created_at":"2026-02-13T20:31:32Z"}]}
